{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "## Assignment 1\n",
    "## verson.1.5\n",
    "\n",
    "**Student details**\n",
    "- Name: Chuangfu Xie\n",
    "- Student ID: 27771539\n",
    "- Name: Kai Wang\n",
    "- Student ID: 28164911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment_Logs\n",
    "```shell\n",
    "#TimeStamp_27_March_2018 - ver.1.0: No re-usability are considered, no justification, just implementation\n",
    "#TimeStamp_03_April_2018 - ver.1.1: Code updated, finish task1, task2, task3(mergesort()).\n",
    "#TimeStamp_08_April_2018 - ver.1.2: Code updated, solve the task 3 bug: [sys.maxsize] changed to [sys.maxsize]*9\n",
    "#TimeStamp_10_April_2018 - ver.1.3: Solve task 4. Add comments and self.__doc__\n",
    "#TimeStamp_11_April_2018 - ver.1.4: Code updated, add justification, optimizing some function for reuse.\n",
    "#TimeStamp_16_April_2018 - ver.1.5: Final touch, more improve in reusability\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages:\n",
    "All packages we used in this notebook are imported as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import re\n",
    "import csv\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ClimateData\n",
    "df_Climate = pd.read_csv(\"./ClimateData.csv\")\n",
    "# Trim the spaces from heading row\n",
    "df_Climate.columns = df_Climate.columns.str.strip()\n",
    "# Specify the order of columns, prepared for swapping columns\n",
    "new_cols = ['Date','Station','Air Temperature(Celcius)','Relative Humidity',\n",
    "            'WindSpeed  (knots)','Max Wind Speed','MAX','MIN','Precipitation']\n",
    "# Swapping columns\n",
    "df_Climate = df_Climate.loc[:,new_cols]\n",
    "# write a new CSV to current working directory\n",
    "df_Climate.to_csv('./new_ClimateData.csv')\n",
    "# with statement just in case the file is not exit\n",
    "# read data in desired format\n",
    "with open(\"./new_ClimateData.csv\",'r') as data:\n",
    "        reader = csv.reader(data)\n",
    "        all_data = list(reader)\n",
    "# keep the heading, prefix with first letter of the table\n",
    "c_header = all_data[0]\n",
    "# strip the heading, purely just contains all records\n",
    "c_records = all_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FireData(almost the same process as in climateData)\n",
    "df_Fire = pd.read_csv(\"./FireData.csv\")\n",
    "new_cols = ['Date','Datetime','Surface Temperature (kelvin)','Surface Temperature (Celcius)',\n",
    "            'Power','Confidence','Latitude','Longitude']\n",
    "df_Fire = df_Fire.loc[:,new_cols]\n",
    "df_Fire.to_csv('./new_FireData.csv')\n",
    "with open(\"./new_FireData.csv\",'r') as data:\n",
    "        reader = csv.reader(data)\n",
    "        all_data = list(reader)\n",
    "f_header = all_data[0]\n",
    "f_records = all_data[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Task.1 Parallel Search\n",
    "#### 1. Write an algorithm to search climate data for the records on 15th December 2017. Justify your choice of the data partition technique and search technique you have used.\n",
    "\n",
    "**Solution & Reasoning**:  \n",
    "For parallel search, definitely I will use parallalism to achieve. In this task, I use **Round-Robin partition** to distribute data set for each processor, then each processor do **hash_partition** and **hash_search** to find the query data. The reason for choosing Round-Robin is that it fairly tackle balance workload problem. As for hash_partition, though each processor will hash every single recordm which is N time in total for hashing (N stands for the total number of record), it may be costly. However, if we pre-process every single record beforehand, the process of query will work efficiently since we only hash the query once, then we can use it as key to efficiently find the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Station</th>\n",
       "      <th>Air Temperature(Celcius)</th>\n",
       "      <th>Relative Humidity</th>\n",
       "      <th>WindSpeed  (knots)</th>\n",
       "      <th>Max Wind Speed</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MIN</th>\n",
       "      <th>Precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>948700</td>\n",
       "      <td>19</td>\n",
       "      <td>56.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>72.0*</td>\n",
       "      <td>61.9*</td>\n",
       "      <td>0.00I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>948700</td>\n",
       "      <td>15</td>\n",
       "      <td>50.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72.0*</td>\n",
       "      <td>58.3</td>\n",
       "      <td>0.02G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>948700</td>\n",
       "      <td>16</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68.4*</td>\n",
       "      <td>54.3*</td>\n",
       "      <td>0.00G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>948700</td>\n",
       "      <td>24</td>\n",
       "      <td>61.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>87.3*</td>\n",
       "      <td>54</td>\n",
       "      <td>0.00I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>948700</td>\n",
       "      <td>24</td>\n",
       "      <td>62.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>85.5*</td>\n",
       "      <td>65.7*</td>\n",
       "      <td>0.00I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Station  Air Temperature(Celcius)  Relative Humidity  \\\n",
       "0  2016-12-31   948700                        19               56.8   \n",
       "1  2017-01-02   948700                        15               50.7   \n",
       "2  2017-01-03   948700                        16               53.6   \n",
       "3  2017-01-04   948700                        24               61.6   \n",
       "4  2017-01-05   948700                        24               62.3   \n",
       "\n",
       "   WindSpeed  (knots)  Max Wind Speed       MAX      MIN Precipitation  \n",
       "0                 7.9            11.1     72.0*    61.9*         0.00I  \n",
       "1                 9.2            13.0     72.0*     58.3         0.02G  \n",
       "2                 8.1            15.0     68.4*    54.3*         0.00G  \n",
       "3                 7.7            14.0     87.3*       54         0.00I  \n",
       "4                 7.0            13.0     85.5*    65.7*         0.00I  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data being prettily printed by pandas\n",
    "df_Climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_partition(data, n):# Done\n",
    "    '''\n",
    "    The Round-Robin partitioning\n",
    "    Evenly divide input data into n sublist\n",
    "    \n",
    "    Arguments:\n",
    "    data -- a list of lists object\n",
    "    n -- The number of sublist\n",
    "    \n",
    "    Return:\n",
    "    A list of n sublists which contains roughly\n",
    "    equal number of records\n",
    "    '''\n",
    "    result = []\n",
    "    #Creating partition n as list of lists\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    for index, element in enumerate(data):\n",
    "        index_bin = (int)(index % n) \n",
    "        #Trick: e.g. 1%4=1; 2%4=2; 3%4=3; 4%4=0\n",
    "        result[index_bin].append(element)\n",
    "    return result\n",
    "\n",
    "def H(s): # Done\n",
    "    # Employ the most used hash function from hashlib\n",
    "    return  hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "def hash_partition_date(data): # Done\n",
    "    '''\n",
    "    The hash partitioning\n",
    "    Based on hash function(md5) to partition data\n",
    "    \n",
    "    Arguments:\n",
    "    data -- a list of lists object\n",
    "    \n",
    "    Return:\n",
    "    A dictionary with hash value as keys\n",
    "    '''\n",
    "    # for storing result\n",
    "    dic = {} \n",
    "    for element in data:\n",
    "        h_value = H(element[1])\n",
    "        if h_value in dic:\n",
    "            #if key exist, add in\n",
    "            dic[h_value].append(element)\n",
    "        else:\n",
    "            # Noooo? create one\n",
    "            dic[h_value] = [element]\n",
    "    return dic\n",
    "\n",
    "def hash_search_date(data, query):# Done\n",
    "    # do hash partitiong for next phase\n",
    "    db = hash_partition_date(data)\n",
    "    # for storing results\n",
    "    results = []\n",
    "    q_key = H(query)\n",
    "    if q_key in db:\n",
    "        for i in db[q_key]:\n",
    "            #column 'Date' index is 1\n",
    "            if (i[1] == query):\n",
    "                results.append(i)\n",
    "    return results\n",
    "\n",
    "def p_search_by_date(data, query, n):# Done\n",
    "    '''\n",
    "    Parallel search function for date-based search\n",
    "    Multiple processors are applied.\n",
    "    Round-Robin partitioning for data distribution.\n",
    "    Each processor do hash search individully\n",
    "    In the end, merge all result\n",
    "    '''\n",
    "    #validate input\n",
    "    val_ptn = r'^\\d{4}-\\d{2}-\\d{2}'\n",
    "    match_str = re.match(val_ptn, query)\n",
    "    if (match_str==None):\n",
    "        raise Exception(\"Error: Query input format: yyyy-mm-dd\")\n",
    "    \n",
    "    #initialise pool\n",
    "    pool = Pool(processes = n)\n",
    "    output = []\n",
    "    db = rr_partition(data,n)\n",
    "    for i in db:\n",
    "        each = pool.apply_async(hash_search_date, args=(i, query)).get()\n",
    "        if each:\n",
    "            #if any result\n",
    "            for i in each:\n",
    "                output.append(i)\n",
    "    return [c_header] + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  'Date',\n",
       "  'Station',\n",
       "  'Air Temperature(Celcius)',\n",
       "  'Relative Humidity',\n",
       "  'WindSpeed  (knots)',\n",
       "  'Max Wind Speed',\n",
       "  'MAX',\n",
       "  'MIN',\n",
       "  'Precipitation'],\n",
       " ['195',\n",
       "  '2017-07-15',\n",
       "  '948701',\n",
       "  '6',\n",
       "  '37.1',\n",
       "  '5.8',\n",
       "  '9.9',\n",
       "  '   53.4*',\n",
       "  '  36.9*',\n",
       "  ' 0.00I']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_search_by_date(c_records,'2017-07-15',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write an algorithm to find the latitude, longitude and confidence when the surface temperature (°C) was between 65 °C and 100 °C. Justify your choice of the data partition technique and search technique you have used.\n",
    "\n",
    "**Solution & Reasoning**:  \n",
    "For the question 2, I also use Round-Robin, but I use range-search as the question ask for range. In my Range Search function, I set it only return the dataset in range rather than the entire records after partitioning, since it is what we want truly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_search(data, range_indices, col):\n",
    "    result = []\n",
    "    #sort data by user specified col\n",
    "    sorted_data = sorted(data,key=lambda x: int(x[col]))\n",
    "    n_sec = len(range_indices)\n",
    "    for i in range(n_sec):\n",
    "        s = [x for x in sorted_data if int(x[col]) < range_indices[i]]\n",
    "        result.append(s)\n",
    "        sorted_data = sorted_data[len(s):]\n",
    "    result.append([x for x in sorted_data if int(x[col]) >= range_indices[-1]])\n",
    "    return result[1]\n",
    "\n",
    "def p_range_search_by(data, query, n, col):\n",
    "    '''\n",
    "    <Reusable function>\n",
    "    It is a parallel range search function which is for searching\n",
    "    dataset by query in specified column.\n",
    "    \n",
    "    Arguments:\n",
    "    data -- List. a list of lists object\n",
    "    query -- str. Query format - 'lower bound,upper bound'. \n",
    "             input error tolerated but must starts with numeric.\n",
    "             RegEx is applied.\n",
    "    n: int. The number of parallel processors \n",
    "    col: int. User specified the index of column according to the \n",
    "           data.\n",
    "    \n",
    "    Return:\n",
    "    a list of lists of all desired result if found.\n",
    "    '''\n",
    "    #validate input\n",
    "    val_ptn = r'^(\\d{1,3}),\\s*?(\\d{1,3})'\n",
    "    match_str = re.match(val_ptn, query)\n",
    "    if (match_str==None):\n",
    "        raise Exception(\"Error: Query input format: 60,100\")\n",
    "    lower = int(match_str.group(1))\n",
    "    upper = int(match_str.group(2))\n",
    "    if (lower>=upper):\n",
    "        raise Exception(\"Error: Query value: the upper bound has to be greater than the lower bound\")\n",
    "    #initialise pool\n",
    "    pool = Pool(processes = n)\n",
    "    db = rr_partition(data,n)\n",
    "    output =[]\n",
    "    # do parallel processing\n",
    "    for i in db:\n",
    "        each = pool.apply_async(range_search, args=(i, [lower,upper], col)).get()\n",
    "        if each:\n",
    "            for i in each:\n",
    "                output.append(i)\n",
    "    # Output only contains full target records(all columns), No header!!\n",
    "    return output\n",
    "\n",
    "def format_S_Temp(header,output):\n",
    "    result = [[header[i] for i in [-2,-1,-3,-5]]]\n",
    "    for record in output:\n",
    "        result.append([record[i] for i in [-2,-1,-3,-5]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Latitude', 'Longitude', 'Confidence', 'Surface Temperature (Celcius)'],\n",
       " ['-37.861999999999995', '144.175', '87', '65'],\n",
       " ['-38.3998', '147.064', '89', '65'],\n",
       " ['-38.4792', '146.3081', '89', '65'],\n",
       " ['-36.7218', '141.6411', '89', '65'],\n",
       " ['-36.2618', '141.8783', '89', '65'],\n",
       " ['-37.0959', '143.8206', '89', '65'],\n",
       " ['-34.9023', '142.0557', '89', '65'],\n",
       " ['-36.0295', '143.6409', '89', '65'],\n",
       " ['-35.7642', '143.3321', '89', '65']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = p_range_search_by(f_records, '65,100', 4, 4)\n",
    "format_S_Temp(f_header,result)[:10]\n",
    "# For better demonstration, only 10 rows displayed \n",
    "# if you want to display more, delete the sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task.2 Parallel Join\n",
    "#### 1. Write an algorithm to find surface temperature (°C), air temperature (°C), relative humidity and maximum wind speed. Justify your choice of the data partition technique and join technique you have used.\n",
    "\n",
    "**Solution & Reasoning**:   \n",
    "As the question ask for serval attributes spread through two table, of course I would use join. I choose hash partitioning-based parallel join algorithm, which equals to this equation: Hash-Based partitioning + local join(Hash join) = FAST. Due to it outstanding attribute, I choose it without question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2668, 366)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek who is bigger?\n",
    "len(f_records),len(c_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_addHeader(result, s_header, l_header):\n",
    "    '''\n",
    "    <Reusable function>\n",
    "    This function is for adding proper heading for result output,\n",
    "    which is a two table join result.\n",
    "    \n",
    "    Arguments:\n",
    "    result -- function output, a list of lists object.\n",
    "    s_header -- small table header\n",
    "    l_header -- large table header\n",
    "    \n",
    "    Return:\n",
    "    Basically just add a heading of the input, then return.\n",
    "    '''\n",
    "    new_header =[s_header[1],l_header[2]] + s_header[2:] + l_header[3:]\n",
    "    output = [new_header]\n",
    "    for each in result:\n",
    "        output.append(each)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join(small_T, large_T):\n",
    "    '''\n",
    "    Hash-based join\n",
    "    Small table for broadcasting, large table do partition\n",
    "    Multiple processors are applied\n",
    "    \n",
    "    Arguments:\n",
    "    small_T -- small table\n",
    "    large_T -- large table\n",
    "    '''\n",
    "    #small_T is c_records\n",
    "    #large_T is f_records\n",
    "    result = []\n",
    "    # do partitioning\n",
    "    db_large = hash_partition_date(large_T)\n",
    "    for s in small_T:\n",
    "        s_key = H(s[1])\n",
    "        #check on the dict's keys\n",
    "        if s_key in db_large: \n",
    "            # linear search applied\n",
    "            # only if key exits\n",
    "            for l in db_large[s_key]:\n",
    "                if l[1] == s[1]:\n",
    "                    # Local join: Two table combine into one\n",
    "                    result.append([s[1],l[2]]+s[2:]+l[3:])\n",
    "    #Output only contains full target records(all columns), No header!!\n",
    "    return result\n",
    "\n",
    "def HBPP_join(small_T, large_T, n):\n",
    "    '''\n",
    "    hash partitioning-based parallel join\n",
    "    '''\n",
    "    #initialise pool\n",
    "    pool = Pool(processes = n)\n",
    "    # Get the large table partitioned\n",
    "    db_l_partitoned = rr_partition(large_T, n)\n",
    "    output=[]\n",
    "    # do parallel\n",
    "    for db_set in db_l_partitoned:\n",
    "        each = pool.apply_async(HB_join, args=(small_T, db_set)).get()\n",
    "        if each:\n",
    "            for i in each:\n",
    "                output.append(i)\n",
    "    return output\n",
    "\n",
    "def format_onSameday(result,s_header, l_header):\n",
    "    output = []\n",
    "    for each in format_addHeader(result, s_header,l_header):\n",
    "        output.append([each[i] for i in [0,11,3,4,6]])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date',\n",
       "  'Surface Temperature (Celcius)',\n",
       "  'Air Temperature(Celcius)',\n",
       "  'Relative Humidity',\n",
       "  'Max Wind Speed'],\n",
       " ['2017-03-08', '48', '21', '51.7', '13.0'],\n",
       " ['2017-03-09', '41', '23', '53.4', '11.1'],\n",
       " ['2017-03-10', '105', '19', '60.2', '12.0'],\n",
       " ['2017-03-10', '55', '19', '60.2', '12.0'],\n",
       " ['2017-03-12', '71', '21', '58.1', '11.1'],\n",
       " ['2017-03-14', '72', '25', '58.3', '13.0'],\n",
       " ['2017-03-14', '65', '25', '58.3', '13.0'],\n",
       " ['2017-03-14', '75', '25', '58.3', '13.0'],\n",
       " ['2017-03-15', '50', '28', '58.4', '18.1']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get surface temperature (°C), air temperature (°C), relative humidity and maximum wind speed.\n",
    "result = HBPP_join(c_records, f_records, 4)\n",
    "format_onSameday(result, c_header, f_header)[:10]\n",
    "# For better demonstration, only 10 rows displayed \n",
    "# if you want to display more, delete the sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write an algorithm to find datetime, air temperature (°C), surface temperature (°C) and confidence when the confidence is between 80 and 100. Justify your choice of the data partition technique and join technique you have used.\n",
    "\n",
    "**Solution & Reasoning**:  \n",
    "The Algorithms I use is the Disjoint Partitioning-based Parallel Join, which is range partitioning plus hash-based join. As the question ask for searching record by range, I definitely choose range partitioning as it return the range I want, and additionaly, again, hash is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_onConfidence(result,s_header, l_header):\n",
    "    '''\n",
    "    This function are built for DPP_join_range(), adding\n",
    "    purpose heading and extracting corresponding data.\n",
    "    \n",
    "    Arguments:\n",
    "    result -- DPP_join_range() result\n",
    "    s_header -- small table header\n",
    "    l_header -- large table header\n",
    "    \n",
    "    Return:\n",
    "    A proper output\n",
    "    '''\n",
    "    output = []\n",
    "    for each in format_addHeader(result, s_header,l_header):\n",
    "        output.append([each[i] for i in [1,3,11,13]])\n",
    "    return output\n",
    "\n",
    "def DPP_join_range(small_T, large_T, query, n, col_l):\n",
    "    '''\n",
    "    The Disjoint Partitioning-based Parallel Join\n",
    "    This function performs hash-based join with range-partitioning.\n",
    "    Multiple processors are applied for achieving parallelism.\n",
    "    \n",
    "    Arguments:\n",
    "    s_header -- small table header\n",
    "    l_header -- large table header\n",
    "    query -- str. Query format - 'lower bound,upper bound'. \n",
    "             input error tolerated but must starts with numeric.\n",
    "             RegEx is applied.\n",
    "    n: int. The number of parallel processors \n",
    "    col: int. User specified the index of column according to the \n",
    "           data.\n",
    "    \n",
    "    Return:\n",
    "    a list of lists of all desired result if found.\n",
    "    '''\n",
    "    #in this assignment: small_T = c_records, large_T = f_records\n",
    "    #validate input\n",
    "    val_ptn = r'^(\\d{1,3}),\\s*?(\\d{1,3})'\n",
    "    match_str = re.match(val_ptn, query)\n",
    "    #initialise pool\n",
    "    pool = Pool(processes = n)\n",
    "    # Get the range result\n",
    "    db_l = p_range_search_by(large_T, query, n, col_l)\n",
    "    # partition the large one\n",
    "    db_l_partitoned = rr_partition(db_l,n)\n",
    "    output = []\n",
    "    # do parallel\n",
    "    for db_set in db_l_partitoned:\n",
    "        each = pool.apply_async(HB_join, args=(small_T, db_set)).get()\n",
    "        if each:\n",
    "            for i in each:\n",
    "                output.append(i)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Datetime',\n",
       "  'Air Temperature(Celcius)',\n",
       "  'Surface Temperature (Celcius)',\n",
       "  'Confidence'],\n",
       " ['2017-03-07T04:16:10', '19', '64', '88'],\n",
       " ['2017-03-09T13:23:40', '23', '41', '86'],\n",
       " ['2017-03-12T04:27:20', '21', '98', '85'],\n",
       " ['2017-03-12T04:33:50', '21', '105', '94'],\n",
       " ['2017-03-14T04:14:40', '25', '55', '81'],\n",
       " ['2017-03-17T04:45:10', '18', '59', '84'],\n",
       " ['2017-03-18T03:50:40', '21', '80', '97'],\n",
       " ['2017-03-19T04:39:50', '24', '60', '85'],\n",
       " ['2017-03-25T03:58:10', '23', '75', '95']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find datetime, air temperature (°C), surface temperature (°C) and confidence\n",
    "result = DPP_join_range(c_records, f_records,'80,100', 4, 6)\n",
    "format_onConfidence(result, c_header, f_header)[:10]\n",
    "# For better demonstration, only 10 rows displayed \n",
    "# if you want to display more, delete the sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 3: Parallel Sort\n",
    "1. Write an algorithm to sort fire data based on surface temperature (°C) in a ascending order. Justify your choice of the data partition technique and sorting technique you have used.\n",
    "\n",
    "**Solution & Reasoning**:  \n",
    "Speaking of the **Big-O** theory, the Merge-sort is the best choice in Sorting algorithms. It performs $O(nlog_{2}n)$ in worst case and $\\Omega(nlog_{2}n)$ in best case. On the other hand, as K-way merge performs $O(nlog_{2}k)$ in running time, it is another best choice too. Hence, merge-sort for internal sorting, merge-all for final merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(left,right,col):\n",
    "    \"\"\" \n",
    "    The sub-fuction of mergesort(data,col)\n",
    "    It merges two arrays which has length shorter equal 1 in a sorted way\n",
    "    \n",
    "    Arguments:\n",
    "    left -- a left-half sublist of another list\n",
    "    right -- a right-half sublist of another list\n",
    "    col -- The column index of data which users specified.\n",
    "    \n",
    "    Return:\n",
    "    A list which have been sorted, and merged by the left-half and right-half.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    while len(left) != 0 and len(right) != 0:\n",
    "        if int(left[0][col]) < int(right[0][col]):\n",
    "            #if the last one in left less than right one\n",
    "            # save it to the result\n",
    "            result.append(left[0])\n",
    "            #remove it from left\n",
    "            left.remove(left[0])\n",
    "        else:\n",
    "            #else do the opposite\n",
    "            result.append(right[0])\n",
    "            right.remove(right[0])\n",
    "    #Check if either has run out of record.\n",
    "    if len(left) == 0:\n",
    "        # no more record on left\n",
    "        # append the rest\n",
    "        result += right\n",
    "    else:\n",
    "        #no more record on right\n",
    "        result += left\n",
    "    return result\n",
    "\n",
    "def mergesort(data,col):\n",
    "    '''\n",
    "    The Merge-sort Function to recursively sort an array\n",
    "    by using merge sort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "    data -- a list of lists object\n",
    "    col -- The column index of data which users specified.\n",
    "    '''\n",
    "    if len(data) <= 1:\n",
    "        return data\n",
    "    else:\n",
    "        middle = len(data)//2\n",
    "        # recusive call on mergesort to sort the list\n",
    "        left_half = mergesort(data[:middle],col)\n",
    "        right_half = mergesort(data[middle:],col)\n",
    "        #merge 2 halves at final phase\n",
    "        return merge(left_half,right_half,col)\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records,col):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    Arguments:\n",
    "    records -- the input record set in format [[],[],[]]\n",
    "    col -- user specified col, the value of that col has to be numeric, or numeric str\n",
    "    \n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    min_val = int(records[0][col])# records = [[],[],[]]\n",
    "    for i, record in enumerate(records):\n",
    "        if int(record[col]) < min_val:  \n",
    "            index = i\n",
    "            min_val = int(record[col])\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets,col):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "    col -- user specified col, the value of that col has to be numeric, or numeric str\n",
    "    \n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \n",
    "    \"\"\"\n",
    "    # input: records_sets = [[[r1],[r2]],[[r3],[r4]],[[r5],[r6]]]\n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    # It plays more like a controller in the following internal loop\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        # Populate 0s as the same as the number of record the record_sets have\n",
    "        indexes.append(0)\n",
    "    # For storing result\n",
    "    result = []  \n",
    "    # the merging unit (i.e. # of the given buffers)\n",
    "    sub = []\n",
    "    while(True):\n",
    "        sub = [] # initialise the merging unit\n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                sub.append([sys.maxsize]*9)\n",
    "            else:\n",
    "                sub.append(record_sets[i][indexes[i]])  \n",
    "                \n",
    "        # find the smallest record \n",
    "        smallest = find_min(sub,col)\n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(sub[smallest] == [sys.maxsize]*9):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result\n",
    "\n",
    "# The serial sorting method\n",
    "def serial_sorting(dataset, buffer_size, col):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "    col -- user specified col, the value of that col has to be numeric, or numeric str\n",
    "    \n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    result = []\n",
    "    sorted_set = []\n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = mergesort(subset,col) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = mergesort(subset,col) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # Merge Phase\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset,col)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset,col)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def parallel_merge_all_sorting(dataset, n_processor, buffer_size, col):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge-all sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set, \n",
    "                   defaultly equals 6\n",
    "    col -- user specified col, the value of that col has to be numeric, or numeric str\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply(serial_sorting, [s, buffer_size, col]))\n",
    "    pool.close()\n",
    "    # ---- Final merge phase ----\n",
    "    result = k_way_merge(sorted_set,col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['312',\n",
       "  '2017-07-02',\n",
       "  '2017-07-02T04:28:42',\n",
       "  '302.0',\n",
       "  '28',\n",
       "  '10.7',\n",
       "  '50',\n",
       "  '-37.885999999999996',\n",
       "  '147.207'],\n",
       " ['311',\n",
       "  '2017-07-02',\n",
       "  '2017-07-02T04:28:42',\n",
       "  '302.0',\n",
       "  '28',\n",
       "  '10.7',\n",
       "  '50',\n",
       "  '-37.885999999999996',\n",
       "  '147.207'],\n",
       " ['101',\n",
       "  '2017-11-11',\n",
       "  '2017-11-11T15:08:00',\n",
       "  '302.7',\n",
       "  '29',\n",
       "  '18.8',\n",
       "  '51',\n",
       "  '-36.943000000000005',\n",
       "  '143.286'],\n",
       " ['313',\n",
       "  '2017-07-01',\n",
       "  '2017-07-01T13:11:41',\n",
       "  '303.1',\n",
       "  '29',\n",
       "  '16.1',\n",
       "  '53',\n",
       "  '-37.062',\n",
       "  '141.373'],\n",
       " ['186',\n",
       "  '2017-10-02',\n",
       "  '2017-10-02T23:44:31',\n",
       "  '302.2',\n",
       "  '29',\n",
       "  '10.9',\n",
       "  '50',\n",
       "  '-37.466',\n",
       "  '148.1'],\n",
       " ['314',\n",
       "  '2017-07-01',\n",
       "  '2017-07-01T13:11:41',\n",
       "  '303.1',\n",
       "  '29',\n",
       "  '16.1',\n",
       "  '53',\n",
       "  '-37.062',\n",
       "  '141.373'],\n",
       " ['185',\n",
       "  '2017-10-03',\n",
       "  '2017-10-03T01:22:44',\n",
       "  '305.1',\n",
       "  '31',\n",
       "  '41.2',\n",
       "  '54',\n",
       "  '-37.227',\n",
       "  '141.14600000000002'],\n",
       " ['47',\n",
       "  '2017-11-30',\n",
       "  '2017-11-30T15:38:32',\n",
       "  '304.5',\n",
       "  '31',\n",
       "  '14.1',\n",
       "  '61',\n",
       "  '-37.38',\n",
       "  '149.334'],\n",
       " ['316',\n",
       "  '2017-07-01',\n",
       "  '2017-07-01T03:46:08',\n",
       "  '305.3',\n",
       "  '32',\n",
       "  '25.7',\n",
       "  '61',\n",
       "  '-36.779',\n",
       "  '146.108'],\n",
       " ['5',\n",
       "  '2017-12-24',\n",
       "  '2017-12-24T13:12:01',\n",
       "  '305.6',\n",
       "  '32',\n",
       "  '11.8',\n",
       "  '65',\n",
       "  '-35.646',\n",
       "  '142.282']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#surface temperature (°C) is on col=4\n",
    "parallel_merge_all_sorting(f_records,4,6,4)[:10]\n",
    "# For better demonstration, only 10 rows displayed \n",
    "# if you want to display more, delete the sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Task 4: Parallel Group-By\n",
    "#### 1. Write an algorithm to get the number of fire in each day. You are required to only display total number of fire and the date in the output. Justify your choice of the data partition technique if any.\n",
    "\n",
    "**Solution & Reasoning**:  \n",
    "I choose Round-Robin partitioning and merge-all GroupBy in this section.\n",
    "As respects usability, I make it capable to do GroupBy and calculate Avg. Just by two arguments to seperate different functions, very convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby(data, col, col_avg=None):\n",
    "    \"\"\"\n",
    "     Peform a local groupby method\n",
    "     \n",
    "     Arguments:\n",
    "     data -- entire record set to be merged\n",
    "     col -- the index of attribute to group by\n",
    "            if col is a list\n",
    "     Return:\n",
    "     return -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    # use as a counter\n",
    "    dic = {}\n",
    "    for i, record in enumerate(data):\n",
    "        key = record[col]\n",
    "        \n",
    "        if key not in dic:\n",
    "            if col_avg:\n",
    "                dic[key] = [0,0]\n",
    "            else:\n",
    "                dic[key] = 0\n",
    "                \n",
    "        if col_avg:\n",
    "            dic[key][0] += int(record[col_avg]) \n",
    "            dic[key][1] +=1\n",
    "        else:\n",
    "            dic[key] += 1\n",
    "    return dic\n",
    "\n",
    "def parallel_merge_all_groupby(data, n, col, col_avg=None):\n",
    "    \"\"\"\n",
    "    <Reusable Function>\n",
    "    Perform a parallel merge_all groupby method\n",
    "    Or perform a average\n",
    "\n",
    "    Arguments:\n",
    "    data -- List. entire record set to be merged\n",
    "    n -- int. number of processors\n",
    "    col -- int. the index of attribute to group by\n",
    "    col_avg -- int. the index of attribute to AVG, default is None\n",
    "    \n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    result_average -- the averaged record dictionary according to \n",
    "                      the group_by attribute index and the Avg columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # do Round-Robin partitioning \n",
    "    db = rr_partition(data,n)\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = Pool(processes = n)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for db_set in db:\n",
    "        # call the local aggregation method\n",
    "        if col_avg:\n",
    "            local_result.append(pool.apply(local_groupby, [db_set, col, col_avg]))\n",
    "        else:    \n",
    "            local_result.append(pool.apply(local_groupby, [db_set, col]))\n",
    "    pool.close()\n",
    "\n",
    "    # For aggregate\n",
    "    result = {}\n",
    "    for i, element in enumerate(local_result):\n",
    "        for key, val in element.items():\n",
    "            if key not in result:\n",
    "                if col_avg:\n",
    "                    #initialise for average\n",
    "                    result[key] = [0,0]\n",
    "                else:\n",
    "                    #initialise for sum\n",
    "                    result[key] = 0\n",
    "            if col_avg:\n",
    "                # the sum of value\n",
    "                result[key][0] += val[0]\n",
    "                # the counter of record\n",
    "                result[key][1] += val[1]\n",
    "            else:\n",
    "                result[key] += val\n",
    "    \n",
    "    # For average\n",
    "    if col_avg:\n",
    "        result_average = {}\n",
    "        for key, val in result.items():\n",
    "            result_average[key] = val[0]/val[1]\n",
    "        return result_average\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2017-03-06': 2,\n",
       " '2017-03-07': 1,\n",
       " '2017-03-08': 2,\n",
       " '2017-03-09': 3,\n",
       " '2017-03-10': 8,\n",
       " '2017-03-12': 5,\n",
       " '2017-03-13': 2,\n",
       " '2017-03-14': 10,\n",
       " '2017-03-15': 7,\n",
       " '2017-03-17': 6,\n",
       " '2017-03-18': 3,\n",
       " '2017-03-19': 21,\n",
       " '2017-03-24': 2,\n",
       " '2017-03-25': 13,\n",
       " '2017-03-26': 17,\n",
       " '2017-03-28': 54,\n",
       " '2017-03-29': 1,\n",
       " '2017-03-31': 22,\n",
       " '2017-04-01': 7,\n",
       " '2017-04-02': 5,\n",
       " '2017-04-03': 72,\n",
       " '2017-04-04': 89,\n",
       " '2017-04-05': 49,\n",
       " '2017-04-06': 118,\n",
       " '2017-04-07': 39,\n",
       " '2017-04-08': 20,\n",
       " '2017-04-11': 24,\n",
       " '2017-04-12': 69,\n",
       " '2017-04-13': 357,\n",
       " '2017-04-14': 18,\n",
       " '2017-04-15': 69,\n",
       " '2017-04-16': 18,\n",
       " '2017-04-17': 38,\n",
       " '2017-04-18': 325,\n",
       " '2017-04-19': 50,\n",
       " '2017-04-20': 31,\n",
       " '2017-04-22': 2,\n",
       " '2017-04-23': 19,\n",
       " '2017-04-24': 8,\n",
       " '2017-04-25': 3,\n",
       " '2017-04-26': 1,\n",
       " '2017-04-29': 3,\n",
       " '2017-05-01': 20,\n",
       " '2017-05-02': 10,\n",
       " '2017-05-03': 64,\n",
       " '2017-05-04': 135,\n",
       " '2017-05-05': 31,\n",
       " '2017-05-06': 17,\n",
       " '2017-05-07': 3,\n",
       " '2017-05-08': 24,\n",
       " '2017-05-09': 13,\n",
       " '2017-05-10': 114,\n",
       " '2017-05-11': 19,\n",
       " '2017-05-12': 10,\n",
       " '2017-05-13': 54,\n",
       " '2017-05-14': 1,\n",
       " '2017-05-15': 102,\n",
       " '2017-05-16': 3,\n",
       " '2017-05-17': 1,\n",
       " '2017-05-18': 7,\n",
       " '2017-05-22': 33,\n",
       " '2017-05-23': 5,\n",
       " '2017-05-24': 3,\n",
       " '2017-05-26': 4,\n",
       " '2017-06-01': 2,\n",
       " '2017-06-02': 11,\n",
       " '2017-06-03': 2,\n",
       " '2017-06-04': 9,\n",
       " '2017-06-07': 14,\n",
       " '2017-06-09': 3,\n",
       " '2017-06-11': 2,\n",
       " '2017-06-13': 1,\n",
       " '2017-06-14': 4,\n",
       " '2017-06-16': 2,\n",
       " '2017-06-18': 2,\n",
       " '2017-06-20': 6,\n",
       " '2017-06-22': 1,\n",
       " '2017-06-30': 6,\n",
       " '2017-07-01': 4,\n",
       " '2017-07-02': 8,\n",
       " '2017-07-04': 1,\n",
       " '2017-07-05': 1,\n",
       " '2017-07-06': 3,\n",
       " '2017-07-14': 2,\n",
       " '2017-07-29': 2,\n",
       " '2017-07-31': 2,\n",
       " '2017-08-01': 2,\n",
       " '2017-08-02': 2,\n",
       " '2017-08-05': 1,\n",
       " '2017-08-10': 1,\n",
       " '2017-08-13': 9,\n",
       " '2017-08-14': 5,\n",
       " '2017-09-10': 4,\n",
       " '2017-09-20': 5,\n",
       " '2017-09-21': 2,\n",
       " '2017-09-22': 1,\n",
       " '2017-09-23': 23,\n",
       " '2017-09-24': 28,\n",
       " '2017-09-26': 1,\n",
       " '2017-09-27': 7,\n",
       " '2017-09-29': 2,\n",
       " '2017-10-01': 8,\n",
       " '2017-10-02': 7,\n",
       " '2017-10-03': 18,\n",
       " '2017-10-04': 5,\n",
       " '2017-10-06': 2,\n",
       " '2017-10-07': 1,\n",
       " '2017-10-08': 1,\n",
       " '2017-10-09': 1,\n",
       " '2017-10-10': 3,\n",
       " '2017-10-15': 3,\n",
       " '2017-10-16': 1,\n",
       " '2017-10-17': 5,\n",
       " '2017-10-18': 6,\n",
       " '2017-10-20': 3,\n",
       " '2017-10-21': 4,\n",
       " '2017-10-23': 1,\n",
       " '2017-10-26': 5,\n",
       " '2017-10-27': 5,\n",
       " '2017-10-28': 1,\n",
       " '2017-11-05': 4,\n",
       " '2017-11-08': 2,\n",
       " '2017-11-09': 10,\n",
       " '2017-11-11': 4,\n",
       " '2017-11-12': 5,\n",
       " '2017-11-13': 5,\n",
       " '2017-11-14': 3,\n",
       " '2017-11-21': 1,\n",
       " '2017-11-22': 2,\n",
       " '2017-11-23': 5,\n",
       " '2017-11-28': 1,\n",
       " '2017-11-29': 8,\n",
       " '2017-11-30': 31,\n",
       " '2017-12-08': 5,\n",
       " '2017-12-09': 4,\n",
       " '2017-12-10': 2,\n",
       " '2017-12-12': 1,\n",
       " '2017-12-13': 1,\n",
       " '2017-12-14': 1,\n",
       " '2017-12-15': 4,\n",
       " '2017-12-16': 15,\n",
       " '2017-12-21': 1,\n",
       " '2017-12-24': 1,\n",
       " '2017-12-25': 1,\n",
       " '2017-12-27': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_merge_all_groupby(f_records,4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write an algorithm to find the average surface temperature (°C) for each day. You are required to only display average surface temperature (°C) and the date in the output. Justify your choice of the data partition technique if any.\n",
    "**Solution & Reasoning**:  \n",
    "Reuse the dedicatedly bulit function `parallel_merge_all_groupby` by given the col_avg to do the AVG function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2017-03-06': 60.5,\n",
       " '2017-03-07': 64.0,\n",
       " '2017-03-08': 51.5,\n",
       " '2017-03-09': 46.666666666666664,\n",
       " '2017-03-10': 69.375,\n",
       " '2017-03-12': 88.2,\n",
       " '2017-03-13': 38.5,\n",
       " '2017-03-14': 65.6,\n",
       " '2017-03-15': 46.0,\n",
       " '2017-03-17': 59.5,\n",
       " '2017-03-18': 79.33333333333333,\n",
       " '2017-03-19': 65.57142857142857,\n",
       " '2017-03-24': 49.0,\n",
       " '2017-03-25': 66.0,\n",
       " '2017-03-26': 56.88235294117647,\n",
       " '2017-03-28': 60.925925925925924,\n",
       " '2017-03-29': 51.0,\n",
       " '2017-03-31': 48.72727272727273,\n",
       " '2017-04-01': 46.714285714285715,\n",
       " '2017-04-02': 45.2,\n",
       " '2017-04-03': 58.44444444444444,\n",
       " '2017-04-04': 62.57303370786517,\n",
       " '2017-04-05': 53.142857142857146,\n",
       " '2017-04-06': 61.71186440677966,\n",
       " '2017-04-07': 50.69230769230769,\n",
       " '2017-04-08': 60.75,\n",
       " '2017-04-11': 46.291666666666664,\n",
       " '2017-04-12': 52.69565217391305,\n",
       " '2017-04-13': 58.57983193277311,\n",
       " '2017-04-14': 61.94444444444444,\n",
       " '2017-04-15': 59.57971014492754,\n",
       " '2017-04-16': 48.72222222222222,\n",
       " '2017-04-17': 50.921052631578945,\n",
       " '2017-04-18': 53.36615384615384,\n",
       " '2017-04-19': 54.16,\n",
       " '2017-04-20': 56.58064516129032,\n",
       " '2017-04-22': 54.5,\n",
       " '2017-04-23': 53.89473684210526,\n",
       " '2017-04-24': 59.375,\n",
       " '2017-04-25': 48.666666666666664,\n",
       " '2017-04-26': 34.0,\n",
       " '2017-04-29': 63.0,\n",
       " '2017-05-01': 68.4,\n",
       " '2017-05-02': 55.6,\n",
       " '2017-05-03': 56.796875,\n",
       " '2017-05-04': 56.80740740740741,\n",
       " '2017-05-05': 51.70967741935484,\n",
       " '2017-05-06': 57.529411764705884,\n",
       " '2017-05-07': 50.333333333333336,\n",
       " '2017-05-08': 56.291666666666664,\n",
       " '2017-05-09': 42.46153846153846,\n",
       " '2017-05-10': 52.86842105263158,\n",
       " '2017-05-11': 59.36842105263158,\n",
       " '2017-05-12': 51.5,\n",
       " '2017-05-13': 58.611111111111114,\n",
       " '2017-05-14': 49.0,\n",
       " '2017-05-15': 53.950980392156865,\n",
       " '2017-05-16': 39.666666666666664,\n",
       " '2017-05-17': 52.0,\n",
       " '2017-05-18': 44.142857142857146,\n",
       " '2017-05-22': 54.484848484848484,\n",
       " '2017-05-23': 51.2,\n",
       " '2017-05-24': 40.333333333333336,\n",
       " '2017-05-26': 49.5,\n",
       " '2017-06-01': 54.0,\n",
       " '2017-06-02': 47.72727272727273,\n",
       " '2017-06-03': 47.0,\n",
       " '2017-06-04': 52.22222222222222,\n",
       " '2017-06-07': 51.857142857142854,\n",
       " '2017-06-09': 49.0,\n",
       " '2017-06-11': 41.5,\n",
       " '2017-06-13': 41.0,\n",
       " '2017-06-14': 53.25,\n",
       " '2017-06-16': 42.5,\n",
       " '2017-06-18': 42.0,\n",
       " '2017-06-20': 71.16666666666667,\n",
       " '2017-06-22': 46.0,\n",
       " '2017-06-30': 44.333333333333336,\n",
       " '2017-07-01': 30.5,\n",
       " '2017-07-02': 43.5,\n",
       " '2017-07-04': 40.0,\n",
       " '2017-07-05': 45.0,\n",
       " '2017-07-06': 56.0,\n",
       " '2017-07-14': 41.5,\n",
       " '2017-07-29': 48.0,\n",
       " '2017-07-31': 47.0,\n",
       " '2017-08-01': 58.0,\n",
       " '2017-08-02': 63.5,\n",
       " '2017-08-05': 40.0,\n",
       " '2017-08-10': 63.0,\n",
       " '2017-08-13': 49.0,\n",
       " '2017-08-14': 40.8,\n",
       " '2017-09-10': 56.5,\n",
       " '2017-09-20': 63.6,\n",
       " '2017-09-21': 40.5,\n",
       " '2017-09-22': 45.0,\n",
       " '2017-09-23': 52.73913043478261,\n",
       " '2017-09-24': 53.57142857142857,\n",
       " '2017-09-26': 33.0,\n",
       " '2017-09-27': 49.714285714285715,\n",
       " '2017-09-29': 43.0,\n",
       " '2017-10-01': 48.25,\n",
       " '2017-10-02': 43.57142857142857,\n",
       " '2017-10-03': 50.0,\n",
       " '2017-10-04': 49.0,\n",
       " '2017-10-06': 44.0,\n",
       " '2017-10-07': 42.0,\n",
       " '2017-10-08': 41.0,\n",
       " '2017-10-09': 44.0,\n",
       " '2017-10-10': 53.333333333333336,\n",
       " '2017-10-15': 72.66666666666667,\n",
       " '2017-10-16': 36.0,\n",
       " '2017-10-17': 51.6,\n",
       " '2017-10-18': 52.166666666666664,\n",
       " '2017-10-20': 50.0,\n",
       " '2017-10-21': 51.25,\n",
       " '2017-10-23': 38.0,\n",
       " '2017-10-26': 44.6,\n",
       " '2017-10-27': 50.4,\n",
       " '2017-10-28': 56.0,\n",
       " '2017-11-05': 58.5,\n",
       " '2017-11-08': 45.5,\n",
       " '2017-11-09': 61.3,\n",
       " '2017-11-11': 46.25,\n",
       " '2017-11-12': 53.0,\n",
       " '2017-11-13': 47.0,\n",
       " '2017-11-14': 52.0,\n",
       " '2017-11-21': 59.0,\n",
       " '2017-11-22': 61.5,\n",
       " '2017-11-23': 58.8,\n",
       " '2017-11-28': 42.0,\n",
       " '2017-11-29': 60.625,\n",
       " '2017-11-30': 52.41935483870968,\n",
       " '2017-12-08': 50.6,\n",
       " '2017-12-09': 58.25,\n",
       " '2017-12-10': 46.0,\n",
       " '2017-12-12': 44.0,\n",
       " '2017-12-13': 60.0,\n",
       " '2017-12-14': 70.0,\n",
       " '2017-12-15': 39.0,\n",
       " '2017-12-16': 57.8,\n",
       " '2017-12-21': 46.0,\n",
       " '2017-12-24': 32.0,\n",
       " '2017-12-25': 54.0,\n",
       " '2017-12-27': 62.75}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# surface temperature (°C) column index is 4\n",
    "parallel_merge_all_groupby(f_records,4,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Task 5: Parallel Group-By Join\n",
    "#### Write an algorithm to find the average surface temperature (°C) for each weather station.You are required to only display average surface temperature (°C) and the station in the output. Justify your choice of the data partition and join technique.\n",
    "\n",
    "**Solution & Reasoning**: \n",
    "Entirely reuse the `HBPP_join()` and `parallel_merge_all_groupby()` to yield the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'948701': 56.06938603868797, '948702': 52.148275862068964}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First Join two tables on `Date`, it yield a big table\n",
    "result = HBPP_join(c_records, f_records, 1)\n",
    "# GoupBy column 'Station'(index 2) \n",
    "# and Avg on surface temperature (°C)(index 11)\n",
    "# Using 4 processors\n",
    "parallel_merge_all_groupby(result,4,2,11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
