{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "D = []\n",
    "def case_generator(n):\n",
    "    for i in range(0,n):\n",
    "        if i <100:\n",
    "            D.append(randint(1,100))\n",
    "        else:\n",
    "            break\n",
    "case_generator(30)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Partitioning\n",
    "## 1.1 Round-Robin Partioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import *\n",
    "D = [55,30,68,39,1,\n",
    "     4,49,90,34,76, \n",
    "     82,56,31,25,78, \n",
    "     56,38,32,88,9, \n",
    "     44,98,11,70,66, \n",
    "     89,99,22,23,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_partition(data, n):\n",
    "    '''\n",
    "Round-robin data partition:\n",
    "the simplest data partitioning method;\n",
    "each record in turn is allocated to a processing element (simply processor);\n",
    "Distributes the data evenly among all processors;\n",
    "Known as “equal-partitioning”.\n",
    "    '''\n",
    "    result = []\n",
    "    #Creating partition n as list of lists\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    #enumerate data and evenly allocation to according partition\n",
    "    for index, element in enumerate(data):\n",
    "        index_bin = (int)(index % n) \n",
    "        #Trick: e.g. 1%4=1; 2%4=2; 3%4=3; 4%4=0\n",
    "        result[index_bin].append(element)\n",
    "    return result\n",
    "\n",
    "rr_partition(D,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hash Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Hash data partition:\n",
    "    Partitioning based on a particular attribute using \n",
    "    a hash function. all records within a partition have\n",
    "    the same hash value.\n",
    "'''\n",
    "def hash_func(x, n):\n",
    "    return x%n\n",
    "\n",
    "def h_partition(data, n):\n",
    "    dic = {} #Using hash value as the key of the item\n",
    "    for element in data:\n",
    "        h_value = hash_func(element, n)\n",
    "        if (h_value in dic.keys()):\n",
    "            items = dic[h_value]\n",
    "            items.add(element) \n",
    "            # Mark: using Set\n",
    "            dic[h_value] = items\n",
    "        else:\n",
    "            '''\n",
    "            Note: using Set can avoid duplicated data.\n",
    "            Searching data in a dict with hash value is O(1).\n",
    "            Without duplicated record, to some extent, the\n",
    "            efficiency can be improved.\n",
    "            '''\n",
    "            tmp = set()\n",
    "            tmp.update({element})\n",
    "            dic[h_value] = tmp\n",
    "    return dic\n",
    "print(D)\n",
    "results = h_partition(D, 3)\n",
    "for each in results:\n",
    "    print(results[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_partition_listVersion(data, n):\n",
    "    dic = {} #Using hash value as the key of the item\n",
    "    for element in data:\n",
    "        h_value = hash_func(element, n)\n",
    "        if (h_value in dic.keys()):\n",
    "            items = dic[h_value]\n",
    "            items.append(element) \n",
    "            # Mark: List Question\n",
    "            dic[h_value] = items\n",
    "        else:\n",
    "            '''\n",
    "            Note: Just curious why not using List\n",
    "            '''\n",
    "            tmp = []\n",
    "            tmp.append(element)\n",
    "            dic[h_value] = tmp\n",
    "    return dic\n",
    "print(D)\n",
    "h_partition_listVersion(D, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Range Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_partition(data, range_indices):\n",
    "    result = []\n",
    "    sorted_data = sorted(data)#replicate data\n",
    "    n_bin = len(range_indices)\n",
    "    for i in range(n_bin):\n",
    "        s = [x for x in sorted_data if x < range_indices[i]]\n",
    "        '''\n",
    "        Separate data by the range:\n",
    "        e.g, let a range to be [40,60], a list contain range(100)\n",
    "        this comprehensive list slice the [0, 40), [40,60)\n",
    "        '''\n",
    "        result.append(s)\n",
    "        sorted_data = sorted_data[len(s):]\n",
    "        '''\n",
    "        Qestion solved:\n",
    "        Error occur when data contain duplicated record\n",
    "        which accidentally rest at the edge of the range\n",
    "        #Original code:\n",
    "        #last_index = sorted_data.index(s[-1])\n",
    "        #sorted_data = sorted_data[last_index+1:]\n",
    "        ''' \n",
    "    result.append([x for x in sorted_data if x >= range_indices[-1]])\n",
    "    '''\n",
    "    The last line finish the slicing.[60,100]\n",
    "    '''\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_partition(D,[40,70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Random-Unequal Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ru_partition(data, n):\n",
    "    '''\n",
    "    Perform random-unequal data partitioning on data\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list \n",
    "    n -- the number of processors\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    #enumerate data and evenly allocation to according partition\n",
    "    for element in data:\n",
    "        # partition rules:\n",
    "        index_bin = int(sum([int(d) for d in str(element)]) % n)\n",
    "        result[index_bin].append(element)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ru_partition_setVersion(data, n):\n",
    "    '''\n",
    "    Perform random-unequal data partitioning on data\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list \n",
    "    n -- the number of processors\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    '''\n",
    "    result = {}\n",
    "    #enumerate data and evenly allocation to according partition\n",
    "    for element in data:\n",
    "        # partition rules:\n",
    "        index_bin = int(sum([int(d) for d in str(element)]) % n)\n",
    "        if index_bin in result.keys():\n",
    "            result[index_bin].add(element)\n",
    "        else:\n",
    "            result[index_bin] = {element}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_partition(D,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_partition(D,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_partition_setVersion(D,3)\n",
    "# Note: \n",
    "# 1. No duplicate data in set\n",
    "# 2. Data in set followed the ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Algorithms\n",
    "## 2.1 Linear Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_search(data, key):\n",
    "    '''\n",
    "    Linear Search(Exhaustive search)\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list or a numpy array \n",
    "    key -- an query record\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    '''\n",
    "    matched_record = 'Not Found' \n",
    "    position = -1 # not found position\n",
    "    ### START CODE HERE ### \n",
    "    for x in data:\n",
    "        if x == key: # If x is matched with key\n",
    "            matched_record = x \n",
    "            position = data.index(x) # Get the index of x \n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    return (position, matched_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = [1,2,4,2,5,1]\n",
    "linear_search(D, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Binary Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarySearch(alist, record):\n",
    "    \"\"\" \n",
    "    Perform binary search on data for the given key\n",
    "    Arguments:\n",
    "    alist -- an input dataset which is a list \n",
    "    record -- an query record\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    if alist:\n",
    "        print(alist)\n",
    "        data = sorted(alist)\n",
    "        while len(data)!=1:\n",
    "            mid = len(data)//2\n",
    "            if data[mid] == record:\n",
    "                return alist.index(record), record\n",
    "            elif data[mid] < record:\n",
    "                #go right, slice off the left-side\n",
    "                data = data[mid:]\n",
    "            else:\n",
    "                #go left, slice off the right-side\n",
    "                data = data[:mid]\n",
    "        if data[0] != record:\n",
    "            return -1,'Not Found'\n",
    "        else:\n",
    "            return alist.index(record),record\n",
    "    else:\n",
    "        print(\"Input Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binarySearch(D,31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parallel Search Algorithms\n",
    "## 3.1 Parallel Searching for Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel_search_exact(data, query, n_processor, m_partition, m_search):\n",
    "    \"\"\"\n",
    "    Perform parallel search for exact match on data for the given key\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list \n",
    "    query -- a query record \n",
    "    n_processor -- the number of parallel processors \n",
    "    m_partition -- a data partitioning method \n",
    "    m_search -- a search method\n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Pool: a Python method enabling parallel processing.\n",
    "    # We need to set the number of processes to n_processor,\n",
    "    # which means that the Pool class will only allow 'n_processor' processes \n",
    "    # running at the same time.\n",
    "    pool = Pool(processes = n_processor)\n",
    "    ### START CODE HERE ###\n",
    "    print(\"data partitioning:\" + str(m_partition.__name__)) \n",
    "    print(\"searching method:\" + str(m_search.__name__))\n",
    "    print(\"Query: \" + str(query))\n",
    "    if m_partition == range_partition: \n",
    "        # for range partitioning method \n",
    "        # Perform data partitioning:\n",
    "        # 2nd parameter is a list of maximum range values (3 ranges) \n",
    "        DD = m_partition(data, [40, 80]) \n",
    "        for d in DD: # Find the range that may contain the query\n",
    "            if query in d:\n",
    "                print(\"Found corresponding range:\")\n",
    "                print(d)\n",
    "                m = list(d) \n",
    "                result = pool.apply(m_search, [m, query]) \n",
    "                results.append(result) \n",
    "                break\n",
    "    elif m_partition == h_partition: \n",
    "        # for hash partitioning method \n",
    "        # Perform data partitioning first \n",
    "        DD = m_partition(data, n_processor)\n",
    "        print(DD)\n",
    "        # Each element in DD has a pair (hash key: records) \n",
    "        query_hash = hash_func(query, n_processor) \n",
    "        print(\"Query Hash value: \" + str(query_hash))\n",
    "        d = list(DD[query_hash]) \n",
    "        print(\"Found corresponding partition:\")\n",
    "        print(d)\n",
    "        result = pool.apply(m_search, [d, query]) \n",
    "        results.append(result)\n",
    "    else: # for round-robin or random-unequal partitioning method \n",
    "        # Perform data partitioning first\n",
    "        DD = m_partition(data, n_processor) \n",
    "        for d in DD: # Perform parallel search on all data partitions\n",
    "            result = pool.apply(m_search, [d, query])\n",
    "            #output = result.get() # if you use pool.apply_sync(), uncomment this. \n",
    "            #results.append(output) # if you use pool.apply_sync(), uncomment this. \n",
    "            results.append(result) # if you use pool.apply_sync(), comment out this.\n",
    "\n",
    "        \"\"\" \n",
    "        The method above 'pool.apply()' will lock the function \n",
    "        program until all a process is finished. Alternatively,\n",
    "        we can use the 'pool.apply_sync()' method to spawn one \n",
    "        process for each CPU core on your machine.\n",
    "        \"\"\" \n",
    "    ### END CODE HERE ### \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sorted(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round-robin partition, linear_search\n",
    "parallel_search_exact(data, 31, 3, rr_partition, linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round-robin partition, binary_search\n",
    "parallel_search_exact(data, 31, 3, rr_partition, binarySearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random-unequal partition, linear_search\n",
    "parallel_search_exact(data, 31, 3, ru_partition, linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random-unequal partition, binary_search \n",
    "parallel_search_exact(data, 31, 3, ru_partition, binarySearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash partition, linear_search \n",
    "parallel_search_exact(data, 31, 3, h_partition, linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash partition, binary_search \n",
    "parallel_search_exact(data, 31, 3, h_partition, binarySearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range partition, linear_search \n",
    "parallel_search_exact(data, 31, 3, range_partition,linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range partition, binary_search\n",
    "parallel_search_exact(data, 31, 3, range_partition,binarySearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Parallel Searching for Range Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Build a parallel search algorithm that uses the linear search\n",
    "algorithm (i.e. linear_search()) and is able to work with the\n",
    "hash partitioning method (i.e. h_partition()).\n",
    "'''\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    results = []\n",
    "    pool = Pool(processes = n_processor)\n",
    "    DD = h_partition(data, n_processor)\n",
    "    if query_range[0] > query_range[1]:\n",
    "        raise Exception(\"Input Error\")\n",
    "    else:\n",
    "        candid_list = [x for x in range(query_range[0],query_range[1]+1)]\n",
    "        for i in range(n_processor):\n",
    "            for query in candid_list:\n",
    "                result = pool.apply(linear_search, [list(DD[i]), query])\n",
    "                if result[0] != -1:\n",
    "                    print(\"Found \" + str(result[1]) \n",
    "                          + \"from hash: \" + str(i))\n",
    "                    results.append(result)\n",
    "    return results\n",
    "\n",
    "results = parallel_search_range(data, [30, 40], 3) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
