{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Reconstruct the Original Meeting Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title**: FIT5196_S1_2018_Assignment3_task1  \n",
    "**Student ID**: 27771539  \n",
    "**Student Name**: Chuangfu Xie  \n",
    "**Date**: 03/06/2018  \n",
    "**Develop-Env**: MacOS - Jupyter Notebook - Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pprint import pprint \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Parsing Topic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_id(topic):\n",
    "    ptn_topic_id = r'(\\w+)\\..*?\\.(.*)'\n",
    "    topic_attrib = list(topic.attrib.items())\n",
    "    if len(topic_attrib) == 2:\n",
    "        topic_id, desc = topic_attrib[0][1], topic_attrib[1][1]\n",
    "    else:\n",
    "        topic_id, desc = topic_attrib[0][1], 'None'\n",
    "    [(file_name, top_id)] = re.findall(ptn_topic_id, topic_id)\n",
    "    return top_id#, file_name, desc\n",
    "\n",
    "def get_top_type(ET_element_pointer):\n",
    "    '''\n",
    "    This function is for extract topic pointer (ET.Element)\n",
    "    '''\n",
    "    role, href = ET_element_pointer.attrib.items()\n",
    "    ptn = r'\\w+#id\\((.*)\\)'\n",
    "    return re.findall(ptn, href[1])[0] #str\n",
    "\n",
    "def get_child_data(ET_element_child):\n",
    "    '''\n",
    "    This function is for extracting child data\n",
    "    return {key:[value]}\n",
    "    '''\n",
    "    [(href,data)] = ET_element_child.attrib.items()\n",
    "    data_ptn = r'.*?\\.(\\w).*?\\((.*)\\)'\n",
    "    [(k,data)] = re.findall(data_ptn, data)\n",
    "    raw_data = data.split(')..id(')\n",
    "    words_ptn = r'.*?.\\.\\w\\.[a-zA-z]+(\\d*)'# modified mark: extract number only\n",
    "    data = []\n",
    "    for each in raw_data:\n",
    "        data.append(int(re.findall(words_ptn,each)[0]))# modified mark: int()\n",
    "    return k, data\n",
    "\n",
    "def parse_topic(topic):\n",
    "    results = {}\n",
    "    _id = get_top_id(topic)\n",
    "    #_type = get_top_type(topic[0])\n",
    "    if _id not in results:\n",
    "        results[_id] = []\n",
    "    \n",
    "    result = results[_id]\n",
    "    # extract data from topic\n",
    "    for i, e in enumerate(topic):\n",
    "        if i==0:\n",
    "            continue\n",
    "        k, data = get_child_data(e)\n",
    "        result.append((k,data))\n",
    "    return results\n",
    "\n",
    "def parse_element_list(element_list):\n",
    "    results = {}\n",
    "    _id = element_list.pop(0)\n",
    "    \n",
    "    if _id not in results:\n",
    "        results[_id] = []\n",
    "        \n",
    "    result = results[_id]\n",
    "    # extract data from topic\n",
    "    for i, element in enumerate(element_list):\n",
    "        if i==0:\n",
    "            continue\n",
    "        k, data = get_child_data(element)\n",
    "        result.append((k,data))\n",
    "    return results\n",
    "\n",
    "def parse_entire_tree(tree):\n",
    "    element_list = tree.getroot().getchildren()\n",
    "    topic_list = []\n",
    "    for topic in element_list:\n",
    "        if topic.findall('topic'):\n",
    "            _elemt_list = topic.getchildren()\n",
    "\n",
    "            elemt_list = [] # go for topic_ele_list_list\n",
    "            _id = get_top_id(topic)\n",
    "            elemt_list.append(_id)\n",
    "\n",
    "            inner_topic = [] # go for topic_in_file\n",
    "\n",
    "            for element in _elemt_list:\n",
    "                if element.tag == 'topic':\n",
    "                    t_dict = parse_topic(element)\n",
    "                    temp = [t_dict[key] for key in t_dict.keys()]\n",
    "                    topic_list += temp\n",
    "                    #pprint(temp)\n",
    "                else:\n",
    "                    elemt_list.append(element)\n",
    "            t_dict = parse_element_list(elemt_list)\n",
    "            temp = [t_dict[key] for key in t_dict.keys()]\n",
    "            topic_list += temp\n",
    "        else:\n",
    "            t_dict = parse_topic(topic)\n",
    "            temp = [t_dict[key] for key in list(t_dict.keys())]\n",
    "            topic_list += temp\n",
    "    return topic_list\n",
    "\n",
    "def parse_all_topic_files(mypath=None):\n",
    "    if not mypath:\n",
    "        mypath = './topics'\n",
    "    # load entire list of fires name from directory\n",
    "    file_list = sorted([f for f in listdir(mypath) if isfile(join(mypath, f))])\n",
    "    print(len(file_list), 'files in total from', mypath) # for checking\n",
    "    \n",
    "    entire_topics_dict = {}#initialise a dict for storing\n",
    "    ptn = r'(.*)\\.\\w+\\..*' # for extract filename, who(A,B,C,D)\n",
    "    f_cnt = 0 # success counter\n",
    "    \n",
    "    for file in file_list:\n",
    "        if re.findall(ptn, file):\n",
    "            fname = re.findall(ptn, file)[0]\n",
    "        path = mypath + '/' + file # construct the path\n",
    "        try:\n",
    "            tree = ET.parse(path)\n",
    "            result = parse_entire_tree(tree)\n",
    "            if fname not in entire_topics_dict:\n",
    "                entire_topics_dict[fname] = []\n",
    "            entire_topics_dict[fname] = parse_entire_tree(tree)\n",
    "            f_cnt += 1\n",
    "        except:\n",
    "            continue\n",
    "    #print(\"Successfully parse\", f_cnt, 'files.') # for checking\n",
    "    return entire_topics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing words files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word(words_list):\n",
    "    '''\n",
    "    : parase_word(words_list) \n",
    "    This function will parse single word file.\n",
    "    \n",
    "    Arguments:\n",
    "        > words_list: list. a list of ET.elements\n",
    "    Return:\n",
    "        > vocas: a list of all words(easy to extract via indexing)\n",
    "    '''\n",
    "    vocas = []\n",
    "    for word in words_list:\n",
    "        if 'type' in word.attrib:\n",
    "            if word.attrib['type']=='laugh':\n",
    "                vocas.append('(laugh)')\n",
    "            else:\n",
    "                vocas.append('_')\n",
    "        else:\n",
    "            vocas.append(word.text)\n",
    "    return vocas\n",
    "\n",
    "def parse_all_wordfiles(mypath=None):\n",
    "    '''\n",
    "    : parse_all_wordfiles(mypath=None)\n",
    "    This function is to parse all words files in target directory.\n",
    "    \n",
    "    Arguments:\n",
    "        > mypath: str. By default is None. The words directory path is './words'. \n",
    "                  Not need to input any argument to this function.\n",
    "    Return:\n",
    "        > entire_words_dict: dict. a dict of dict of all words files\n",
    "    \n",
    "    HELP - How to extract target words:\n",
    "    { 'filename1':{'A':[...],\n",
    "                   'B':[...],\n",
    "                   'C':[...],\n",
    "                   'D':[...],},\n",
    "      'filename2':{'A':[...],\n",
    "                   'B':[...],\n",
    "                   'C':[...],\n",
    "                   'D':[...],},\n",
    "         ...             \n",
    "    }\n",
    "    '''\n",
    "    if not mypath:\n",
    "        mypath = './words'\n",
    "    # load entire list of fires name from directory\n",
    "    file_list = sorted([f for f in listdir(mypath) if isfile(join(mypath, f))])\n",
    "    print(len(file_list), 'files in total from', mypath) # for checking\n",
    "    \n",
    "    entire_words_dict = {} #initialise a dict for storing\n",
    "    ptn = r'(.*)\\.(\\w)\\..*' # for extract filename, who(A,B,C,D)\n",
    "    f_cnt = 0 # success counter\n",
    "    for file in file_list:\n",
    "        fname, who = re.findall(ptn, file)[0]\n",
    "        if fname not in entire_words_dict:\n",
    "            entire_words_dict[fname] = {}\n",
    "        if who not in entire_words_dict[fname]:\n",
    "            entire_words_dict[fname][who] = []\n",
    "        path = mypath + '/' + file # construct the path\n",
    "        try:\n",
    "            words_tree = ET.parse(path) #load xml\n",
    "            words_elemt_list = words_tree.getroot().getchildren() #get all elements\n",
    "            vocas = parse_word(words_elemt_list)\n",
    "            entire_words_dict[fname][who] += vocas\n",
    "            f_cnt += 1\n",
    "        except:\n",
    "            print('missing:', who)\n",
    "            continue\n",
    "    #print(\"Successfully parse\", f_cnt, 'files.') # for checking\n",
    "    return entire_words_dict\n",
    "\n",
    "# Reminder: multi-processing to accelerate the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing segment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_segment(segments_list, target_dict):\n",
    "    for segment in segments_list:\n",
    "        # extract time-range\n",
    "        [_, chn, start, end] = segment.attrib.items()\n",
    "        key = (start[1], end[1]) \n",
    "        #print(key)\n",
    "\n",
    "        # extract word-range\n",
    "        [( _, raw )] = segment.getchildren()[0].attrib.items()\n",
    "        data_ptn = r'.*?\\.(\\w).*?\\((.*)\\)'\n",
    "        words_ptn = r'.*?.\\.\\w\\.[a-zA-z]+(\\d*)' # modified mark: extract number only\n",
    "        #words_ptn = r'.*?.\\.\\w\\.(.*)'\n",
    "        [(who, _range)] = re.findall(data_ptn, raw)\n",
    "        raw_range = _range.split(')..id(')\n",
    "        word_range = [who]\n",
    "        for each in raw_range:\n",
    "            word_range.append(int(re.findall(words_ptn,each)[0])) # modified mark: int()\n",
    "        # store in to dict\n",
    "        target_dict[key] = word_range\n",
    "    #print('Success:', len(list(segments_dict.keys()))) #for checking\n",
    "\n",
    "def parse_all_segmentfiles(mypath=None):\n",
    "    '''\n",
    "    : parse_all_segmentfiles(mypath=None)\n",
    "    This function is to parse all segment files in target directory.\n",
    "    \n",
    "    Arguments:\n",
    "        > mypath: str. By default is None. The words directory path is './segments'. \n",
    "                  Not need to input any argument to this function.\n",
    "    Return:\n",
    "        > entire_segment_dict: dict. a dict of dict of all segment files\n",
    "    \n",
    "    HELP - How to extract target segment:\n",
    "    { 'filename1':{(start, end)': ['A', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['B', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['C', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['D', word0, word15],\n",
    "                    ...\n",
    "                    },\n",
    "      'filename2':{(start, end)': ['A', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['B', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['C', word0, word15],\n",
    "                    ...,\n",
    "                   (start, end)': ['D', word0, word15],\n",
    "                    ...\n",
    "                    },\n",
    "         ...             \n",
    "    }\n",
    "    '''\n",
    "    if not mypath:\n",
    "        mypath = './segments'\n",
    "    file_list = sorted([f for f in listdir(mypath) if isfile(join(mypath, f))])\n",
    "    print(len(file_list), 'files in total from', mypath) # for checking\n",
    "    \n",
    "    entire_segments_dict = {} #initialise a dict for storing\n",
    "    ptn = r'(.*)\\.\\w\\..*' # for extract filename, who(A,B,C,D)\n",
    "    f_cnt = 0 # success counter\n",
    "\n",
    "    for file in file_list:\n",
    "        fname = re.findall(ptn, file)[0]\n",
    "        if fname not in entire_segments_dict:\n",
    "            entire_segments_dict[fname] = {}\n",
    "\n",
    "        path = mypath + '/' + file # construct the path\n",
    "        try:\n",
    "            segments_tree = ET.parse(path)\n",
    "            segments_elemt_list = segments_tree.getroot().getchildren()\n",
    "            #print('load',len(segments_list),'segments') #for checking\n",
    "            # get the dict updated\n",
    "            #target_dict = entire_segments_dict[fname]\n",
    "            parse_segment(segments_elemt_list, entire_segments_dict[fname])\n",
    "            f_cnt += 1\n",
    "        except:\n",
    "            print('missing:', fname)\n",
    "            continue\n",
    "    #print(\"Successfully parse\", f_cnt, 'files.') # for checking\n",
    "    return entire_segments_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other function for parsing to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_range(subtopic):\n",
    "    speaker_range = {}\n",
    "    for speaker in subtopic:\n",
    "        who, _range = speaker\n",
    "        if who not in speaker_range:\n",
    "            speaker_range[who] = []\n",
    "        speaker_range[who].append(_range[-1])\n",
    "        speaker_range[who].append(_range[0])\n",
    "    for each in speaker_range:\n",
    "        max_range = max(speaker_range[each])\n",
    "        min_range = min(speaker_range[each])\n",
    "        speaker_range[each] = [min_range,max_range]\n",
    "    return speaker_range\n",
    "\n",
    "def segment_in_topic(speaker_range, line_range, who):\n",
    "    '''\n",
    "    Arguemnts\n",
    "    > speaker_range: dict. speaker words range in a subtopic\n",
    "    > line_range: list. segment words range\n",
    "    > who: str. which speaker\n",
    "    '''\n",
    "    s_start, s_end = speaker_range[who]\n",
    "    s_range = range(s_start, s_end+1)\n",
    "    if line_range[0] in s_range and line_range[-1] in s_range:\n",
    "        return line_range\n",
    "    elif line_range[0] in s_range:\n",
    "        return [line_range[0], s_end]\n",
    "    elif line_range[-1] in s_range:\n",
    "        return [s_start, line_range[-1]]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_segments(top, segments_list):\n",
    "    new_segments_list = []\n",
    "    speaker_range = get_topic_range(top) \n",
    "    for segment in segments_list:\n",
    "        who, s_range = segment[0], segment[1:]\n",
    "        if who in speaker_range:\n",
    "            new_s_range = segment_in_topic(speaker_range, s_range, who)\n",
    "            if new_s_range:\n",
    "                if new_s_range == s_range:\n",
    "                    new_segments_list.append(segment)\n",
    "                else:\n",
    "                    # if tail got trim\n",
    "                    if s_range[-1] > new_s_range[-1] and s_range[0] == new_s_range[0]:\n",
    "                        suplm_segment = [who, new_s_range[-1]+1, s_range[-1]] #trim part\n",
    "                        new_segments_list.append([who,new_s_range[0],new_s_range[1]]) # updated\n",
    "                        new_segments_list.append(suplm_segment)\n",
    "\n",
    "                    # if head got trim\n",
    "                    if s_range[0] < new_s_range[0] and s_range[-1] == new_s_range[-1]:\n",
    "                        #suplm_segment = (who, [s_range[0], new_s_range[0]-1]) #trim part\n",
    "                        #new_segments_list.append(suplm_segment)\n",
    "                        new_segments_list.append([who,new_s_range[0],new_s_range[1]])\n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "    return new_segments_list\n",
    "\n",
    "def convert_to_txt(topics_list, all_words):\n",
    "    all_txt = []\n",
    "    for topic in topics_list:\n",
    "        topic_sentence_list = []\n",
    "        for line in topic:\n",
    "            sentence = ''\n",
    "            who, w_range = line[0], line[1:]\n",
    "            sentence += who + ': ' #for checking\n",
    "            if len(w_range) > 1:\n",
    "                s, e = w_range\n",
    "                for index in range(s,e+1):\n",
    "                    word = all_words[who][index]\n",
    "                    if word:\n",
    "                        sentence += word + ' '\n",
    "            else:\n",
    "                index = w_range[0]\n",
    "                word = all_words[who][index]\n",
    "                if word:\n",
    "                    sentence += word + ' '\n",
    "            topic_sentence_list.append(sentence)\n",
    "        topic_sentence_list.append('**********')\n",
    "        all_txt.append(topic_sentence_list)\n",
    "    return all_txt\n",
    "\n",
    "def write_file(file_txt, filename):\n",
    "    f = open('./txt_files/'+filename+'.txt','w')\n",
    "    for topic in file_txt:\n",
    "        for line in topic:\n",
    "            f.write(line+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#All topic\n",
    "all_topic_files = parse_all_topic_files()\n",
    "\n",
    "#All Words\n",
    "all_wordfiles = parse_all_wordfiles()\n",
    "test_words = all_wordfiles['ES2002a']\n",
    "\n",
    "#All segments\n",
    "all_segments = parse_all_segmentfiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "# Parse and Write to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = './topics'\n",
    "# load entire list of fires name from directory\n",
    "file_list = sorted([f for f in listdir(mypath) if isfile(join(mypath, f))])\n",
    "entire_words_dict = {} #initialise a dict for storing\n",
    "ptn = r'(.*)\\.\\w+\\..*' # for extract filename, who(A,B,C,D)\n",
    "filename_list = []\n",
    "\n",
    "for file in file_list:\n",
    "    fname = re.findall(ptn, file)[0]\n",
    "    filename_list.append(fname)\n",
    "\n",
    "for filename in filename_list:\n",
    "    try:\n",
    "        #load taget topic file\n",
    "        #filename = 'ES2002a'\n",
    "        topic_file = all_topic_files[filename]\n",
    "\n",
    "        #preprocess for segment\n",
    "        _temp = all_segments[filename]\n",
    "        segment_list = list(_temp.keys())\n",
    "        segment_list.sort(key=lambda x:float(x[0]))\n",
    "        scripts_list = [_temp[key] for key in segment_list]\n",
    "\n",
    "        new_topic_list = []\n",
    "        for subtopic in topic_file:\n",
    "            new_topic_list.append(preprocess_segments(subtopic, scripts_list))   \n",
    "\n",
    "        words_file = all_wordfiles[filename]#\n",
    "        all_txt = convert_to_txt(new_topic_list, words_file)\n",
    "        #pprint(all_txt)\n",
    "        write_file(all_txt, filename)\n",
    "    except:\n",
    "        pass\n",
    "end = time.time()\n",
    "print('Total use time:', round(end-start,4),'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
