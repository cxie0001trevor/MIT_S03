{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing PDF Files\n",
    "\n",
    "Portable Document Format ([PDF](https://acrobat.adobe.com/au/en/products/about-adobe-pdf.html)), invented by Adobe, is \"*a file format used to present and exchange documents reliably, independent of software, hardware, or operating system.*\" It is a great format for representing digital documents since each PDF file encapsulates a complete description of the layout of the original document (i.e., the text, fonts, graphics, and other meta information of the document). However, itâ€™s a document representation format and not a data format that is machine readable, like CSV, JSON, and XML. Unfortunately, much of real world data is stored in PDF files, particularly the data published by some government agencies and finance institutions. \n",
    "Here we would also like to point out that if you can avoid having to extract\n",
    "data from PDFs, you should.\n",
    "\n",
    "For data analysis, PDF is not a preferred storage or presentation format. However, sometimes we do not have any other choice. Throughout this chapter, you are going to learn two different ways of scraping data from PDFs with examples. We will cover how to write your own Python scripts, how to use some existing tools, and finally how to save the parsed data into a CSV file.\n",
    "\n",
    "The example used in this chapter is \"[Table 2: Nutrition](http://www.unicef.org/sowc2014/numbers/documents/english/EN-FINAL%20Table%202.pdf)\" from Unicef's report on [The State of the Worlds Children](http://www.unicef.org/sowc2014/numbers/) for 2014. Click the link to download the pdf file, named \"EN-FINAL Table 2.pdf\" and save it into the same folder as where you stored this notebook. It is the same data as that used in the previous chapter, but in PDF format. The following screenshot shows what the the first page of the PDF file looks like. \n",
    "![](./EN_FINAL_Table_2_page_1.jpg)\n",
    "\n",
    "PDFs are more difficult to work with than Excel files because different PDFs can have different formats that are unpredictable. For those curious why it is so difficult to extract data from PDFs, you might be interested in reading the story from [ProPublica](https://www.propublica.org/nerds/item/heart-of-nerd-darkness-why-dollars-for-docs-was-so-difficult) (Read Section \"PDFs Considered Harmful\" ðŸ“– ). There are many ways of extracting data from PDFs. Just to name a few, here is a list of tools:\n",
    "\n",
    "* [pdfminer](http://www.unixuser.org/~euske/python/pdfminer/): A tool for extracting text, images, object coordinates, metadata from PDF documents. It includes a PDF converter and an extensible PDF parser. \n",
    "* [pdftables](https://github.com/chrisdev/pdftables): A tool for extracting tables from PDF files, it uses pdfminer to get information on the locations of text elements. Each row in the table is extracted and stored in a list.\n",
    "* [slate](https://pypi.python.org/pypi/slate): A Python package that simplifies the process of extracting text from PDF files. It is a small Python module that wraps pdfminer's API.\n",
    "* [PyPDF2](http://mstamy2.github.io/PyPDF2/): A Python library built for manipulating PDFs, such as extracting document information, splitting, merging, and cropping pages, etc.\n",
    "* [Tabula](http://tabula.technology/): A simple tool for extracting data tables out of PDF files. It is quite simple to use.\n",
    "\n",
    "Try to find more tools on Internet! Note that you should search for PDF parsing tools that are capable of extracting data from PDFs, as some parsing tools are not suitable for data extraction.\n",
    "\n",
    "Besides these tools, you can also scrape data from PDF files with many programming languages, like Python. After searching for online tutorials, documentation, and blog post, such as \n",
    "* [Get Started With Scraping â€“ Extracting Simple Tables from PDF Documents](http://schoolofdata.org/2013/06/18/get-started-with-scraping-extracting-simple-tables-from-pdf-documents/) ðŸ“– . It dicusses how to use pdftohtml to extract tables from PDFs.\n",
    "\n",
    "In this chapter we will demonstrate how to use pdfminder and pdftables to extract data tables out from the downloaded PDF file and save the extracted data into a CSV file.\n",
    "You are also required to try Tabula on the same PDF file as an exercise.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping data from PDFs with PDFMiner\n",
    "We start with a crude approach in which one first converts PDF to text, and then extracts data for the text using, for example, regular expression. This approach is better if you have a very large PDF file or a series of PDF files that corresponds to a set of consistent documents. We will also show the drawbacks of this approach later in this section. \n",
    "\n",
    "\n",
    "### 1.1 Converting PDF to Text\n",
    "To convert the downloaded PDF file to a text file, we are going to use *pdf2txt.py*, a command that comes with pdfminer. Let's install pdfminer. In your command line window, type either of the following scripts:\n",
    "```shell\n",
    "    pip install pdfminer\n",
    "```\n",
    "or \n",
    "```\n",
    "    conda install -c https://conda.anaconda.org/hargup pdfminer\n",
    "```\n",
    "\n",
    "If you do not have *pip* or another Python package manager installed, You can also download the pdfminer package directly from its website, and install it using the Makefile as follows:\n",
    "```shell\n",
    "    make install\n",
    "```\n",
    "\n",
    "Now we have pdfminer installed and are ready to convert our PDF to text by running the following command:\n",
    "```shell\n",
    "    pdf2txt.py -o en_final_table_2.txt EN_FINAL_Table_2.pdf\n",
    "```\n",
    "The argument `-o` is the text file we want to create, the second argument is the PDF file that we want to convert. After running the above command, we have a text version of the PDF file, i.e., `en_final_table_2.txt`.\n",
    "\n",
    "Take a moment to skim the txt file and the original PDF file, and have a comparison. What do you find? \n",
    "\n",
    "The text file is quite messy. All the tables have been converted into text form, and the nice table layout shown in the PDF file is lost. Now how can we extract data tables and reconstruct the layout? In the following section, you will learn how to gradually develop a Python script for scraping data from our converted text file. \n",
    "\n",
    "First, let's read the new text file into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "# loop over all the lines\n",
    "for line in pdf_txt:\n",
    "    # repr() is a built-in Python fuction that returns a string containing a printable representation of an object.\n",
    "    print repr(line) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code read the text file line-by-line and printed each line. You should notice that we have converted each line into a printable representation of a string object using Python's build-in function, `repr()`, as it will help us  discover some patterns that can be used to extract those data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting all the country names\n",
    "We start with collecting all the country names, because the country names are going to be the unique identifier of each record in our final dataset, i.e., indices in Pandas's DataFrame. To do so, let's open up the text file, i.e., `en_final_table_2.txt`, and search for blocks of text that contain country names. Can you identify any pattern?\n",
    "\n",
    "We can find the following patterns that are consistent for all blocks of text that contain country names.\n",
    "\n",
    "* Country names start after the line containing \"and areas\". For example,\n",
    "    ```\n",
    "        3 'Countries  \\n'\n",
    "        4 'and areas\\n'\n",
    "        5 'Afghanistan \\n'\n",
    "        6 'Albania \\n'\n",
    "        7 'Algeria \\n'\n",
    "        8 'Andorra \\n'\n",
    "    ```\n",
    "* The last country name in the name block is followed by a line containing just a new line character (`\\n`). For example,\n",
    "    ```\n",
    "        41 'China \\n'\n",
    "        42 'Colombia \\n'\n",
    "        43 'Comoros \\n'\n",
    "        44 'Congo \\n'\n",
    "        45 '\\n'\n",
    "        46 'Low  \\n'\n",
    "    ```\n",
    "\n",
    "Thus, to extract the country names, we need to create a Boolean variable to indicate the start and end of each name block. This Boolean variable should be set to `True` when we hit the \"and areas\" line, and to `false` when we hit the line containing only a new line character. We then update our python script with the Boolean variable accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "# \n",
    "# Create a Boolean variable that acts as an on/off switch\n",
    "#\n",
    "isCountryName = False\n",
    "\n",
    "for line in pdf_txt:\n",
    "\n",
    "    if isCountryName:\n",
    "        print repr(line)\n",
    "    #\n",
    "    # Search for the line that starts with 'and areas'. If the line starts with 'and areas', \n",
    "    # we set  isCountryName to True\n",
    "    #\n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "    #\n",
    "    # If isCountryName is turned on, and the line is equal to a new line character,\n",
    "    # Set isCountryName to False.\n",
    "    #\n",
    "    elif isCountryName and line == '\\n':\n",
    "        isCountryName = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the above script, we get what looks like all the lines with country names returned. However, if we look closely at the output, we will find that our script is not adequately parsing the lines with country name. The following issues can be identified:\n",
    "\n",
    "1. Line breaks with or without white spaces. For example, at the end of the output, you will find \n",
    "    ```\n",
    "        'Viet Nam \\n'\n",
    "        'Yemen \\n'\n",
    "        'Zambia \\n'\n",
    "        'Zimbabwe \\n'\n",
    "        ' \\n'\n",
    "        '\\n'\n",
    "    ```\n",
    "    The script we have written so far cannot exclude the lines equal to '\\n' and handle the lines containining only while spaces. Note that line breaks, as shown above, are difficult to find with the naked eye. That is why we used `repr()` to print out each line. \n",
    "2. Countries with names spreading over more than one line. For example,\n",
    "   ```\n",
    "       'Bolivia (Plurinational \\n'\n",
    "       '   State of) \\n'\n",
    "   ```\n",
    "3. All the country names end with '\\n' and some country names containing special characters, for example\n",
    "    ```\n",
    "        'Democratic People\\xe2\\x80\\x99s \\n'\n",
    "        '   Republic of Korea \\n'\n",
    "    ```\n",
    "    We need to clean those names to make them readable.\n",
    "\n",
    "First, we start with excluding all the lines that are equal to either '\\n' or '\\n' with leading white spaces. Here we choose to use regular expressions:\n",
    "```python\n",
    "    import re\n",
    "    reg = re.complie(r\"^\\s*$\")\n",
    "    for line in pdf_txt:\n",
    "        reg.match(line) != None\n",
    "```\n",
    "This regular expression matches all empty lines that contain zero or more space characters. Inserting those code into our script, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reg = re.compile(r\"^\\s*$\")\n",
    "\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "isCountryName = False\n",
    "for line in pdf_txt:\n",
    "    #\n",
    "    # Print out all the country names and exclude line breaks\n",
    "    #\n",
    "    if isCountryName and reg.match(line) == None:\n",
    "        print repr(line)\n",
    "    # Set the switch\n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "    #\n",
    "    # Set the boolean variable to False, if we reach a line break\n",
    "    #\n",
    "    elif isCountryName and reg.match(line) != None:\n",
    "        isCountryName = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the second issue in the list, let's look at all the countries names that spread over two lines. \n",
    "```\n",
    "'Bolivia (Plurinational \\n'\n",
    "'   State of) \\n'\n",
    "'Democratic People\\xe2\\x80\\x99s \\n'\n",
    "'   Republic of Korea \\n'\n",
    "'Democratic Republic \\n'\n",
    "'   of the Congo \\n'\n",
    "'Lao People\\xe2\\x80\\x99s \\n'\n",
    "'   Democratic Republic \\n'\n",
    "'Micronesia (Federated \\n'\n",
    "'   States of) \\n'\n",
    "'Saint Vincent and \\n'\n",
    "'   the Grenadines \\n'\n",
    "'The former Yugoslav \\n'\n",
    "'   Republic of Macedonia \\n'\n",
    "'United Republic \\n'\n",
    "'   of Tanzania \\n'\n",
    "'Venezuela (Bolivarian \\n'\n",
    "'   Republic of) \\n'\n",
    "```\n",
    "It is clear that there is a consistent pattern that the second line of each of those names starts with a couple of white spaces. To find all the lines starting with white spaces, we can use the following regular expression\n",
    "```python\n",
    "    re.match(r\"^\\s+\", line) != None\n",
    "```\n",
    "The regular expression matches strings that start with 1 or more white spaces. Now you should see the difference between '*' and '+' in regular expression. \n",
    "\n",
    "However, this regular expression can only identify every second line in the above list. Our final goal is to merge, for example, 'Bolivia (Plurinational \\n' and '   State of) \\n' into one line. To do so, we will create a variable, called *previous_line*, to temporarily store 'Bolivia (Plurinational \\n' before we hit '   State of) \\n'.\n",
    "The updated script is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reg = re.compile(\"^\\s*$\")\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "isCountryName = False\n",
    "\n",
    "#\n",
    "# A variabel used to cache the preceding line\n",
    "#\n",
    "previous_line =''\n",
    "\n",
    "for line in pdf_txt:\n",
    "    if isCountryName and reg.match(line) == None: \n",
    "        #\n",
    "        # If the current line starts with one or more white spaces, it will be merged with the preceding \n",
    "        # line to generate the full country name\n",
    "        #\n",
    "        if re.match(r\"^\\s+\", line) != None:\n",
    "            #\n",
    "            # Join two strings\n",
    "            #\n",
    "            line = ''.join([previous_line, line])\n",
    "            print repr(line)\n",
    "        else:\n",
    "            print repr(line)\n",
    "    \n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "    elif isCountryName and reg.match(line) !=None:\n",
    "        isCountryName = False\n",
    "    #\n",
    "    # Cache the line right before the current line.\n",
    "    #\n",
    "    previous_line = line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After joining the previous line with the current line, we have not yet removed the previous line from the printout. Next we will remove those redundant lines and store all the country names in a list, `countryNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "reg = re.compile(\"^\\s*$\")\n",
    "\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "isCountryName = False\n",
    "previous_line =''\n",
    "#\n",
    "# Intialise an empty list in Python\n",
    "#\n",
    "countryNames = []\n",
    "\n",
    "for line in pdf_txt:\n",
    "    if isCountryName and reg.match(line) == None: \n",
    "\n",
    "        if re.match(r\"^\\s+\", line) != None:\n",
    "            line = ''.join([previous_line, line])\n",
    "            #\n",
    "            # Delete previous_line from the list, and add the merged line\n",
    "            #\n",
    "            del countryNames[-1]\n",
    "            countryNames.append(line)\n",
    "        else:\n",
    "            countryNames.append(line)\n",
    "    \n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "    elif isCountryName and reg.match(line) !=None:\n",
    "        isCountryName = False\n",
    "    previous_line = line\n",
    "pprint.pprint(countryNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected all the country names from the text version of our PDF file. The total number of countries is 197. Now, we are going to do some cleaning to resolve the last issue. Data cleaning will be explained in greater detail in Module 3. For now, we will just clean up the country names, as they are not easy to read. We wrap the cleaning code into a Python function as follows.\n",
    "```python\n",
    "    def clean(line):\n",
    "        line = line.strip('\\n')  \n",
    "        line = line.strip() \n",
    "        line = line.replace('\\xe2\\x80\\x99', '\\'')\n",
    "        return line\n",
    "```\n",
    "The first line in the function removes both the leading and the trailing new line characters, '\\n'. The second line removes the leading and railing white spaces. The third line replaces a special character encoding. Now insert the `clean` function into the FOR-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "reg = re.compile(\"^\\s*$\")\n",
    "\n",
    "def clean(line):\n",
    "        line = line.strip('\\n') # remove leading and training '\\n' \n",
    "        line = line.strip() # remove leading and trailing while spaces\n",
    "        line = line.replace('\\xe2\\x80\\x99', '\\'')\n",
    "        return line\n",
    "\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "isCountryName = False\n",
    "previous_line =''\n",
    "countryNames = []\n",
    "for line in pdf_txt:\n",
    "    if isCountryName and reg.match(line) == None: \n",
    "        if re.match(r\"^\\s+\", line) != None:\n",
    "            line = ' '.join([clean(previous_line), clean(line)])\n",
    "            del countryNames[-1]\n",
    "            countryNames.append(line)\n",
    "        else:\n",
    "            countryNames.append(clean(line))\n",
    "    \n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "    elif isCountryName and reg.match(line) !=None:\n",
    "        isCountryName = False\n",
    "    previous_line = line\n",
    "pprint.pprint(countryNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we have successfully extracted the names of 197 countries and stored them in a list. Next we are going to extract all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 Extracting all the table columns\n",
    "\n",
    "Extracting the table columns is not as easy as collecting the country names. Scanning the text file, you will find that you cannot simply create a Boolean variable, as there are no patterns that can be used to identify the start and the end of each column on each PDF page. All columns on the same PDF page are stacked together and interleaved with either text or line breaks. \n",
    "\n",
    "How can we extract the data in columns and recover the table structure? \n",
    "\n",
    "Fortunately, it looks like `pdf2txt.py` extracted the data from our PDF file in a column-wise way. Each cell in our PDF was extracted as one line in the text file; and cells were stacked according to the linear layout of the table on each page. If we are able to extract all the cell values in order and know the number of records and the number of columns on each page, we might then be able to unstack all the cells and put them into a tabular format. \n",
    "\n",
    "Getting the number of columns in the PDF table is easy. Manually counting the number of columns, you will get the number 12. \n",
    "\n",
    "Next, let's count the number of records on each PDF page. Since we have written the script to collect all the country names from each PDF page, it should be easy to compute the number of records on each page by inserting a count variable into the same script. Let the count variable be `numRec`. While the script hits the line starting with 'and areas', we set `numRec` to zero. We then increase `numRec` by one every time we successfully retrieve a name until the script hits the end of the name block. Let's insert this logic into the script and save the counts in a list, `recordsPerPage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reg = re.compile(\"^\\s*$\")\n",
    "\n",
    "def clean(line):\n",
    "        line = line.strip('\\n') \n",
    "        line = line.strip()\n",
    "        line = line.replace('\\xe2\\x80\\x99', '\\'')\n",
    "        return line\n",
    "\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "isCountryName = False\n",
    "previous_line =''\n",
    "countryNames = []\n",
    "recordsPerPage = [] ###\n",
    "numRec = 0 ###\n",
    "\n",
    "for line in pdf_txt:\n",
    "    if isCountryName and reg.match(line) == None: \n",
    "        #print repr(line)\n",
    "        if re.match(r\"^\\s+\", line) != None:\n",
    "            line = ' '.join([clean(previous_line), clean(line)])\n",
    "            del countryNames[-1]\n",
    "            countryNames.append(line)\n",
    "            numRec -= 1 ###\n",
    "        else:\n",
    "            countryNames.append(clean(line))\n",
    "        numRec += 1 ###\n",
    "    \n",
    "    if line.startswith('and areas'):\n",
    "        isCountryName = True\n",
    "        numRec = 0 ###\n",
    "    elif isCountryName and reg.match(line) !=None:\n",
    "        isCountryName = False\n",
    "        recordsPerPage.append(numRec) ###\n",
    "    previous_line = line\n",
    "    \n",
    "print(recordsPerPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect all the cell values from the text file, we will use regular expressions. The values that each cell can take are\n",
    "```\n",
    "    '6  \\n'\n",
    "    '6 x \\n'  \n",
    "    '12 x \\n'\n",
    "    '39 x,y \\n'\n",
    "    '96 x,y\\n'\n",
    "    '76 y \\n'\n",
    "    '100 \\n'\n",
    "    '100 x\\n'\n",
    "    '90 w \\n'\n",
    "    '-  \\n'\n",
    "```\n",
    "It is not hard for you to figure out the patterns in the above values, which are a dash, a number with 1, 2 or 3 digits, or a number followed by 'x', 'x,y', 'y' or 'w'. Taking into account white spaces, we generate the following regular expression to encode all the patterns.\n",
    "```python\n",
    "    regx = re.compile(\"^(\\d{1,3}|-)\\s?(x|x,y|y|w)?\\s*$\")\n",
    "```\n",
    "* `^(\\d{1,3}|-)`: The matched line should start with a dash symbol or a number with 1 to 3 digits\n",
    "* `(x|x,y|y|w)?`: The matched line should contain none or one of elements in the parentheses\n",
    "* `\\s*$`: The matched line should end with zero or more white space characters.\n",
    "\n",
    "The following script will print out all the cell values that match these patterns.\n",
    "\n",
    "```python\n",
    "    import re\n",
    "    regx = re.compile(\"^(\\d{1,3}|-)\\s?(x|x,y|y|w)?\\s*$\")\n",
    "    pdfTxtFile = './en_final_table_2.txt'\n",
    "    pdf_txt = open(pdfTxtFile, 'r')\n",
    "\n",
    "    for line in pdf_txt:\n",
    "        line = line.replace('\\xe2\\x80\\x93', '-')\n",
    "        if regx.match(line) != None:\n",
    "            print repr(line)\n",
    "```\n",
    "\n",
    "However, this script will also extract '2' in the following lines:\n",
    "```\n",
    "    'T\\n'\n",
    "    'A\\n'\n",
    "    'B\\n'\n",
    "    'L\\n'\n",
    "    'E\\n'\n",
    "    '2\\n'\n",
    "```\n",
    "'2' following 'E' is not a cell value. We need to exclude it in order to make proper alignment\n",
    "among rows and columns. Checking whether the preceding line of '2' is equal to 'E\\n' or not will\n",
    "solve this problem. Similar to the method we used to handle country names that spread over two lines, \n",
    "we introduce a string variable, ` previous_line`, to cache the preceding line. Thus, if the \n",
    "preceding line is 'E\\n', the following line equal to '2\\n' will be excluded. We add the following \n",
    "conditition into the IF statement\n",
    "\n",
    "```python\n",
    "    re.match(r\"^E\\s*$\", previous_line) == None:\n",
    "```\n",
    "\n",
    "So, the updated script will be\n",
    "\n",
    "```python\n",
    "    import re\n",
    "    regx = re.compile(\"^(\\d{1,3}|-)\\s?(x|x,y|y|w)?\\s*$\")\n",
    "    pdfTxtFile = './en_final_table_2.txt'\n",
    "    pdf_txt = open(pdfTxtFile, 'r')\n",
    "    previous_line = ''\n",
    "    for line in pdf_txt:\n",
    "        line = line.replace('\\xe2\\x80\\x93', '-')\n",
    "        if regx.match(line) != None and re.match(r\"^E\\s*$\", previous_line) == None:\n",
    "            print repr(line)\n",
    "        previous_line = line\n",
    "```\n",
    "\n",
    "Now, we can merge all the scripts that we have written so far together and generate the final script \n",
    "for scraping data tables from the PDF file.\n",
    "In the following merged script, the part of collecting country names and counting the number of \n",
    "records on each page is wrapped in a Python function, called `extract`. This function takes the \n",
    "the text file as input and output two lists, one for country names, and another for record counts.\n",
    "The extracted data is going to be stored in a dictionary, where keys are column indices, values \n",
    "are lists of cell values in individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(line):\n",
    "    line = line.strip('\\n')\n",
    "    line = line.strip() \n",
    "    line = line.replace('\\xe2\\x80\\x99', '\\'')\n",
    "    return line\n",
    "\n",
    "def extract(pdfTxtFile):\n",
    "    \"\"\"\n",
    "        Collecting all the country names and counting the number\n",
    "        of records, i.e., countries, on each page. \n",
    "    \"\"\"\n",
    "    reg = re.compile(\"^\\s*$\")\n",
    "    isCountryName = False\n",
    "    countryNames = []\n",
    "    recordsPerPage = []\n",
    "    numRec = 0\n",
    "    previous_line =''\n",
    "\n",
    "    pdf_txt = open(pdfTxtFile, 'r')\n",
    "    for line in pdf_txt:\n",
    "        if isCountryName and reg.match(line) == None: \n",
    "            #print repr(line)\n",
    "            if re.match(r\"^\\s+\", line) != None:\n",
    "                line = ' '.join([clean(previous_line), clean(line)])\n",
    "                del countryNames[-1]\n",
    "                countryNames.append(line)\n",
    "                numRec -= 1\n",
    "            else:\n",
    "                countryNames.append(clean(line))\n",
    "            numRec += 1\n",
    "\n",
    "        if line.startswith('and areas'):\n",
    "            isCountryName = True\n",
    "            numRec = 0\n",
    "        elif isCountryName and reg.match(line) !=None:\n",
    "            isCountryName = False\n",
    "            recordsPerPage.append(numRec)\n",
    "        previous_line = line\n",
    "    return countryNames,recordsPerPage\n",
    "\n",
    "pdfTxtFile = './en_final_table_2.txt'\n",
    "\n",
    "countryNames, recordsPerPage = extract(pdfTxtFile)\n",
    "\n",
    "regx = re.compile(r\"^(\\d{1,3}|-)\\s?(x|x,y|y|w)?\\s*$\")\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "totalNumCols = 12\n",
    "\n",
    "pageNum = -1;\n",
    "numRecs = 0\n",
    "colIdx = 0\n",
    "\n",
    "#\n",
    "# Python dictinoary used to store all the data\n",
    "#\n",
    "data = {}\n",
    "for i in xrange(totalNumCols):\n",
    "    data[i] = []\n",
    "    \n",
    "idx = 0\n",
    "previous_line = ''\n",
    "for line in pdf_txt:\n",
    "    if line.startswith('and areas'):\n",
    "        pageNum += 1\n",
    "        numRecs = recordsPerPage[pageNum]\n",
    "        colIdx = 0\n",
    "        idx = 0\n",
    "    line = line.replace('\\xe2\\x80\\x93', '-')\n",
    "    if regx.match(line) != None and re.match(r\"^E\\s*$\", previous_line) == None and colIdx < 12:\n",
    "        line = line.strip('\\n').strip()\n",
    "        data[colIdx].append(line)\n",
    "        idx += 1\n",
    "        if idx % numRecs == 0:\n",
    "            colIdx += 1\n",
    "    previous_line = line        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Storing data in CSV format \n",
    "\n",
    "The final step of scraping data from PDFs is to store the extracted data in a machine readable format. Here \n",
    "we are going to store the data in CSV format using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, index = countryNames)\n",
    "df.to_csv('en_final_table_2_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping data from PDFs using `pdf2txt.py` is crude, as you need to go over the text dozens of times \n",
    "to manually identify patterns, and encode these patterns with regular expressions. Checking the CSV file, \n",
    "you will find the script does not correctly extract the table in the last page of our PDF. The patterns\n",
    "we found while extracting cell values do not apply to the text extracted from the last page. \n",
    "`pdf2txt.py` has stacked columns in arbitrary order. However, one can image that if a tool \n",
    "can make use of the location information of the text elements, this problem will then be solved. \n",
    "\n",
    "Besides the above approach, there are multiple ways of scraping data from PDFs, which utilise the meta information\n",
    "encapsulated in PDF. We will walk through some of them in the following sections.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Scraping data from PDFs with  pdftables\n",
    " \n",
    "After scratching our heads at the complexity shown in the approach of using 'pdf2txt.py'. We started searching for\n",
    "other tools or libraries that make use of information on the locations of text elements in a PDF document. We came across a Python library, called pdftables. In this section, you will learn how to use pdftables to extract data from our PDF files. To install this library, use the following command\n",
    "```shell\n",
    "    conda install -c https://conda.anaconda.org/jacksongs pdftables\n",
    "```\n",
    "Note that installing pdftables might downgrade your numpy version, which could cause Pandas to fail. In this case,\n",
    "you need to upgrade your numpy after installing pdftables. We should mention that the drawback of using pdftables \n",
    "is that its developers do not maintain proper documentation. Hence you might need to look at the source code to\n",
    "figure out the functions that you are going to use. Nevertheless, it is a good tool for extracting data tables from\n",
    "PDFs. You will eventually find that the all-in-one function that you are going to use to get the data is\n",
    "```python\n",
    "    pdftables.get_tables()\n",
    "```\n",
    "\n",
    "In this section, we will use the same PDF file as we used in the previous section to demonstrate how to use pdftables to scrape all the tables from that PDF file. Let's start with loading our PDF with the `get_tables()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdftables import get_tables\n",
    "pdfFile = './EN_FINAL_Table_2.pdf'\n",
    "pdfobj = open(pdfFile, 'rb')\n",
    "tables = get_tables(pdfobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script will take a couple of seconds to load our PDF. The `get_tables()` function returns each page as its\n",
    "own table, each of those tables have a list of rows, and each of those row is a contained list of columns. You can use the following Python code to print out each row in each table:\n",
    "```python\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            print row\n",
    "```\n",
    "After printing all the rows, you will find that all the strings returned by pdftables are Unicode strings. So you need to convert them to strings with Python's string encoding function.\n",
    "```python\n",
    "    str.encode('UTF8')\n",
    "```\n",
    "We also need to replace the following UTF-8 literals with their corresponding strings.\n",
    "```\n",
    "    '\\xe2\\x80\\x93': -\n",
    "    '\\xe2\\x80\\x99': '\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    for row in table:\n",
    "        for i in xrange(len(row)):\n",
    "            row[i] = row[i].encode('UTF8')\n",
    "            row[i] = row[i].replace('\\xe2\\x80\\x93', '-')\n",
    "            row[i] = row[i].replace('\\xe2\\x80\\x99', '\\'')\n",
    "        print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the script, you should see that each row is much more readable. Next we will start scraping \n",
    "data in each table. Let's print out the first 10 lines in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    for row in table[:10]:\n",
    "        print row\n",
    "    print '==========================\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you find? \n",
    "\n",
    "All the titles are included in the first 5 lists, and they are very messy. For simplification,\n",
    "we do not extract column titles here with a Python script. Instead assume that we can manually set up the \n",
    "title list by eyeballing the original PDF. However, we can also see country rows start from the sixth list,\n",
    "and those rows are quite clean. To exclude the first five lists in each table, we can use list slicing in \n",
    "the FOR-loop over rows:\n",
    "``` python\n",
    "    for row in table[5:]\n",
    "```\n",
    "\n",
    "Similarly, if we print out the last 10 rows of each table,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    for row in table[-10:]:\n",
    "        print row\n",
    "    print '==========================\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what did you find?\n",
    "\n",
    "In the first five tables, the last country row is always followed by a similar row like\n",
    "```\n",
    "['39      THE STATE OF', 'THE WORLD\\xe2\\x80\\x99', 'S CHILDREN', '2014 IN', 'NU', 'MBERS', '', '', '', '', '', '', '', '']\n",
    "```\n",
    "Therefore, our script should stop collecting country rows while it hit the above row. In the FOR-loop over rows, we should have something like\n",
    "```python\n",
    "    if 'THE STATE OF' in row[0]:\n",
    "            break\n",
    "```\n",
    "\n",
    "Unfortunately, the above pattern does not apply to the last table. It needs specially treatment. If we look at the \n",
    "original PDF file, we will see that the last country row to be collected is the row for 'Zimbabwe'. It appears in\n",
    "the last table as\n",
    "```\n",
    "['Zimbabwe', '11', '65', '31', '', '86', '20', '10', '2', '32', '3', '6', '61', '94 y']\n",
    "```\n",
    "Thus, we can put another IF statement to check if the first string in the list is \n",
    "equal to 'Zimbabwe'. If it is, then we stop collecting country rows after collecting the current row. \n",
    "```python\n",
    "    if row[0] == 'Zimbabwe':\n",
    "        print row\n",
    "        break\n",
    "```\n",
    "Let's insert this logic into the FOR loop over rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    for row in table[5:]:\n",
    "        if 'THE STATE' in row[0]:\n",
    "            break\n",
    "        if row[0] == 'Zimbabwe':\n",
    "            print row\n",
    "            break;\n",
    "        print row\n",
    "    print '==========================\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that running the above script returns all the country rows. It will. However, it will also return, for example,\n",
    "```\n",
    "['Bolivia (Plurinational', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "['State of)', '6', '64', '60', '', '83', '40', '4', '1', '27', '1', '9', '41', '89 y']\n",
    "```\n",
    "This is similar to what we found earlier in Section 2, while we were handling country names spreading over two rows.\n",
    "We want to programmatically solve this problem with some tests based on what we have learnt so far. Since '-' is used to indicate missing data in our PDF, we know for sure that if the first element of the row is a string (i.e., not null)\n",
    "and all the following elements are null, this row must contain the first part of a country name. Before we skip this\n",
    "row, we need to use a variable (say 'first_name') to cache the first part, as we need to merge it with the\n",
    "corresponding second part.\n",
    "The code should look like\n",
    "```python\n",
    "    if row[2] == '':\n",
    "        first_name = row[0]\n",
    "        continue\n",
    "```\n",
    "Since these country names spread over two consecutive rows, we add the following IF statement to join the two parts \n",
    "of a country name:\n",
    "```python\n",
    "    if first_name != '':\n",
    "        row[0] = '{} {}'.format(first_name, row[0])\n",
    "        first_name = ''\n",
    "```\n",
    "Now, we put the two IF statements into the FOR loop over rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_name = ''\n",
    "\n",
    "for table in tables:\n",
    "    for row in table[5:]:\n",
    "        if row[2] == '':\n",
    "            first_name = row[0]\n",
    "            continue\n",
    "        if first_name != '':\n",
    "            row[0] = '{} {}'.format(first_name, row[0])\n",
    "            first_name = ''\n",
    "        if 'THE STATE OF' in row[0]:\n",
    "            break\n",
    "        if row[0] == 'Zimbabwe':\n",
    "            print row\n",
    "            break\n",
    "        print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have completely extracted all the country rows from the six tables. Next we are going to store them in Panda's\n",
    "DataFrame. There are multiple ways of creating a DataFrame. Here we create DataFrame by passing a dictionary of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {}\n",
    "for table in tables:\n",
    "    for row in table[5:]:\n",
    "        if row[2] == '':\n",
    "            continue\n",
    "        if row[0] == 'Zimbabwe':\n",
    "            data[row[0]] = row[1:]\n",
    "            break\n",
    "        if 'THE STATE' in row[0]:\n",
    "            break\n",
    "        data[row[0]] = row[1:] \n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When you run the script, you will find that the forth row is empty. The number of rows is supposed to be 12, as there are 12 columns in our PDF. The empty row needs to be dropped, which can be easily done with the `drop()` function of\n",
    "Pandas' DataFrame. i.e.,\n",
    "```python\n",
    "    data.drop(3, 0)\n",
    "```\n",
    "The last step is to transpose the DataFrame so that each row is a record for a country, and save the data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(3, 0)\n",
    "data = data.T\n",
    "data.columns = xrange(12)\n",
    "data.to_csv('./en_final_table_2_2.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Summary\n",
    "\n",
    "PDF is one of the hard-to-parse formats that you will encounter. \n",
    "In this chapter, we have learnt how to scrap data tables from PDFs using the following python libraries \n",
    "* pdfminer - converts PDF into text, so you can parse the text file by finding patterns and writing regular\n",
    "    expressions\n",
    "* pdftables - uses pdfminer to find both text elements and their locations and put aligned elements in a list.\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Exercise \n",
    "1. [Tabula](http://tabula.technology/) is an open source tool that is specifically designed for scraping data within tables from PDFs and saving the data into a CSV file. With a small PDF like the one we used in the chapter, you \n",
    "could try Tabula. Thus, please download Tabula and try it on our PDF file. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
