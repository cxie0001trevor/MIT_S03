{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Pre-Processing Technologies\n",
    "\n",
    "The aim of this tutorial is to demonstrate the basic technologies used to pre-process text data in the text mining, Information Retrieval (IR) and Natural Language Processing (NLP) communities. Those technologies include\n",
    "* Tokenizing text\n",
    "* Removing stop words\n",
    "* Stemming & Lemmatization\n",
    "* Sentence segmentation\n",
    "\n",
    "The ultimate goal of pre-processing text is to convert unstructured and free language text into structured data so that text analysis algorithms can directly take the structured data as input. For example, the UCI machine learning database provides free download of the bag-of-words datasets that contain ENRON emails, NIPS articles, the New York Times news articles, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed\">PubMed</a> articles. Those are the bench mark datasets used in text analysis. Let's have a look at one of the datasets, PubMed. The image below shows a screenshot of the first 15 lines in the data set\n",
    "<img src=\"pubmed_example.png\">\n",
    "The the three lines are the total number of PubMed abstracts, the vocabulary size, and the total number of work tokens in the datasets. Each abstract is stored in a sparse format that is often used in text analysis, where each row contains document ID, word index and the corresponding word count in the document. For example, \"1 6811 1\" means word 6811 appears in document 1 just once. To find the word string for \"6811\", you then go to the vocabulary and find the 6811th word. Now, how can we pre-process text data and save the processed data in the spare format.\n",
    "\n",
    "Assume that we are going to analyze some medical reports that are about fungal disease. The goal of the analysis is to predict how likely a patient has fungal infection given some diagnostic report. The prediction can be formulated as a classification task where we are going to assign a binary label to a patient. 1 means the patient has fungal infection, and 0 means the patient does not. The text in the following cell contains a short diagnostic report for a patient. In this tutorial, you are going to learn the basic techniques often used in preprocessing text. In next tutorial, you will learn how to put these techniques together to count vocabulary and generate the final structure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = \"\"\"Previous right upper lobe nodule? Fungal question resolution change. Findings: Comparison is made to prior CT dated November 30, 2004. Significant resolution in the previously noted fluid overload status. Ectasia of the thoracic aorta measuring 4.2 cm. Features of generalised centrilobular emphysema. Resolution of right upper lobe nodule. There is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature. Given the interval development of this fungal/inflammatory aetiology is likely. There is a 13 mm right axillary node which is a new finding since the prior study. No significant mediastinal or hilar adenopathy. Conclusion: Nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\"\"\"\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization\n",
    "\n",
    "Now, we need to think about how to break such a long sequence of characters into word tokens. The task of breaking a character sequence into pieces is known as tokenization. In the lecture, we have covered different tokenizers built in NLTK. For example, whitespace tokenizer, regular expression tokenizer and etc. You can find more information on the NLTK website, e.g.,\n",
    "* <a href=\"http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\">tokenize module</a> in nltk.\n",
    "* <a href=\"http://www.nltk.org/howto/tokenize.html\">tokenize</a>: shows you how to use Treebank tokenizer and Regexp tokenizer\n",
    "\n",
    "You can also refer to the Jupyter Notebook we provided. After tokenizing the <font color=\"brown\">raw_text</font>, you should derive the following list of tokens\n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe', 'nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'features', 'of', 'generalised', 'centrilobular', 'emphysema', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory', 'aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens are all unigrams. Except for \"non-specific\" that contains a hyphen and numbers, all the other tokens are single word tokens. As we know, phrases are more meaningful than single word, which makes us think that it would be good to tokenize a text so that phrases are kept as phrases. Then, the question is how can we merge multi-word expressions into single tokens. Assume that we are going to have the following multi-word expressions being treated as single tokens\n",
    "* \"<font color=\"red\">generalised centrilobular emphysema</font>\"\n",
    "* \"<font color=\"red\">inflammatory aetiology</font>\"\n",
    "* \"<font color=\"red\">lobe nodule</font>\"\n",
    "* \"<font color=\"red\">axillary node</font>\"\n",
    "* \"<font color=\"red\">thoracic aorta measuring</font>\"\n",
    "\n",
    "In other words, you cannot split those phrases into individual words. It is lucky that NLTK provides us a <a href=\"http://www.nltk.org/_modules/nltk/tokenize/mwe.html#MWETokenizer\">multi-word expression tokenizer</a>. The output should be\n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'of', 'generalised_centrilobular_emphysema', 'resolution', 'of', 'right', 'upper', 'lobe_nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory_aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary_node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar_adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory_aetiology']\n",
    "```\n",
    "\n",
    "Firstly, you should think about how to expand the list of unique words give by the unigram tokenizer above. In order to get a unique list of tokens (about 76 tokens in total), you can use <font color=\"blue\">set</font> function, then convert the set to a list, and append the list with multi-word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tokenize the <font color=\"brown\"> raw_tex</font> with multi-word expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also have try the different tokenizer on line at http://text-processing.com/demo/tokenize/.\n",
    "\n",
    "The <font color='brown'>raw_text</font> has been split into a list of tokens that contains both unigrams and multi-word expressions. However, the list contains a lot of functional words, such as \"to\", \"in\", \"the\", \"is\" and so on. These functional words usually do not contribute much to the semantics of the text, except for increase the dimensionality of the data in text analysis. Also, note that our goal is to build a classification model of predicting fungal disease. Thus, we are more interested in the meaning of the diagnostic report than the syntax. Therefore, we can choose to remove those words, which is your next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stop Words Removal\n",
    "\n",
    "As we have discussed in the lecture and in the Jupyter Notebook, stop words carry little lexical content. They are often functional words in English, for example, articles, pronouns, particles, and so on. In NLP and IR, we usually exclude stop words from the vocabulary. Otherwise, we will face the curse of dimensionality. There are some exceptions, such as syntactic analysis like parsing, we choose to keep those functional words. However, you are going to remove all the stop words in the above list by using the stop word list in NLTK, which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your turn to remove all the stop words from the output of your <b>MWEtokenizer</b>. While writing the list comprehension code, you should think about the difference between <font color=\"blue\">list</font> and <font color=\"blue\">set</font>. Sets are significantly faster when your task is to determine if an object is present in the set, but are slower than lists when you try to iterate over the elements. The list of tokens after stop words be removed should be \n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'mm', 'non-specific', 'nature', 'given', 'interval', 'development', 'fungal', 'inflammatory_aetiology', 'likely', '13', 'mm', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'conclusion', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'fungal', 'inflammatory_aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a rich stopword list, as the one used in the lecture. You can also expand the stopword list by adding corpus specific stop words, for example those more frequent words. By more frequent, we meant that the words appear in every documents, they do not help us distinguish documents. For example, the following words do appear in each diagnostic report. (In next tutorial, we will demonstrate how to use basic statistics to identify them.)\n",
    "* <font color=\"red\">ct</font>\n",
    "* <font color=\"red\">mm</font>\n",
    "* <font color=\"red\">cm</font>\n",
    "* <font color=\"red\">fungal</font>\n",
    "* <font color=\"red\">conclusion</font>\n",
    "\n",
    "You task is to expand the stopword list with the four words, and process the list of tokens again. The output should be\n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'non-specific', 'nature', 'given', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'inflammatory_aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### write your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we should inspect the output, which is a very good practice in data preprocessing. You will find that we have words like \"find\" and \"findings\", \"previous\" and \"previously\", \"noted\", etc. Should we keep them as they are? or Should we reduce them to the base form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming, Lemmatization, setence segmentation and POS tagging\n",
    "\n",
    "The task of stemming and lemmatization is to reduce the same word in different lexical forms to its base form in the lexicon without significantly loosing the meaning. In English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are inflected in the comparative/superlative. In morphology, the derivation process creates a new word out of an existing one often by adding either a prefix or a suffix. In this exercise, you are going to apply the <b>WordNetLemmatizer</b> provided in the NLTK's <a href=\"http://www.nltk.org/api/nltk.stem.html\">stem</a> package. Note that the <b>WordNetLemmatizer</b> can take the POS tag of each word as one argument, specifying which can give us more accurate base form of the word. \n",
    "\n",
    "Therefore, first you should carry out sentence segmentation. You code should produce \n",
    "```\n",
    "previous right upper lobe nodule?\n",
    "fungal question resolution change.\n",
    "findings: comparison is made to prior ct dated november 30, 2004. significant resolution in the previously noted fluid overload status.\n",
    "ectasia of the thoracic aorta measuring 4.2 cm.\n",
    "features of generalised centrilobular emphysema.\n",
    "resolution of right upper lobe nodule.\n",
    "there is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature.\n",
    "given the interval development of this fungal/inflammatory aetiology is likely.\n",
    "there is a 13 mm right axillary node which is a new finding since the prior study.\n",
    "no significant mediastinal or hilar adenopathy.\n",
    "conclusion: nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\n",
    "```\n",
    "\n",
    "In order to segment the given text into sentences, you can refer to the Jupyter Notebook used in the lecture or search \"<b>Punkt Sentence Tokenizer</a>\" on the http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use the POS tagger to assign POS tag to each word in each sentences. Please refer to Section 1 of http://www.nltk.org/book/ch05.html. The step you are going to use is: \n",
    "\n",
    "For each sentence\n",
    "1. use the unigram tokenizer you developed in Exercise 1 to tokenize the sentence\n",
    "2. use the <b>MWETokenizer</b> used in Exercise 1 to tokenize the sentence with multi-word expressions\n",
    "3. use the information in Section 1 of http://www.nltk.org/book/ch05.html to help you finish the POS tagging.\n",
    "4. remove the stop words in each sentence\n",
    "\n",
    "Finally save the tagged sentences in a list. The output you will derive should be\n",
    "```\n",
    "[[('previous', 'JJ'), ('right', 'JJ'), ('upper', 'NN'), ('lobe_nodule', 'NN')], [('question', 'NN'), ('resolution', 'NN'), ('change', 'NN')], [('findings', 'NNS'), ('comparison', 'NN'), ('made', 'VBN'), ('prior', 'VB'), ('dated', 'JJ'), ('november', 'RB'), ('30', 'CD'), ('2004', 'CD'), ('significant', 'JJ'), ('resolution', 'NN'), ('previously', 'RB'), ('noted', 'VBN'), ('fluid', 'NN'), ('overload', 'NN'), ('status', 'NN')], [('ectasia', 'NN'), ('thoracic_aorta_measuring', 'VBG'), ('4.2', 'CD')], [('features', 'NNS'), ('generalised_centrilobular_emphysema', 'NN')], [('resolution', 'NN'), ('right', 'JJ'), ('upper', 'JJ'), ('lobe_nodule', 'NN')], [('presence', 'NN'), ('nodule', 'NN'), ('within', 'IN'), ('medial', 'JJ'), ('segment', 'NN'), ('right', 'JJ'), ('lower', 'JJR'), ('lobe', 'NN'), ('measures', 'VBZ'), ('5.4', 'CD'), ('non-specific', 'JJ'), ('nature', 'NN')], [('given', 'VBN'), ('interval', 'NN'), ('development', 'NN'), ('inflammatory_aetiology', 'NN'), ('likely', 'JJ')], [('13', 'CD'), ('right', 'NN'), ('axillary_node', 'NN'), ('new', 'JJ'), ('finding', 'NN'), ('since', 'IN'), ('prior', 'JJ'), ('study', 'NN')], [('significant', 'JJ'), ('mediastinal', 'NN'), ('hilar_adenopathy', 'NN')], [('nodule', 'NN'), ('right', 'NN'), ('lower', 'JJR'), ('lobe', 'NN'), ('keeping', 'VBG'), ('inflammatory_aetiology', 'NN')]]\n",
    "```\n",
    "Now, your task is to fill the for loop below by following the steps above. If you would like to know the meaning of each tag, you can type for example\n",
    "```\n",
    "print nltk.help.upenn_tagset('NNP')\n",
    "```\n",
    "Replacing \"NNP\" with the tag you want, you should see the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents = []\n",
    "for sent in sentences:\n",
    "    ### write your code below\n",
    "\n",
    "    #####\n",
    "print tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to apply the <b>WordNetLemmatizer</b>. You code should make use the POS tags of each word to decide the lexical base form. The <font color=\"blue\">lemmatize</font> function in <b>WordNetLemmatizer</b> can accept the following wordnet tags\n",
    "* wordnet.ADJ\n",
    "* wordnet.VERB\n",
    "* wordnet.NOUN \n",
    "* wordnet.ADV\n",
    "\n",
    "The function of converting POS tags to wordnet tags is given bellow. In you code, you should think about how to call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The code in this cell is adapted from the following website\n",
    "# http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now writing your lemmatization code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "final_tokens =[]\n",
    "### write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the difference between the tokenization with and without lemmatization. For example, if the list of tokens generated in Exercise 2 is \"stopped_tokens\", then you can use the following code to see the difference\n",
    "```python\n",
    "set(final_tokens) - set(stopped_tokens)\n",
    "```\n",
    "```python\n",
    "set(stopped_tokens) - set(final_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To use the stemmers discussed in the lecture, you can simply replace the following code in the above code cell\n",
    "```python\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "```\n",
    "with \n",
    "```python\n",
    "    stemmer = PorterStemmer()\n",
    "```\n",
    "and replance\n",
    "```python\n",
    "    lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) \n",
    "```\n",
    "with \n",
    "```python\n",
    "    stemmer.stem(w[0])\n",
    "```\n",
    "Don't forget import the corresponding modules.\n",
    "\n",
    "Note that we have not yet done the preprocessing. Next tutorial, we will learn how to count the vocabulary by further removing the most and less frequent words, to generate numberical represenation of a document, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
