{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Pre-Processing Technologies\n",
    "\n",
    "The aim of this tutorial is to demonstrate the basic technologies used to pre-process text data in the text mining, Information Retrieval (IR) and Natural Language Processing (NLP) communities. Those technologies include\n",
    "* Tokenizing text\n",
    "* Removing stop words\n",
    "* Stemming & Lemmatization\n",
    "* Sentence segmentation\n",
    "\n",
    "The ultimate goal of pre-processing text is to convert unstructured and free language text into structured data so that text analysis algorithms can directly take the structured data as input. For example, the UCI machine learning database provides free download of the bag-of-words datasets that contain ENRON emails, NIPS articles, the New York Times news articles, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed\">PubMed</a> articles. Those are the bench mark datasets used in text analysis. Lets have a look at one of the datasets, PubMed. The image below shows a screenshot of the first 15 lines in the data set\n",
    "<img src=\"pubmed_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the three lines are the total number of PubMed abstracts, the vocabulary size, and the total number of work tokens in the datasets. Each abstract is stored in a sparse format that is often used in text analysis, where each row contains **document ID**, **word index** and the corresponding **word count** in the document. For example, \"1 6811 1\" means word 6811 appears in document 1 just once. To find the word string for \"6811\", you then go to the vocabulary and find the 6811th word. Now, how can we pre-process text data and save the processed data in the spare format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we are going to analyze some medical reports that are about fungal disease. The goal of the analysis is to **predict how likely a patient has fungal infection given some diagnostic report**. The prediction can be formulated as a **classification task** where we are going to **assign a binary label to a patient**: 1 means the patient has fungal infection, and 0 means the patient does not. \n",
    "\n",
    "The text in the following cell contains a short diagnostic report for a patient. In this tutorial, you are going to learn **the basic techniques** often used in preprocessing text. In next tutorial, you will learn how to put these techniques together to count vocabulary and generate the final structure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"Previous right upper lobe nodule? Fungal question resolution change. \n",
    "Findings: Comparison is made to prior CT dated November 30, 2004. Significant resolution \n",
    "in the previously noted fluid overload status. Ectasia of the thoracic aorta measuring 4.2 cm.\n",
    "Features of generalised centrilobular emphysema. Resolution of right upper lobe nodule. \n",
    "There is now presence of a nodule within the medial segment of the right lower lobe which \n",
    "measures 5.4 mm and is non-specific in nature. Given the interval development of this \n",
    "fungal/inflammatory aetiology is likely. There is a 13 mm right axillary node which is a new \n",
    "finding since the prior study. No significant mediastinal or hilar adenopathy. Conclusion: \n",
    "Nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\"\"\"\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization 单词化\n",
    "\n",
    "Now, we need to think about how to break such a long sequence of characters into word tokens. The task of breaking a character sequence into pieces is known as tokenization. In the lecture, we have covered different tokenizers built in NLTK. For example, whitespace tokenizer (**WhitespaceTokenizer**仅根据空格划分), regular expression tokenizer (**RegexpTokenizer**根据正则表达划分) and etc. You can find more information on the NLTK website, e.g.,\n",
    "* <a href=\"http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\">tokenize module</a> in nltk.\n",
    "* <a href=\"http://www.nltk.org/howto/tokenize.html\">tokenize</a>: shows you how to use Treebank tokenizer and Regexp tokenizer\n",
    "\n",
    "You can also refer to the Jupyter Notebook we provided. After tokenizing the <font color=\"brown\">raw_text</font>, you should derive the following list of tokens\n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe', 'nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'features', 'of', 'generalised', 'centrilobular', 'emphysema', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory', 'aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpTokenizer -- 正则表达式划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'right', 'upper', 'lobe', 'nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'features', 'of', 'generalised', 'centrilobular', 'emphysema', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory', 'aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "unigram_tokens = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\").tokenize(raw_text)\n",
    "print (unigram_tokens)\n",
    "len(set(unigram_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer -- 根据空格划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'right', 'upper', 'lobe', 'nodule?', 'fungal', 'question', 'resolution', 'change.', 'findings:', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30,', '2004.', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status.', 'ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm.', 'features', 'of', 'generalised', 'centrilobular', 'emphysema.', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule.', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature.', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal/inflammatory', 'aetiology', 'is', 'likely.', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study.', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy.', 'conclusion:', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal/inflammatory', 'aetiology.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokens_white = WhitespaceTokenizer().tokenize(raw_text)\n",
    "print(tokens_white)\n",
    "len(set(tokens_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nodule?', 'change.', 'findings:', '30,', '2004.', 'resolution', 'status.', 'the', 'cm.', 'of', 'emphysema.', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule.', 'is', 'of', 'the', 'of', 'the', 'right', 'lobe', 'is', 'in', 'nature.', 'the', 'of', 'fungal/inflammatory', 'is', 'likely.', 'there', 'is', 'a', 'mm', 'right', 'which', 'is', 'a', 'the', 'prior', 'study.', 'significant', 'adenopathy.', 'conclusion:', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'fungal/inflammatory', 'aetiology.']\n"
     ]
    }
   ],
   "source": [
    "for i in list(set(unigram_tokens)):\n",
    "    for j in list(set(tokens_white)):\n",
    "        if i == j:\n",
    "            tokens_white.remove(j)\n",
    "print(tokens_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:  \n",
    "Using <font color='blue'>**WhitespaceTokenizer**</font> just separate words between spaces. Non-alphanumeric characters (such as `?`, `,`, `.`, etc.) are included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multi-word expression 组合词\n",
    "\n",
    "The tokens are all unigrams. Except for `\"non-specific\"` that contains a hyphen and numbers, all the other tokens are single word tokens. As we know, **phrases are more meaningful than single word**, which makes us think that it would be good to tokenize a text so that phrases are kept as phrases. Then, the question is how can we **merge multi-word expressions into single tokens**. Assume that we are going to have the following multi-word expressions being treated as single tokens\n",
    "* \"<font color=\"red\">generalised centrilobular emphysema</font>\"\n",
    "* \"<font color=\"red\">inflammatory aetiology</font>\"\n",
    "* \"<font color=\"red\">lobe nodule</font>\"\n",
    "* \"<font color=\"red\">axillary node</font>\"\n",
    "* \"<font color=\"red\">thoracic aorta measuring</font>\"\n",
    "\n",
    "In other words, **you cannot split those phrases into individual words**. It is lucky that NLTK provides us a <a href=\"http://www.nltk.org/_modules/nltk/tokenize/mwe.html#MWETokenizer\">multi-word expression tokenizer</a>. The output should be\n",
    "```\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'of', 'generalised_centrilobular_emphysema', 'resolution', 'of', 'right', 'upper', 'lobe_nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory_aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary_node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar_adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory_aetiology']\n",
    "```\n",
    "\n",
    "Firstly, you should think about how to expand the list of unique words give by the unigram tokenizer above. In order to get a unique list of tokens (about 76 tokens in total), you can use <font color=\"blue\">set</font> function, then convert the set to a list, and append the list with multi-word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'fluid', 'resolution', 'inflammatory', 'or', '2004', '13', 'lobe', 'previous', 'question', 'is', 'which', 'ct', 'noted', 'conclusion', 'medial', 'mediastinal', 'nature', 'centrilobular', 'study', 'presence', 'significant', 'likely', 'right', 'finding', 'axillary', 'upper', 'this', 'generalised', 'aorta', 'a', 'keeping', 'adenopathy', 'the', 'with', 'dated', 'new', 'overload', 'cm', '4.2', 'november', 'nodule', 'interval', 'node', 'no', 'now', 'status', 'emphysema', 'development', 'to', 'previously', 'given', 'aetiology', 'findings', 'made', 'thoracic', 'features', 'mm', 'hilar', 'non-specific', 'of', 'there', 'in', 'segment', 'ectasia', 'lower', 'within', 'measuring', 'fungal', '30', '5.4', 'measures', 'change', 'comparison', 'prior', 'since', ('generalised', 'centrilobular', 'emphysema'), ('inflammatory', 'aetiology'), ('hilar', 'adenopathy'), ('lobe', 'nodule'), ('axillary', 'node'), ('thoracic', 'aorta', 'measuring')]\n"
     ]
    }
   ],
   "source": [
    "uni_voc = list(set(unigram_tokens))\n",
    "# include the Multi-word expression\n",
    "uni_voc.append(('generalised', 'centrilobular', 'emphysema'))\n",
    "uni_voc.append(('inflammatory', 'aetiology'))\n",
    "uni_voc.append(('hilar', 'adenopathy'))\n",
    "uni_voc.append(('lobe', 'nodule'))\n",
    "uni_voc.append(('axillary', 'node'))\n",
    "uni_voc.append(('thoracic', 'aorta', 'measuring'))\n",
    "print(uni_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, tokenize the <font color=\"brown\">**raw_tex**</font> with multi-word expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'fluid', 'resolution', 'inflammatory', 'or', '2004', '13', 'lobe', 'previous', 'question', 'is', 'which', 'ct', 'noted', 'conclusion', 'medial', 'mediastinal', 'nature', 'centrilobular', 'study', 'presence', 'significant', 'likely', 'right', 'finding', 'axillary', 'upper', 'this', 'generalised', 'aorta', 'a', 'keeping', 'adenopathy', 'the', 'with', 'dated', 'new', 'overload', 'cm', '4.2', 'november', 'nodule', 'interval', 'node', 'no', 'now', 'status', 'emphysema', 'development', 'to', 'previously', 'given', 'aetiology', 'findings', 'made', 'thoracic', 'features', 'mm', 'hilar', 'non-specific', 'of', 'there', 'in', 'segment', 'ectasia', 'lower', 'within', 'measuring', 'fungal', '30', '5.4', 'measures', 'change', 'comparison', 'prior', 'since'}\n",
      "\n",
      "-----After MWE------\n",
      "\n",
      "{'and', 'fluid', 'resolution', 'generalised_centrilobular_emphysema', 'or', '2004', '13', 'previous', 'lobe', 'question', 'is', 'which', 'ct', 'axillary_node', 'noted', 'conclusion', 'medial', 'mediastinal', 'nature', 'study', 'presence', 'significant', 'likely', 'right', 'finding', 'upper', 'this', 'keeping', 'a', 'with', 'the', 'dated', 'new', 'overload', 'lobe_nodule', '4.2', 'november', 'cm', 'nodule', 'interval', 'no', 'hilar_adenopathy', 'now', 'status', 'thoracic_aorta_measuring', 'inflammatory_aetiology', 'development', 'to', 'previously', 'given', 'findings', 'made', 'features', 'mm', 'non-specific', 'of', 'there', 'in', 'segment', 'ectasia', 'lower', 'within', '5.4', 'fungal', '30', 'measures', 'change', 'comparison', 'prior', 'since'}\n",
      "\n",
      "-----Difference------\n",
      "\n",
      "{'lobe_nodule', 'generalised_centrilobular_emphysema', 'hilar_adenopathy', 'thoracic_aorta_measuring', 'axillary_node', 'inflammatory_aetiology'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer #Multi-words Expression Tokenizer\n",
    "mwe_tokenizer = MWETokenizer(uni_voc) #input the unique tokens\n",
    "mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
    "print(set(unigram_tokens))\n",
    "print(\"\\n-----After MWE------\\n\")\n",
    "print(set(mwe_tokens))\n",
    "print(\"\\n-----Difference------\\n\")\n",
    "print(set(mwe_tokens)-set(unigram_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:  \n",
    "1. `'lobe', 'nodule'` has become `'lobe_nodule'` \n",
    "2. `'thoracic', 'aorta', 'measuring'` has become `'thoracic_aorta_measuring'`\n",
    "3. `'generalised', 'centrilobular', 'emphysema'` has become  `'generalised_centrilobular_emphysema'`\n",
    "4. `'hilar', 'adenopathy'` has become `'hilar_adenopathy'`\n",
    "5. `'inflammatory', 'aetiology'` has become `'inflammatory_aetiology'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also have try the different tokenizer on line at http://text-processing.com/demo/tokenize/.\n",
    "\n",
    "The <font color='brown'>**raw_text**</font> has been split into a list of tokens that contains **both unigrams and multi-word expressions**. However, the list contains a lot of functional words, such as \"to\", \"in\", \"the\", \"is\" and so on. These functional words usually do not contribute much to the semantics of the text, except for increase the dimensionality of the data in text analysis. Also, note that our goal is to build a classification model of predicting fungal disease. Thus, we are more interested in the meaning of the diagnostic report than the syntax. Therefore, we can choose to **remove those words**, which is your next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stop Words Removal\n",
    "\n",
    "As we have discussed in the lecture and in the Jupyter Notebook, **stop words carry little lexical content**. They are often functional words in English, for example, articles, pronouns, particles, and so on. In NLP and IR, we usually **exclude stop words from the vocabulary**. Otherwise, we will face the curse of dimensionality. There are some exceptions, such as syntactic analysis like parsing, we choose to keep those functional words. However, you are going to remove all the stop words in the above list by using the stop word list in NLTK, which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your turn to remove all the stop words from the output of your <b>MWEtokenizer</b>.\n",
    "\n",
    "> **Note**: the difference between <font color=\"blue\">**list**</font> and <font color=\"blue\">**set**</font>.  \n",
    "Sets are **significantly faster** when your task is to determine if an object **is present in the set**. \n",
    "But are **slower** than lists when you try to **iterate over the elements**. \n",
    "\n",
    "The list of tokens after stop words be removed should be :\n",
    "```Python\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'mm', 'non-specific', 'nature', 'given', 'interval', 'development', 'fungal', 'inflammatory_aetiology', 'likely', '13', 'mm', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'conclusion', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'fungal', 'inflammatory_aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'mm', 'non-specific', 'nature', 'given', 'interval', 'development', 'fungal', 'inflammatory_aetiology', 'likely', '13', 'mm', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'conclusion', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'fungal', 'inflammatory_aetiology']\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords_list)\n",
    "stopped_tokens = [w for w in mwe_tokens if w not in stopwords_set] #use sets to check if word present\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a rich stopword list, as the one used in the lecture. You can also expand the stopword list by adding corpus specific stop words, for example those **more frequent words** (the words appear in every document but do not help us distinguish documents). For example, the following words do appear in each diagnostic report. (In next tutorial, we will demonstrate how to use basic statistics to identify them.)\n",
    "* <font color=\"red\">ct</font>\n",
    "* <font color=\"red\">mm</font>\n",
    "* <font color=\"red\">cm</font>\n",
    "* <font color=\"red\">fungal</font>\n",
    "* <font color=\"red\">conclusion</font>\n",
    "\n",
    "You task is to expand the stopword list with the four words, and process the list of tokens again. The output should be\n",
    "```Python\n",
    "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'non-specific', 'nature', 'given', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'inflammatory_aetiology']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fluid', 'resolution', 'generalised_centrilobular_emphysema', '2004', '13', 'previous', 'lobe', 'question', 'axillary_node', 'noted', 'medial', 'mediastinal', 'nature', 'study', 'presence', 'significant', 'likely', 'right', 'finding', 'upper', 'keeping', 'dated', 'new', 'overload', 'lobe_nodule', '4.2', 'november', 'nodule', 'interval', 'hilar_adenopathy', 'status', 'thoracic_aorta_measuring', 'inflammatory_aetiology', 'development', 'previously', 'given', 'findings', 'made', 'features', 'non-specific', 'segment', 'ectasia', 'lower', 'within', '5.4', '30', 'measures', 'change', 'comparison', 'prior', 'since'}\n"
     ]
    }
   ],
   "source": [
    "### write your code below\n",
    "stopwords_set.add('ct')\n",
    "stopwords_set.add('mm')\n",
    "stopwords_set.add('cm')\n",
    "stopwords_set.add('fungal')\n",
    "stopwords_set.add('conclusion')\n",
    "stopped_tokens = [w for w in mwe_tokens if w not in stopwords_set]\n",
    "print(set(stopped_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we should inspect the output, which is a very good practice in data preprocessing. You will find that we have words like `\"find\"` and `\"findings\"`, `\"previous\"` and `\"previously\"`, `\"noted\"`, etc. Should we keep them as they are? or Should we reduce them to the base form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming, Lemmatization, setence segmentation and POS tagging\n",
    "\n",
    "The task of stemming and lemmatization is to **reduce the same word in different lexical forms to its base form** in the lexicon without significantly loosing the meaning. In English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are inflected in the comparative/superlative. In morphology, the derivation process creates a new word out of an existing one often by adding either a prefix or a suffix. In this exercise, you are going to apply the <font color='blue'>**WordNetLemmatizer**</font> provided in the NLTK's <a href=\"http://www.nltk.org/api/nltk.stem.html\">stem</a> package. Note that the <font color='blue'>**WordNetLemmatizer**</font> can take **the POS tag** of each word as one argument, specifying which can give us more accurate base form of the word. \n",
    "\n",
    "Therefore, first you should carry out sentence segmentation. You code should produce \n",
    "```\n",
    "previous right upper lobe nodule?\n",
    "fungal question resolution change.\n",
    "findings: comparison is made to prior ct dated november 30, 2004. significant resolution in the previously noted fluid overload status.\n",
    "ectasia of the thoracic aorta measuring 4.2 cm.\n",
    "features of generalised centrilobular emphysema.\n",
    "resolution of right upper lobe nodule.\n",
    "there is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature.\n",
    "given the interval development of this fungal/inflammatory aetiology is likely.\n",
    "there is a 13 mm right axillary node which is a new finding since the prior study.\n",
    "no significant mediastinal or hilar adenopathy.\n",
    "conclusion: nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\n",
    "```\n",
    "\n",
    "In order to segment the given text into sentences, you can refer to the Jupyter Notebook (chapter 1) or search \"<b>Punkt Sentence Tokenizer</a>\" on the http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punkt Sentence Tokenizer 分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x10a2133c8>\n",
      "1 previous right upper lobe nodule?\n",
      "2 fungal question resolution change.\n",
      "3 findings: comparison is made to prior ct dated november 30, 2004. significant resolution \n",
      "in the previously noted fluid overload status.\n",
      "4 ectasia of the thoracic aorta measuring 4.2 cm.\n",
      "5 features of generalised centrilobular emphysema.\n",
      "6 resolution of right upper lobe nodule.\n",
      "7 there is now presence of a nodule within the medial segment of the right lower lobe which \n",
      "measures 5.4 mm and is non-specific in nature.\n",
      "8 given the interval development of this \n",
      "fungal/inflammatory aetiology is likely.\n",
      "9 there is a 13 mm right axillary node which is a new \n",
      "finding since the prior study.\n",
      "10 no significant mediastinal or hilar adenopathy.\n",
      "11 conclusion: \n",
      "nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sent_detector.tokenize(raw_text.strip())\n",
    "print(sent_detector)\n",
    "i = 1\n",
    "for sent in sentences:\n",
    "    print(i, sent)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use **the POS tagger** to assign POS tag to each word in each sentence. Please refer to Section 1 of http://www.nltk.org/book/ch05.html. The step you are going to use is: \n",
    "\n",
    "For each sentence\n",
    "1. use the unigram tokenizer you developed in Exercise 1 to tokenize the sentence\n",
    "2. use the **MWETokenizer** used in Exercise 1 to tokenize the sentence with **multi-word expressions (MWE)**  \n",
    "3. use the information in Section 1 of http://www.nltk.org/book/ch05.html to help you finish the POS tagging.\n",
    "4. remove the stop words in each sentence\n",
    "\n",
    "Finally save the tagged sentences in a list. The output you will derive should be\n",
    "```\n",
    "[[('previous', 'JJ'), ('right', 'JJ'), ('upper', 'NN'), ('lobe_nodule', 'NN')], [('question', 'NN'), ('resolution', 'NN'), ('change', 'NN')], \n",
    "[('findings', 'NNS'), ('comparison', 'NN'), ('made', 'VBN'), ('prior', 'VB'), ('dated', 'JJ'), ('november', 'RB'), ('30', 'CD'), ('2004', 'CD'), ('significant', 'JJ'), ('resolution', 'NN'), ('previously', 'RB'), ('noted', 'VBN'), ('fluid', 'NN'), ('overload', 'NN'), ('status', 'NN')], \n",
    "[('ectasia', 'NN'), ('thoracic_aorta_measuring', 'VBG'), ('4.2', 'CD')], \n",
    "[('features', 'NNS'), ('generalised_centrilobular_emphysema', 'NN')], \n",
    "[('resolution', 'NN'), ('right', 'JJ'), ('upper', 'JJ'), ('lobe_nodule', 'NN')], [('presence', 'NN'), ('nodule', 'NN'), ('within', 'IN'), ('medial', 'JJ'), ('segment', 'NN'), ('right', 'JJ'), ('lower', 'JJR'), ('lobe', 'NN'), ('measures', 'VBZ'), ('5.4', 'CD'), ('non-specific', 'JJ'), ('nature', 'NN')], \n",
    "[('given', 'VBN'), ('interval', 'NN'), ('development', 'NN'), ('inflammatory_aetiology', 'NN'), ('likely', 'JJ')], \n",
    "[('13', 'CD'), ('right', 'NN'), ('axillary_node', 'NN'), ('new', 'JJ'), ('finding', 'NN'), ('since', 'IN'), ('prior', 'JJ'), ('study', 'NN')], \n",
    "[('significant', 'JJ'), ('mediastinal', 'NN'), ('hilar_adenopathy', 'NN')], \n",
    "[('nodule', 'NN'), ('right', 'NN'), ('lower', 'JJR'), ('lobe', 'NN'), ('keeping', 'VBG'), ('inflammatory_aetiology', 'NN')]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `CC`: Coordinating conjunction 连接词\n",
    "* `RB`: Adverbs 副词\n",
    "* `IN`: Preposition 介词\n",
    "* `JJ`: Adjective 形容词\n",
    "* `NN`: Noun 名词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to fill the for loop below by following the steps above. If you would like to know the meaning of each tag, you can type for example\n",
    "```\n",
    "print nltk.help.upenn_tagset('NNP')\n",
    "```\n",
    "Replacing `\"NNP\"` with the tag you want, you should see the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('previous', 'JJ'), ('right', 'JJ'), ('upper', 'NN'), ('lobe_nodule', 'NN')], [('question', 'NN'), ('resolution', 'NN'), ('change', 'NN')], [('findings', 'NNS'), ('comparison', 'NN'), ('made', 'VBN'), ('prior', 'VB'), ('dated', 'JJ'), ('november', 'RB'), ('30', 'CD'), ('2004', 'CD'), ('significant', 'JJ'), ('resolution', 'NN'), ('previously', 'RB'), ('noted', 'VBN'), ('fluid', 'NN'), ('overload', 'NN'), ('status', 'NN')], [('ectasia', 'NN'), ('thoracic_aorta_measuring', 'VBG'), ('4.2', 'CD')], [('features', 'NNS'), ('generalised_centrilobular_emphysema', 'NN')], [('resolution', 'NN'), ('right', 'JJ'), ('upper', 'JJ'), ('lobe_nodule', 'NN')], [('presence', 'NN'), ('nodule', 'NN'), ('within', 'IN'), ('medial', 'JJ'), ('segment', 'NN'), ('right', 'JJ'), ('lower', 'JJR'), ('lobe', 'NN'), ('measures', 'VBZ'), ('5.4', 'CD'), ('non-specific', 'JJ'), ('nature', 'NN')], [('given', 'VBN'), ('interval', 'NN'), ('development', 'NN'), ('inflammatory_aetiology', 'NN'), ('likely', 'JJ')], [('13', 'CD'), ('right', 'NN'), ('axillary_node', 'NN'), ('new', 'JJ'), ('finding', 'NN'), ('since', 'IN'), ('prior', 'JJ'), ('study', 'NN')], [('significant', 'JJ'), ('mediastinal', 'NN'), ('hilar_adenopathy', 'NN')], [('nodule', 'NN'), ('right', 'NN'), ('lower', 'JJR'), ('lobe', 'NN'), ('keeping', 'VBG'), ('inflammatory_aetiology', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = []\n",
    "for sent in sentences:\n",
    "    uni_sent = tokenizer.tokenize(sent)\n",
    "    mwe_text = mwe_tokenizer.tokenize(uni_sent)\n",
    "    tagged_sent = nltk.tag.pos_tag(mwe_text)\n",
    "    stopped_tagged_sent = [x for x in tagged_sent if x[0] not in stopwords_set]  \n",
    "    tagged_sents.append(stopped_tagged_sent)\n",
    "    '''\n",
    "    print('sentence: ', sent) # single sentence\n",
    "    print('uni_sent: ', uni_sent) # tokens\n",
    "    print('MWE: ', mwe_text) # check multi-words\n",
    "    print('POS tagging: ', tagged_sent)\n",
    "    print('Stopped: ', stopped_tagged_sent)\n",
    "    print('----------')\n",
    "    '''\n",
    "print (tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet and Lamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about **WordNet**: [WordNet Tutorial](http://localhost:8888/notebooks/gitfiles/MIT_S03/5196_Exercise/NLTK_python/Wordnet_tutorial.ipynb) and [official doc](http://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to apply the <b>WordNetLemmatizer</b>.  \n",
    "\n",
    "You code should **make use the POS tags of each word to decide the lexical base form**.  \n",
    "The <font color=\"blue\">**lemmatize**</font> function in <b>WordNetLemmatizer</b> can accept the following wordnet tags\n",
    "* `wordnet.ADJ`\n",
    "* `wordnet.VERB`\n",
    "* `wordnet.NOUN` \n",
    "* `wordnet.ADV`\n",
    "\n",
    "The function of **converting POS tags to wordnet tags** is given bellow. In you code, you should think about how to call the function.  \n",
    "[code source](http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, each word in a form of `('word', 'tag')`. To make use of the `tag`, we use `wordnet` to improve the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ # 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB # 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN # 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV # 'r'\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now writing your lemmatization code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordNetLemmatizer().lemmatize(word, pos='n')` method:  \n",
    "using WordNet’s built-in morphy function, returns the input word unchanged if it cannot be found in WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "1. Don't forget the parentheses `()`\n",
    "2. the input must be in lowercase, otherwise it won't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implies -> Implies \n",
      "implies -> imply\n"
     ]
    }
   ],
   "source": [
    "case = WordNetLemmatizer().lemmatize('Implies', wordnet.VERB)\n",
    "case_lower = WordNetLemmatizer().lemmatize('implies', wordnet.VERB)\n",
    "print('Implies ->', case, '\\nimplies ->', case_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "final_tokens = []\n",
    "for tagged_set in tagged_sents:\n",
    "    final_tokens.append([lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) for w in tagged_set ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the difference between the tokenization with and without lemmatization. For example, if the list of tokens generated in Exercise 2 is \"stopped_tokens\", then you can use the following code to see the difference\n",
    "```python\n",
    "set(final_tokens) - set(stopped_tokens)\n",
    "```\n",
    "```python\n",
    "set(stopped_tokens) - set(final_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,3,4])-set([1,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To use the stemmers discussed in the lecture, you can simply replace the following code in the above code cell\n",
    "```python\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "```\n",
    "with \n",
    "```python\n",
    "    stemmer = PorterStemmer()\n",
    "```\n",
    "and replance\n",
    "```python\n",
    "    lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) \n",
    "```\n",
    "with \n",
    "```python\n",
    "    stemmer.stem(w[0])\n",
    "```\n",
    "Don't forget import the corresponding modules.\n",
    "\n",
    "Note that we have not yet done the preprocessing. Next tutorial, we will learn how to count the vocabulary by further removing the most and less frequent words, to generate numerical represenation of a document, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['previous', 'right', 'upper', 'lobe_nodule'], ['question', 'resolution', 'change'], ['finding', 'comparison', 'make', 'prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'note', 'fluid', 'overload', 'status'], ['ectasia', 'thoracic_aorta_measuring', '4.2'], ['feature', 'generalised_centrilobular_emphysema'], ['resolution', 'right', 'upper', 'lobe_nodule'], ['presence', 'nodule', 'within', 'medial', 'segment', 'right', 'low', 'lobe', 'measure', '5.4', 'non-specific', 'nature'], ['give', 'interval', 'development', 'inflammatory_aetiology', 'likely'], ['13', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study'], ['significant', 'mediastinal', 'hilar_adenopathy'], ['nodule', 'right', 'low', 'lobe', 'keep', 'inflammatory_aetiology']]\n"
     ]
    }
   ],
   "source": [
    "print(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'non-specific', 'nature', 'given', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'inflammatory_aetiology']\n"
     ]
    }
   ],
   "source": [
    "print(stopped_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
