{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Pre-Processed text and Generating Features\n",
    "One of the challenges of text analysis is to convert unstructured and semi-structured text into a structured representation. This must be done prior to carrying out any text analysis tasks. This chapter will show you \n",
    "how to put some of those basic steps discussed in the previous chapter together to generate different **vecto\n",
    "representations** for some given text. You will learn how to compute some basic **statistics** for text, and how to extract features rather than unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counting Vocabulary by Selecting Tokens of Interest\n",
    "Two important concepts that should be mentioned first are **type** and **token**.\n",
    "Here are the definitions of the two terms, quoted from \"[tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\",  \n",
    "\n",
    ">a <font color='blue'>**token**</font> is an instance of a sequence of characters in some particular document that are grouped together as a **useful semantic unit** for processing;\n",
    "\n",
    "> a <font color='blue'>**type**</font> is the class of all tokens containing the **same character** sequence. \n",
    "\n",
    "A <font color='blue'>**type**</font> is also a **vocabulary entry**. In other words, a vocabulary consists of a number of word types.\n",
    "The distinction between a type and its tokens is a distinction that separates a descriptive concept from its particular concrete instances. \n",
    "This is quite similar to the distinction in object-oriented programming between classes and objects.\n",
    "\n",
    "In this section, you are going to learn **how to count types** in a given corpus by further processing the text.\n",
    "\n",
    "The document collection that we are going to use is a set of Reuters articles that comes with NLTK.\n",
    "It contains 10788 Reuters articles in total and has been **split into two subsets**, **training** and **testing**.\n",
    "\n",
    "Although this collection has already been pre-processed (e.g., you can access the text at different levels, like raw text, tokens, and sentences),\n",
    "we would still like to demonstrate how to put some of the basis text preprocessing steps together and process the raw Reuters articles step by step.\n",
    "First, import the main Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tokenizer works on a per document level, we can **parallelize the process of tokenization** with Python's multi-processing module. Please refer to its official documentation [here](https://docs.python.org/2/library/multiprocessing.html).\n",
    "\n",
    "In the following code, we wrap tokenization in a Python function, and then\n",
    "create a pool of four worker processes with the Python Pool class.\n",
    "The `Pool.map()`, a parallel equivalent of the  built-in  `map()` function, takes one iterable argument.\n",
    "\n",
    "The iterable will be split into a number of chunks, each of which will be submitted to a process in the process pool.\n",
    "Each process will apply a callable function to each element in the chunk it has received.\n",
    "Note that you can replace the NLTK tokenizer with the one you implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeRawData(fileid):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text document.\n",
    "    \"\"\"\n",
    "    raw_article = reuters.raw(fileid).lower() # convert all words to lowercase\n",
    "    # word_tokenize return words, punctuations (except periods)\n",
    "    tokenised_article = nltk.tokenize.word_tokenize(raw_article) # tokenize each Reuters articles\n",
    "    # input as key, the output as values\n",
    "    return (fileid, tokenised_article) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list, as in multiprocessing, a list of arguments by default will be regarded as different process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(processes=4) # Build a pool of 4 processess \n",
    "tokenized_reuters = dict(pool.map(tokenizeRawData, reuters.fileids()))\n",
    "# Outputs are (key, values), group in dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above parallelized code does not work on your computer, you can try the following code running with a signle thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_reuters =  dict(tokenizeRawData(fileid) for fileid in reuters.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Removing Words with Non-alphabetic Characters\n",
    "The NLTK's built-in `word_tokenize` function tokenizes a string to split off punctuation other than periods.\n",
    "Not only does it return words with alphanumerical characters, but also punctuations. \n",
    "Let's take a look at one Reuters articles,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['partnership', 'cuts', 'stake', 'in', 'erc', 'international', '&', 'lt', ';', 'erc', '>', 'parsow', 'partnership', 'ltd', ',', 'a', 'nevada', 'investment', 'partnership', ',', 'said', 'it', 'lowered', 'its', 'stake', 'in', 'erc', 'international', 'inc', 'to', '343,500', 'shares', 'or', '8.3', 'pct', 'of', 'the', 'total', 'outstanding', 'common', 'stock', ',', 'from', '386,300', 'shares', ',', 'or', '9.3', 'pct', '.', 'in', 'a', 'filing', 'with', 'the', 'securities', 'and', 'exchange', 'commission', ',', 'parsow', 'said', 'it', 'sold', '42,800', 'erc', 'common', 'shares', 'between', 'jan', '9', 'and', 'march', '2', 'at', 'prices', 'ranging', 'from', '12.125', 'to', '14.50', 'dlrs', 'each', '.', 'the', 'partnership', 'said', 'its', 'dealings', 'in', 'erc', 'stock', 'are', 'for', 'investment', 'purposes', 'and', 'it', 'has', 'no', 'intention', 'of', 'seeking', 'control', 'of', 'the', 'company', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_reuters['training/1684'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Assume that we are interested in words containing **alphabetic characters only** \n",
    "and would like to **remove all the other tokens**\n",
    "that contain digits, punctuation and the other symbols.\n",
    "\n",
    "Removing all the non-alphabetic words from the vocabulary is\n",
    "usually required in some text analysis tasks, such as <font color=\"blue\">**Topic Modelling**</font> that\n",
    "**learns the semantic meaning of documents**.\n",
    "It can be easily done with the  `isalpha()` function.  \n",
    "\n",
    " `isalpha()` checks whether the string consists of alphabetic characters **only** or not.\n",
    "This method returns true if all characters in the string are in the alphabet and there \n",
    "is at least one character, false otherwise.\n",
    "\n",
    "If you would like to keep all words with **alphanumeric characters**, you can use\n",
    " `isalnum()`. Refer to Python's [built-in types](https://docs.python.org/2/library/stdtypes.html) for more detail.\n",
    "\n",
    "Indeed, you can construct your tokenizer in a way such that the tokenizer only extracts words with either alphabetic or alphanumerical characters, as we discussed in the previous chapter. We will leave this as a simple exercise for you to do on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in tokenized_reuters.items():\n",
    "    tokenized_reuters[k] = [word for word in v if word.isalpha()] # as filter \n",
    "\n",
    "#tokenized_reuters['training/1684']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have derived much cleaner text for each Reuters article.\n",
    "Let's check how many types we have in the whole corpus and the lexical diversity (i.e., the average number \n",
    "of times a type apprearing in the collection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  27944 \n",
      "Total number of tokens:  1274688 \n",
      "Lexical diversity:  45.61580303464071\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import division # for python 2\n",
    "from itertools import chain\n",
    "\n",
    "words_list = list(chain.from_iterable(tokenized_reuters.values())) #break the inner list\n",
    "vocab = set(words_list) # unique\n",
    "lexical_diversity = len(words_list)/len(vocab)\n",
    "print(\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words_list), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The second statement imported a `chain()` iterator from the  <font color=\"blue\">**itertools**</font> module.\n",
    "\n",
    ">It works as iterator to join all the words in all the Reuters articles together.\n",
    "```python\n",
    "   for wordList in tokenized_reuters.values():\n",
    "       for word in wordList:\n",
    "           yield word\n",
    "```\n",
    "相当于把list全部拆分后，再组合起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 1.27 million word tokens in the tokenized Reuters corpus.\n",
    "The vocabulary size is 27,944, which is still quite large according to our knowledge of this corpus. The lexical diversity tells us that words occur on average about 46 times each.\n",
    "\n",
    "You might think that there could still be words that occur very frequently, such as **stopwords**, and those that only occur once or twice.\n",
    "For example, if an article \"the\" appears in almost every document in a corpus, it might not help you at all and would only contribute noise.\n",
    "\n",
    "Similarly if a word **appears only once in a corpus** or **only in one document** of the corpus, it could carry little useful information for downstream analysis.\n",
    "\n",
    "Therefore, we would better remove those words from the vocabulary, which will benefit the text analysis algorithms in terms of **reducing running time and memory requirement**, and **improving their performance**.\n",
    "To do so, we need to further explore the corpus by computing some simple\n",
    "statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Removing the Most and Less Frequent Words\n",
    "It is quite useful for us to identify the words that are most informative about the sematic \n",
    "meaning of the text regardless of syntax.\n",
    "One common statistics often used in text processing is frequency distribution.\n",
    "It can tell us how frequent a word is in a given corpus in terms of either term frequency or document frequency.\n",
    "Term frequency counts the number of times a word occurs in the whole corpus regardless which document it is in.\n",
    "Frequency distribution based on term frequency tells us how the total number of word tokens are distributed across all the types.\n",
    "NLTK provides a built-in function `FreqDist` to compute this distribution directly from a set of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "fd_1 = FreqDist(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words in the corpus?\n",
    "we can use the `most_common()` function to print out **the most frequent words** together with their frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FreqDist().most_common()`:  \n",
    "List the **n most** common elements and their counts from the most common to the least.  \n",
    "If n is `None`, then list all element counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69245),\n",
       " ('of', 36749),\n",
       " ('to', 36275),\n",
       " ('in', 29217),\n",
       " ('and', 25616),\n",
       " ('said', 25381),\n",
       " ('a', 24723),\n",
       " ('mln', 18598),\n",
       " ('vs', 14332),\n",
       " ('for', 13420),\n",
       " ('dlrs', 12329),\n",
       " ('it', 11087),\n",
       " ('pct', 9771),\n",
       " ('on', 9094),\n",
       " ('lt', 8696),\n",
       " ('cts', 8308),\n",
       " ('from', 8217),\n",
       " ('is', 7673),\n",
       " ('that', 7538),\n",
       " ('its', 7402),\n",
       " ('by', 7082),\n",
       " ('at', 7014),\n",
       " ('net', 6986),\n",
       " ('year', 6687),\n",
       " ('be', 6354)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_1.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above contains the 25 most frequent words.\n",
    "You can see that it is mostly dominated by the little words of the English language which have important grammatical roles.\n",
    "Those words are **articles(冠词), prepositions(介词), pronouns(代词), auxiliary webs(), conjunctions(连词)**, etc.\n",
    "They are usually referred to **as function words** in linguistics, which **tell us nothing about the meaning of the text**.\n",
    "What proportion of the text is taken up with such words?\n",
    "We can generate a cumulative frequency plot for them\n",
    "using `fd.plot(25, cumulative=True)`.\n",
    "\n",
    "If you set `cumulative` argument to `False`, it will plot the frequencies of these 25 words.\n",
    "\n",
    "These 25 words account for about 33% of the while Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a11198470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd_1.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a111aae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd_1.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words in terms of document frequency?\n",
    "Here we are going to count how many documents a word appears in, which is referred to as document frequency.\n",
    "Instead of writing nested FOR loops to count the document frequency for each word,\n",
    "we can use  `FreqDist()` jointly with `set()` as follows:\n",
    "1. Apply  `set()` to each Reuters article to generate **a set of unique words** in the article and save all sets in a list\n",
    "```python\n",
    "    [set(value) for value in tokenized_reuters.values()]\n",
    "```\n",
    "2. Similar to what we have done before, we put all the words in a list using `chain.from_iterable`and past it to  `FreqDist`.\n",
    "\n",
    "The first step makes sure that each word in an article appears only once, thus the total number of times a word appears in all the sets is equal to the number of documents containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 7621),\n",
       " ('the', 6950),\n",
       " ('to', 6911),\n",
       " ('said', 6784),\n",
       " ('and', 6761),\n",
       " ('in', 6580),\n",
       " ('a', 6222),\n",
       " ('lt', 6069),\n",
       " ('for', 5415),\n",
       " ('mln', 4845),\n",
       " ('it', 4788),\n",
       " ('dlrs', 4193),\n",
       " ('from', 4006),\n",
       " ('on', 3987),\n",
       " ('its', 3761),\n",
       " ('is', 3569),\n",
       " ('by', 3511),\n",
       " ('year', 3416),\n",
       " ('at', 3392),\n",
       " ('with', 3253),\n",
       " ('pct', 3212),\n",
       " ('cts', 3068),\n",
       " ('inc', 3017),\n",
       " ('vs', 2982),\n",
       " ('be', 2927)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of set of unique words in each doc\n",
    "words_each = [set(value) for value in tokenized_reuters.values()]\n",
    "\n",
    "# total unique words in Reuters\n",
    "words_unique = list(chain.from_iterable(words_each))\n",
    "fd_unique = FreqDist(words_unique)\n",
    "fd_unique.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will find is that the majority of the most frequent words according to their document frequecy are still functional words.\n",
    "Therefore, the next step is to remove all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignoring Stopwords\n",
    "As discussed in section 3 of chapter 1, we often remove function words from the text completely for most text analysis tasks.\n",
    "Instead of using the built-in stopword list of NLTK, we use a much rich stopword list that has been downloaded in Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.726263046264648"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "tokenized_reuters_1 = {}\n",
    "for fileid in reuters.fileids():\n",
    "    tokenized_reuters_1[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwords_list]\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/trevortse/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def remove_stpwds(fid):\n",
    "    global reuters\n",
    "    global stopwords_set\n",
    "    return (fid, [w for w in tokenized_reuters[fid] if w not in stopwords_set])\n",
    "\n",
    "stopwords_set = set(stopwords_list)\n",
    "#start = time.time()\n",
    "pool = mp.Pool(processes=4)\n",
    "tokenized_reuters_mp = dict(pool.map(remove_stpwds, reuters.fileids()))\n",
    "#end = time.time()\n",
    "#end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list comprehension \n",
    "```python\n",
    "    [w for w in tokenized_reuters[fileid] if w not in stopwords]\n",
    "```\n",
    "says: For each word in each Reuters article, keep the word if the word is not contained in the stopword list.\n",
    "\n",
    "Checking for membership of a value in a list takes time proportional to the list's length in the average and worst cases. \n",
    "It causes the above code to run quite slow as we need to do the check for every word in each Reuters article\n",
    "and the size of the stopword list is large.\n",
    "However, if you have hashable items, which means both the item order and duplicates are disregarded, \n",
    "Python `set` is better choice than `list`. The former runs much faster than the latter in terms of searching\n",
    "a large number of hashable items. Indeed, `set` takes constant time to check the membership.\n",
    "Let's try converting the stopword list into a stopword set, then search to remove all the stopwords.\n",
    "Please also note that if you try to perform iteration, `list` is much better than `set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance\n",
    "1. 0.362 (mp_set) vs 1.81s (mp_list) \n",
    "2. 1.81s (mp_list) v.s 6.78s (for-loop_list)\n",
    "3. 0.055 (for-loop_set) v.s 6.78s (for-loop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsSet = set(stopwords_list)\n",
    "#start = time.time()\n",
    "for fileid in reuters.fileids():\n",
    "    # filter out those stopwords\n",
    "    tokenized_reuters[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwordsSet]\n",
    "#end = time.time()\n",
    "#end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above stopping process, 481 stopwords have been removed from the vocabulary. You might wonder what those removed words are.  \n",
    "\n",
    "It is quite easy to check those words by differentiating the vocabulary before and after stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['therefore',\n",
       " 'plus',\n",
       " 'currently',\n",
       " 'thus',\n",
       " 'anybody',\n",
       " 'little',\n",
       " 'used',\n",
       " 'more',\n",
       " 'very',\n",
       " 'appropriate']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = list(chain.from_iterable(tokenized_reuters.values())) # without stpwds\n",
    "fd_3 = FreqDist(words_3)\n",
    "\n",
    "# check which stpwds has been ignored\n",
    "ign_stpwds = vocab - set(fd_3.keys()) \n",
    "print(len(ign_stpwds))\n",
    "list(ign_stpwds)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside **stopwords**, there might some other words that occur quite often as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mln', 18598),\n",
       " ('dlrs', 12329),\n",
       " ('pct', 9771),\n",
       " ('lt', 8696),\n",
       " ('cts', 8308),\n",
       " ('net', 6986),\n",
       " ('year', 6687),\n",
       " ('billion', 5809),\n",
       " ('loss', 5115),\n",
       " ('company', 4593)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_3.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we decide to remove those words from our vocabulary, it might be worth checking what \n",
    "those words mean and the context of those words. Fortunately NLTK provides a `concordance`\n",
    "function in the `nltk.text` module. A concordance view shows us every occurrence of a given \n",
    "word, together with the corresponding context. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.Text(reuters.words()).concordance('mln')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.Text(reuters.words()).concordance('net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing those words, you might also want to remove them from the vocabulary. \n",
    "We will leave it as an excersie for you to do on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Less Frequent Words\n",
    "If the most common words do not benefit the downstream text analysis tasks, except for contributing noises, **how about the words that occur once or twice?**\n",
    "\n",
    "Here another interesting statistic to look at is the frequency of the frequencies of word types in a given corpus. We would like to see **how many words appear only once**, **how many words appear twice**, **how many words appear three times**, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a10ea5cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ffd = FreqDist(fd_3.values())# all words from different articles without stpwds \n",
    "from pylab import *\n",
    "\n",
    "y = [0]*14 #slot for storing data\n",
    "\n",
    "for k, v in ffd.items(): # k: frequencies, v: times\n",
    "    if k <= 10:\n",
    "        y[k-1] = v\n",
    "    elif k >10 and k <= 50:\n",
    "        y[10] =  y[10] + v\n",
    "    elif k >50 and k <= 100:\n",
    "        y[11] =  y[11] + v\n",
    "    elif k > 100 and k <= 500:\n",
    "        y[12] =  y[12] + v\n",
    "    else:\n",
    "        y[13] =  y[13] + v\n",
    "        \n",
    "x = range(1, 15) # generate integer from 1 to 14\n",
    "\n",
    "# convert a integer list to a string list:['1','2', ... '9', '10']\n",
    "ytks =list(map(str, range(1, 11)))\n",
    "\n",
    "ytks.append('10-50')\n",
    "ytks.append('51-100')\n",
    "ytks.append('101-500')\n",
    "ytks.append('>500')\n",
    "\n",
    "barh(x,y, align='center')\n",
    "yticks(x, ytks)\n",
    "\n",
    "xlabel('Frequency of Frequency')\n",
    "ylabel('Word Frequency')\n",
    "\n",
    "grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bar chart generated above shows **how many word types occur with a certain frequency**. There are 241 types occurring over 500 times and therefore individually accounting for about 1% of the vocabulary. However, on the other extreme, **more than one-third** of the word types occur **only once** in the Reuters corpus.\n",
    "\n",
    "Note that the majority of word types occur quite infrequently given the size of the whole corpus (i.e., 721,371 word tokens):\n",
    "about 78% of the word types occur 10 times or less.\n",
    "\n",
    "Similarly, you can also look at the bar chart based on the document frequency. Try it by yourself!\n",
    "\n",
    "Let's further remove those words that occur only once. \n",
    "To get those words, you can write the code like\n",
    "```python\n",
    "    lessFreqWords = set([k for k, v in fdist.items() if v < 2])\n",
    "```\n",
    "or choose to use `hapaxes()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hapaxes**: The words that occur once only.  \n",
    "View them by typing `FreqDist().hapaxes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cour'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(fd_3.hapaxes()))[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessFreqWords = set(fd_3.hapaxes()) # set is faster\n",
    "\n",
    "def removeLessFreqWords(fileid):\n",
    "    #filter out the less-freq-words\n",
    "    return (fileid, [w for w in tokenized_reuters[fileid] if w not in lessFreqWords])\n",
    "\n",
    "#pool = mp.Pool(4)\n",
    "#tokenized_reuters = dict(pool.map(removeLessFreqWords, reuters.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no stp-wds, no less-freq-wds\n",
    "tokenized_reuters = dict(removeLessFreqWords(fileid) for fileid in reuters.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have a pretty clean set of Reuters articles, each of which is stored as a list of word tokens.\n",
    "Let's further print out some statistics that summarize this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  17403\n",
      "Total number of tokens:  711311\n",
      "Lexical diversity:  45.61580303464071\n",
      "Total number of articles: 10788\n",
      "Average document length: 65.93539117538005\n",
      "Maximun document length: 703\n",
      "Minimun document length: 0\n",
      "Standard deviation of document length: 65.71555877530848\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "vocab = set(words)# no stp-wds, no less-freq-wds\n",
    "\n",
    "print (\"Vocabulary size: \",len(vocab))\n",
    "print (\"Total number of tokens: \", len(words))\n",
    "print (\"Lexical diversity: \", lexical_diversity)\n",
    "print (\"Total number of articles:\", len(tokenized_reuters))\n",
    "\n",
    "lens = [len(value) for value in tokenized_reuters.values()]\n",
    "\n",
    "print (\"Average document length:\", np.mean(lens))\n",
    "print (\"Maximun document length:\", np.max(lens))\n",
    "print (\"Minimun document length:\", np.min(lens))\n",
    "print (\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the minimun document length is 0. There must be some Reuters articles that are extremely short,\n",
    "after tokenization and stopping, there are no words left. Can you check those documents to see what they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nempty_article_list = []\\nfor k,v in tokenized_reuters.items():\\n    if len(v)==0:\\n        empty_article_list.append(k)\\npick = empty_article_list[59]\\n\\nprint(tokenized_reuters[pick])\\n\\nreuters.raw(pick)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check some empty article\n",
    "'''\n",
    "empty_article_list = []\n",
    "for k,v in tokenized_reuters.items():\n",
    "    if len(v)==0:\n",
    "        empty_article_list.append(k)\n",
    "pick = empty_article_list[59]\n",
    "\n",
    "print(tokenized_reuters[pick])\n",
    "\n",
    "reuters.raw(pick)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Vector Representation\n",
    "\n",
    "After text pre-processing has been completed, each individual document needs to be transformed into some kind of numeric representation that can be input into most NLP and text mining algorithms.\n",
    "\n",
    "For example, classification algorithms, such as Support Vector Machine, can only take data **in a structured and numerical form**. They do not accept free languge text.\n",
    "\n",
    "The most popular structured representation of text is **the vector-space model**, which represents text as a **vector** where the elements of the vector indicate **the occurence of words within the text**.\n",
    "\n",
    "The vector-space model makes an implicit assumption that **the order of words** in a text document are **not as important as words themselves**, and thus disregarded. This assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model):\n",
    "\n",
    "> **Bag-of-words**: A text (sentence or document) is represented as the bag (**multiset**) of its words, disregarding **grammar** and even **word order** but keeping **multiplicity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of documents and a pre-defined list of words appearing \n",
    "in those documents (i.e., a vocabulary), you can compute a vector representation for each document. This vector representation can take one of the following **three forms**:\n",
    "\n",
    "* a **binary representation**,\n",
    "* an **integer count**,\n",
    "* and a **float-valued weighted vector**.\n",
    "\n",
    "To highlight the difference among the three approaches, we use a very simple example as follows:\n",
    "```\n",
    "    document_1: \"Data analysis is important.\"\n",
    "    document_2: \"Data wrangling is as important as data analysis.\"\n",
    "    document_3: \"Data science contains data analysis and data wrangling.\"\n",
    "```\n",
    "The three documents contain 20 tokens and 9 unique words.\n",
    "Those unique words are sorted alphabetically with total counts:\n",
    "```\n",
    "     'analysis': 3,\n",
    "     'and': 1,\n",
    "     'as': 2,\n",
    "     'contains': 1,\n",
    "     'data': 6,\n",
    "     'important': 2,\n",
    "     'is': 2,\n",
    "     'science': 1,\n",
    "     'wrangling': 2\n",
    "```\n",
    "Given the vocabulary above, both the binary and the integer count vectors are easy to compute. A binary vector stores 1s for the word that appears in a document and 0s for the other words in the vocabulary, whereas a count vector stores the frequency of each word appearing in the document.\n",
    "Thus, **the binary vector representations** for the three documents above are\n",
    "   \n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|1|0|1|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|1|0|0|1|1|\n",
    "\n",
    "**The count vector representations** for the same documents would look as follows:\n",
    "\n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|2|0|2|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|3|0|0|1|1|\n",
    "\n",
    "Instead of using the two vector representations above, \n",
    "**most existing text analysis algorithms**, like document classification and information retrieval, **prefer** representing documents as **weighted vectors**.\n",
    "The raw term frequency is often replaced with a weighted term frequency\n",
    "that indicates **how important a word is in a particular document**.\n",
    "\n",
    "There are many different term weighting schemes online.\n",
    "To store each document as a weighted vector, we first need to **choose a weighting scheme**. \n",
    "The most popular scheme is <font color='blue'>**the TF-IDF weighting approach**</font>. \n",
    "\n",
    "<font color='blue'>**TF-IDF**</font> stands for **Term Frequency-Inverse Document Frequency**. \n",
    "\n",
    "<font color='blue'>**The term frequency**</font> for a word is **the number of times the word appears** in a document. \n",
    "In the preceding example, the term frequency in Document 2 for “data” is 2, since it appears twice in the document.  \n",
    "<font color='blue'>**Document frequency**</font> for a word is **the number of documents that contain the word**; \n",
    "it would also be 3 for “data” in the collection of the three preceding documents. \n",
    "The Wikipidia entry on [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) lists \n",
    "a number of variants of TF-IDF. \n",
    "\n",
    "One variant is reproduced here:  \n",
    "$tf(): Term Frequency$  \n",
    "$idf(): Inverse Document Freqency$  \n",
    "\n",
    "$$tf\\cdot idf(w,d) = tf(w, d) * idf(w)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$tf(w,d)\\,=\\, \\sum_{i}^{|d|} 1_{w = w_{d,i}}$$\n",
    "and\n",
    "$$idf(w) = log\\left(\\frac{|D|}{|d \\in D: w \\in d |}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption behind TF-IDF is that **words with high term frequency should receive high weight unless they also have high document frequency**. \n",
    "\n",
    "**Stopwords** are **the most commonly occurring words** in the English language. They often occur many times within a single document, but they also **occur in nearly every document**. \n",
    "These two competing effects cancel out to **give them low weights**, **as those very common words** carry very little meaningful information about the actual contents of the document.   \n",
    "\n",
    "Therefore, the TF-IDF weights for stopwords are almost **always 0**.\n",
    "\n",
    "With the TF-DF formulas above, the weighted vector representations for the example documents are computed as\n",
    "\n",
    "||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|0|0|0|0|0|0.176|0.176|0|0|\n",
    "   |document 2:|0|0|0.954|0|0|0.176|0.176|0|0.176|\n",
    "   |document 3:|0|0.477|0|0.477|0|0|0|0.477|0.176|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the cleaned up Reuters documents, how can we generate those vectors for each documents? \n",
    "\n",
    "Unfortunately, NLTK does not implement methods that directly produce those vectors.\n",
    "Therefore, we will either write our own code to compute them or appeal to other data analysis libraries.\n",
    "\n",
    "Here we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine learning library for Python.\n",
    "\n",
    "If you use Anaconda, you should already have scikit-learn installed, otherwise you will need to [install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n",
    "\n",
    "Although scikit-learn features various classification, regression and clustering algorithms we are particularly interested in its **feature extraction** module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\n",
    "\n",
    "This module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as **text** and **image**.\" Please refer to its documentation on text feature extraction, section 4.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). We will demonstrate the usage of the following two classes:\n",
    "\n",
    "1. [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of **text documents** to a matrix of **token counts**. \n",
    "2. [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer): It converts a collection of **raw documents** to a matrix of **TF-IDF features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 <font color='red'>Count Vectors</font>\n",
    "Let's start with generating **the count vector representation** for each Reuters document.\n",
    "\n",
    "Initialise the `CountVector` object:   \n",
    "since we have pre-processed all the Reuters documents, the parameters: \n",
    "1. `tokenizer`: callable or None (default)\n",
    "2. `preprocessor`: callable or None (default)\n",
    "3. `stop_words`: str. list. None (default) \n",
    " \n",
    "are set to their default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **transform Reuters articles into feature vectors**.  \n",
    "`fit_transform` does two things:  \n",
    "1. it **fits the model** and **learns the vocabulary**; \n",
    "2. it transforms the text data into **feature vectors**. \n",
    "\n",
    "Please note the input to `fit_transform` should be a list of strings. \n",
    "Since we have stored each tokenised article as a list of words, we concatenate all the words in the list and separate them with white spaces. \n",
    "\n",
    "The following code will do that:\n",
    "```python\n",
    "[' '.join(value) for value in tokenized_reuters.values()]\n",
    "```\n",
    "Then, we input this list of strings into `fit_transform`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10788, 17403)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform(\n",
    "    [' '.join(value) for value in tokenized_reuters.values()]\n",
    ")\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of document-by-word matrix should be $10788 \\times 17403$. \n",
    "However, in order to save such a matrix in memory but also to speed up algebraic operations on the matrix, `scikit-learn` implements matrix/vector in a **sparse representation**.  \n",
    "\n",
    "Let's check the count vector for the first article, i.e., `'training/1684'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10788x17403 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 457415 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultramar : 1\n",
      "hurdles : 1\n",
      "facing : 1\n",
      "aims : 2\n",
      "stafford : 1\n",
      "penetrate : 1\n",
      "ditch : 1\n",
      "empresa : 1\n",
      "goedf : 2\n",
      "vamgas : 1\n",
      "mediterranee : 1\n",
      "tied : 2\n",
      "macomson : 1\n",
      "surge : 1\n",
      "slim : 1\n",
      "chv : 1\n",
      "metall : 1\n",
      "squeezes : 1\n",
      "felix : 5\n",
      "kitty : 1\n",
      "syeduzzaman : 2\n",
      "lanston : 1\n",
      "stearns : 1\n",
      "resilient : 1\n",
      "contrast : 4\n",
      "reaffirming : 2\n",
      "ultrasystems : 1\n",
      "allocates : 1\n",
      "butter : 1\n",
      "pointing : 1\n",
      "tdri : 1\n",
      "pesch : 1\n",
      "alimullah : 1\n",
      "sww : 1\n",
      "televisions : 1\n",
      "hurting : 1\n",
      "dubin : 1\n",
      "kuna : 1\n",
      "reservations : 1\n",
      "raimond : 1\n",
      "hesitant : 1\n",
      "guardman : 1\n",
      "rationale : 2\n",
      "uncoupling : 1\n",
      "ava : 2\n",
      "wrongly : 1\n",
      "filling : 2\n",
      "andover : 1\n",
      "chaparral : 1\n",
      "collecting : 1\n",
      "unit : 2\n",
      "tinto : 1\n",
      "bow : 1\n",
      "res : 1\n",
      "funding : 2\n",
      "thinking : 6\n",
      "peanut : 1\n",
      "building : 1\n",
      "airco : 2\n",
      "signed : 2\n",
      "industries : 1\n",
      "disturbance : 1\n",
      "air : 4\n",
      "cobalt : 1\n",
      "kullberg : 1\n",
      "fairer : 1\n",
      "eiu : 1\n",
      "disadvantages : 1\n",
      "centennial : 1\n",
      "yoot : 1\n",
      "contac : 3\n",
      "smit : 1\n",
      "schneider : 6\n",
      "dateline : 1\n",
      "doresy : 1\n",
      "currency : 1\n",
      "lly : 1\n",
      "preceded : 1\n",
      "spiralled : 1\n",
      "chfd : 1\n",
      "zorinsky : 1\n",
      "alden : 1\n",
      "bismuth : 1\n",
      "lighter : 1\n",
      "compromise : 1\n",
      "thaw : 1\n",
      "disincentives : 2\n",
      "supplement : 1\n",
      "pound : 1\n",
      "helping : 1\n",
      "reallocated : 1\n",
      "downtown : 1\n",
      "coceral : 1\n",
      "plantation : 4\n",
      "boundaries : 1\n",
      "natsuo : 1\n",
      "dublin : 1\n",
      "bd : 5\n",
      "sarney : 1\n",
      "norcen : 1\n",
      "awaited : 1\n",
      "fee : 3\n",
      "politburo : 1\n",
      "telc : 1\n",
      "neptunia : 1\n",
      "kirschner : 12\n",
      "schrader : 4\n",
      "slightly : 1\n",
      "denied : 1\n",
      "natl : 4\n",
      "alarms : 3\n",
      "customer : 1\n",
      "greenbaum : 1\n",
      "error : 2\n",
      "auction : 1\n",
      "permissible : 2\n",
      "oke : 1\n",
      "mp : 1\n",
      "matsushita : 1\n",
      "dram : 1\n",
      "blessing : 3\n",
      "socanav : 2\n",
      "controversial : 1\n",
      "honan : 1\n",
      "ferc : 1\n",
      "unusually : 1\n",
      "lebanese : 1\n",
      "anticancer : 2\n",
      "wing : 1\n",
      "lid : 1\n",
      "attribute : 1\n",
      "statfjord : 2\n",
      "showpiece : 1\n",
      "technologies : 1\n",
      "mnst : 1\n",
      "rachmat : 3\n",
      "historically : 1\n",
      "differ : 1\n",
      "assessments : 1\n",
      "financiere : 1\n",
      "zayre : 1\n",
      "strohmeyer : 1\n",
      "kaohsiung : 1\n",
      "jonathan : 1\n",
      "peoria : 1\n",
      "suntrust : 1\n",
      "grant : 1\n",
      "baybanks : 1\n",
      "airlines : 2\n",
      "mns : 2\n",
      "parana : 1\n",
      "bugs : 1\n",
      "raul : 1\n",
      "manitoba : 1\n",
      "alligator : 1\n",
      "affiliates : 1\n",
      "displaced : 1\n",
      "whitehall : 1\n",
      "goodman : 1\n",
      "trauma : 2\n",
      "essex : 1\n",
      "downs : 2\n",
      "insele : 1\n",
      "alloyed : 1\n",
      "valentchits : 1\n",
      "pepsi : 1\n",
      "alex : 1\n",
      "boltz : 3\n",
      "mccall : 1\n",
      "evidence : 1\n",
      "indicator : 1\n",
      "settlements : 1\n",
      "canterra : 1\n",
      "expiry : 1\n",
      "atlantico : 1\n",
      "taylor : 1\n",
      "vmlpz : 1\n",
      "greater : 1\n",
      "french : 1\n",
      "sail : 1\n",
      "remains : 1\n",
      "lots : 1\n",
      "successive : 1\n",
      "korea : 1\n",
      "compare : 1\n",
      "wilkinson : 2\n",
      "counterbalance : 1\n",
      "lmfe : 1\n",
      "equine : 1\n",
      "jcp : 1\n",
      "byrd : 1\n",
      "tsb : 1\n",
      "frederick : 1\n",
      "politics : 1\n",
      "transportation : 3\n",
      "fixtures : 2\n",
      "comprise : 1\n",
      "wellemeyer : 1\n",
      "margin : 1\n",
      "newmont : 1\n",
      "padding : 1\n",
      "profitable : 1\n",
      "icahn : 1\n",
      "pittston : 1\n",
      "overfunded : 1\n",
      "forbid : 3\n",
      "meares : 2\n",
      "avgprice : 1\n",
      "ordered : 1\n",
      "sna : 1\n",
      "helena : 1\n",
      "subsidizing : 1\n",
      "doyon : 1\n",
      "fractional : 1\n",
      "uncommon : 3\n",
      "ingemar : 1\n",
      "maple : 4\n",
      "ties : 1\n",
      "areas : 1\n",
      "fourth : 5\n",
      "automakers : 1\n",
      "hcl : 1\n",
      "seamen : 1\n",
      "reading : 1\n",
      "realized : 2\n",
      "statutory : 1\n",
      "earned : 1\n",
      "erskine : 1\n",
      "ounce : 15\n",
      "renovation : 1\n",
      "williams : 1\n",
      "shallow : 2\n",
      "conceded : 1\n",
      "guardian : 1\n",
      "declined : 1\n",
      "faults : 1\n",
      "included : 1\n",
      "roost : 2\n",
      "invested : 1\n",
      "withstand : 1\n",
      "ecuadorian : 4\n",
      "providence : 1\n"
     ]
    }
   ],
   "source": [
    "# the vocab using here: no stp-wds, no less-freq-wds\n",
    "for word, count in zip(vocab, data_features.toarray()[0]):\n",
    "    if count > 0:\n",
    "        print (word, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get the count list above is to use `FreqDist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'commission': 1,\n",
       "          'common': 2,\n",
       "          'company': 1,\n",
       "          'control': 1,\n",
       "          'cuts': 1,\n",
       "          'dealings': 1,\n",
       "          'dlrs': 1,\n",
       "          'erc': 5,\n",
       "          'exchange': 1,\n",
       "          'filing': 1,\n",
       "          'intention': 1,\n",
       "          'international': 2,\n",
       "          'investment': 2,\n",
       "          'jan': 1,\n",
       "          'lowered': 1,\n",
       "          'lt': 1,\n",
       "          'march': 1,\n",
       "          'nevada': 1,\n",
       "          'outstanding': 1,\n",
       "          'parsow': 2,\n",
       "          'partnership': 4,\n",
       "          'pct': 2,\n",
       "          'prices': 1,\n",
       "          'purposes': 1,\n",
       "          'ranging': 1,\n",
       "          'securities': 1,\n",
       "          'seeking': 1,\n",
       "          'shares': 3,\n",
       "          'sold': 1,\n",
       "          'stake': 2,\n",
       "          'stock': 2,\n",
       "          'total': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(tokenized_reuters['training/1684'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocabulary you just got with `vectorizer.get_feature_names()`  shoud be exactly the same\n",
    "as the one you got in section 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2 = vectorizer.get_feature_names()\n",
    "list(vocab-set(vocab2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <font color='red'>TF-IDF Vectors</font>\n",
    "Similar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying \n",
    "the value of \"analyzer\", and then covert the Reuters data into a list of strings, each of which corresponds\n",
    "to a Reuters articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 17403)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\")\n",
    "tfs = tfidf.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\n",
    "tfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the weighted vector for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10788x17403 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 457415 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting : 0.038235730031632956\n",
      "action : 0.030603756662730266\n",
      "advantage : 0.043914840084456494\n",
      "alleged : 0.08925586636134304\n",
      "american : 0.027671241326262693\n",
      "analyst : 0.034717404878887824\n",
      "april : 0.020867068588964373\n",
      "asia : 0.043780644169735164\n",
      "asian : 0.08810338703203563\n",
      "asked : 0.030564378365548584\n",
      "association : 0.03139802300392408\n",
      "australia : 0.07291864821824522\n",
      "australian : 0.03703556377377631\n",
      "awaiting : 0.048841945274933685\n",
      "aware : 0.045948291554218285\n",
      "barriers : 0.0435198255765529\n",
      "beef : 0.04202909114182352\n",
      "biggest : 0.03991321083427934\n",
      "billion : 0.09865210654969163\n",
      "block : 0.04071770792857356\n",
      "boost : 0.06987132219065047\n",
      "broker : 0.045948291554218285\n",
      "budget : 0.03324004443226463\n",
      "business : 0.026607771245008217\n",
      "businessmen : 0.18102651014822307\n",
      "button : 0.13259386759447706\n",
      "call : 0.036789599291540416\n",
      "canberra : 0.06349481614925918\n",
      "capel : 0.050738543313134155\n",
      "capitals : 0.05671643027940679\n",
      "centred : 0.05590244674236398\n",
      "chairman : 0.0270419853614545\n",
      "chief : 0.032956772300091565\n",
      "coal : 0.0423456750026592\n",
      "commercial : 0.03353542582172308\n",
      "complete : 0.03771932513432333\n",
      "concern : 0.035237823865711976\n",
      "concerns : 0.0435198255765529\n",
      "conflict : 0.047135926761575055\n",
      "continue : 0.028595905355944744\n",
      "correspondents : 0.05671643027940679\n",
      "cost : 0.031375908193189736\n",
      "countries : 0.05481032408618094\n",
      "country : 0.02961217277183705\n",
      "curbs : 0.0938465355064654\n",
      "cut : 0.027645454458816766\n",
      "damage : 0.07745914763527406\n",
      "day : 0.030584039466881263\n",
      "defuse : 0.05517431365662817\n",
      "democratic : 0.045948291554218285\n",
      "deputy : 0.08109828341397356\n",
      "deterioration : 0.047135926761575055\n",
      "diplomatic : 0.04735533796630917\n",
      "disadvantage : 0.05590244674236398\n",
      "dispute : 0.07695648333440674\n",
      "dlrs : 0.08064458082945693\n",
      "domestically : 0.0503840618977123\n",
      "due : 0.025561467598290363\n",
      "economic : 0.054297897098674185\n",
      "economy : 0.0619372763783101\n",
      "effort : 0.03835592214732308\n",
      "electric : 0.03991321083427934\n",
      "electronics : 0.16026716839278932\n",
      "emergency : 0.043268493426119235\n",
      "end : 0.025764316991845566\n",
      "erosion : 0.05391431263142745\n",
      "estimates : 0.03447118999818609\n",
      "exchange : 0.023341662438252864\n",
      "expand : 0.03755542324374333\n",
      "export : 0.02716093618886878\n",
      "exporters : 0.1070283720576721\n",
      "exporting : 0.0425650862073933\n",
      "exports : 0.1607237071113289\n",
      "extended : 0.03744827578804067\n",
      "failure : 0.03983768142175566\n",
      "fear : 0.044191311849579286\n",
      "fears : 0.04182581553269218\n",
      "federation : 0.042677460593225774\n",
      "financial : 0.026188802215869\n",
      "firm : 0.02921335794904132\n",
      "firms : 0.03381251350168583\n",
      "fiscal : 0.030885872260308166\n",
      "foreign : 0.02526337837392109\n",
      "friction : 0.04857089592865102\n",
      "friday : 0.03315383700518558\n",
      "gain : 0.02740516204309047\n",
      "goods : 0.06542278020957684\n",
      "government : 0.023924809522229435\n",
      "group : 0.02198567553036606\n",
      "half : 0.02985590638114546\n",
      "halt : 0.04433380911359572\n",
      "helped : 0.038924259155361275\n",
      "hit : 0.03531547493497943\n",
      "hong : 0.1618655031125596\n",
      "hurt : 0.04182581553269218\n",
      "impact : 0.033875626930809054\n",
      "import : 0.03263149473567427\n",
      "imports : 0.14210468607034749\n",
      "impose : 0.04202909114182352\n",
      "include : 0.029595087787963204\n",
      "industrial : 0.029646469976012446\n",
      "industry : 0.08098491732704524\n",
      "interest : 0.02461963713407169\n",
      "international : 0.023225280458743985\n",
      "james : 0.03486214048218121\n",
      "japan : 0.31539909028696617\n",
      "japanese : 0.12095369531503587\n",
      "john : 0.03811759258295876\n",
      "kind : 0.043780644169735164\n",
      "kong : 0.1635619549575162\n",
      "korea : 0.11906716434130579\n",
      "kuroda : 0.05763924870286683\n",
      "large : 0.031785224201264284\n",
      "largest : 0.06235999265476794\n",
      "lawrence : 0.05111219498344811\n",
      "lead : 0.06794278225997828\n",
      "leading : 0.03447118999818609\n",
      "length : 0.05284899694395097\n",
      "liberal : 0.04671695773243584\n",
      "loss : 0.021092709734134078\n",
      "lt : 0.03265736563001262\n",
      "major : 0.05063659900737532\n",
      "makoto : 0.05763924870286683\n",
      "malaysia : 0.041926705973519975\n",
      "manoeuvres : 0.061506682038322665\n",
      "manufacturers : 0.04097852652175582\n",
      "market : 0.020838172912900032\n",
      "markets : 0.055735544540417875\n",
      "matsushita : 0.061506682038322665\n",
      "matter : 0.04182581553269218\n",
      "measure : 0.039261441757101945\n",
      "measures : 0.06813700201184225\n",
      "meet : 0.03309695707539404\n",
      "michael : 0.04106770030684036\n",
      "mills : 0.044051693516017815\n",
      "minister : 0.08392475917773479\n",
      "miti : 0.048310077335468764\n",
      "mln : 0.012442151455344176\n",
      "months : 0.025254267136665703\n",
      "mounting : 0.04941818493958739\n",
      "move : 0.030332707316447608\n",
      "murtha : 0.059964565415544034\n",
      "nakasone : 0.04509401127059949\n",
      "named : 0.03968905478043526\n",
      "nations : 0.03195118794778464\n",
      "newspapers : 0.04972538540121205\n",
      "office : 0.035237823865711976\n",
      "officers : 0.04671695773243584\n",
      "official : 0.05200735698354712\n",
      "officials : 0.05473594680402399\n",
      "open : 0.03133188974846607\n",
      "outcome : 0.04477982660175362\n",
      "outlined : 0.0469232677532327\n",
      "outweighed : 0.05763924870286683\n",
      "package : 0.03905716464482311\n",
      "pact : 0.03335668639874077\n",
      "partners : 0.03447118999818609\n",
      "party : 0.039330898346337496\n",
      "paul : 0.0396159231725238\n",
      "pct : 0.030564328834757053\n",
      "place : 0.035513507752767765\n",
      "pressure : 0.06725311905281636\n",
      "prevent : 0.038858752764836996\n",
      "prime : 0.033781171622764214\n",
      "problems : 0.03413400739644541\n",
      "produced : 0.036322470677561294\n",
      "producers : 0.03169214707099988\n",
      "products : 0.08105531677455703\n",
      "program : 0.03128814990335975\n",
      "promotion : 0.0503840618977123\n",
      "proposed : 0.02927824924700526\n",
      "protectionist : 0.04054914170698678\n",
      "public : 0.03040907132500602\n",
      "purpose : 0.046321943224532244\n",
      "put : 0.03212123556794458\n",
      "quickly : 0.04022386414256948\n",
      "raised : 0.03176183713171253\n",
      "record : 0.02279753166321871\n",
      "reform : 0.0416283486000593\n",
      "relations : 0.041436367575598765\n",
      "remain : 0.032195413802006335\n",
      "remove : 0.04525662753705577\n",
      "representative : 0.03860276526060169\n",
      "reserves : 0.030428294904737016\n",
      "restraining : 0.05284899694395097\n",
      "retaliation : 0.0799791496856755\n",
      "reuter : 0.04182581553269218\n",
      "rift : 0.061506682038322665\n",
      "row : 0.042791692490817095\n",
      "ruling : 0.0396159231725238\n",
      "safe : 0.05517431365662817\n",
      "sales : 0.02175687322771273\n",
      "sell : 0.027788487968998913\n",
      "selling : 0.03353542582172308\n",
      "semiconductors : 0.13094701357125857\n",
      "senior : 0.06749994248529811\n",
      "sentiment : 0.0435198255765529\n",
      "seriousness : 0.061506682038322665\n",
      "serves : 0.048841945274933685\n",
      "share : 0.0196127002248661\n",
      "significant : 0.03559433759182549\n",
      "similar : 0.036144115683152095\n",
      "smith : 0.04364900452375286\n",
      "solve : 0.04576877828067168\n",
      "sources : 0.028264350500815716\n",
      "south : 0.09406159177270869\n",
      "spending : 0.07319788488536816\n",
      "spokesman : 0.026432803103132972\n",
      "spokesmen : 0.05150720949135171\n",
      "stick : 0.050738543313134155\n",
      "stimulate : 0.04106770030684036\n",
      "stock : 0.021082744523384763\n",
      "subject : 0.03062353027409489\n",
      "supplementary : 0.05590244674236398\n",
      "surplus : 0.09341161323222547\n",
      "swell : 0.06349481614925918\n",
      "taiwan : 0.1551754458674922\n",
      "taiwanese : 0.05336114768756689\n",
      "talks : 0.029476661126991384\n",
      "tariffs : 0.17817554909489694\n",
      "taxes : 0.03912458832554064\n",
      "textile : 0.04735533796630917\n",
      "threat : 0.042677460593225774\n",
      "time : 0.02655261634365745\n",
      "tokyo : 0.07160129522524471\n",
      "told : 0.022216443964373063\n",
      "tom : 0.04972538540121205\n",
      "tough : 0.043780644169735164\n",
      "trade : 0.3403484004161321\n",
      "trading : 0.028056384784645044\n",
      "unofficial : 0.05671643027940679\n",
      "view : 0.07447767876581532\n",
      "virtually : 0.04462793318067152\n",
      "warning : 0.047135926761575055\n",
      "washington : 0.03306869170896893\n",
      "week : 0.02413840757160281\n",
      "works : 0.04651662856225648\n",
      "world : 0.05295226319805183\n",
      "worried : 0.04651662856225648\n",
      "yasuhiro : 0.04559381013879644\n",
      "year : 0.059426982680502526\n",
      "yesterday : 0.02754325854911457\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "for word, weight in zip(vocab, tfs.toarray()[0]):\n",
    "    if weight > 0:\n",
    "        print (word, \":\", weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have converted all the Reuters articles into feature vectors. \n",
    "We can use those vectors to, for example,\n",
    "* compute the **similarity** between two articles, \n",
    "* search articles for a given **query**\n",
    "* do other advance text analysis, such as **document classification** and **clustering**.\n",
    "\n",
    "Assume that we have a new document, how can we get its **TF-IDF vector**?\n",
    "\n",
    "We do this by using the transform function as follows. We have randomly chosen a sentence from [a recent Reuters news](http://www.reuters.com/article/us-usa-election-idUSKCN0W346T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win  -  0.307399276859104\n",
      "vermont  -  0.4040204786477111\n",
      "step  -  0.25696902690642953\n",
      "states  -  0.17542070972270207\n",
      "state  -  0.18734841995701546\n",
      "senator  -  0.32522928223627556\n",
      "secretary  -  0.20107422757443014\n",
      "nomination  -  0.39136988850775073\n",
      "hoped  -  0.2667815869522921\n",
      "fight  -  0.3013242256926447\n",
      "democratic  -  0.2923711237015126\n",
      "big  -  0.252077968138518\n"
     ]
    }
   ],
   "source": [
    "astr = \"\"\"\n",
    "the former secretary of state hoped to win enough states to take a big step toward wrapping up her nomination fight\n",
    "with a democratic senator from Vermont.\n",
    "\"\"\"\n",
    "response = tfidf.transform([astr])\n",
    "for col in response.nonzero()[1]:\n",
    "    print (vocab[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the text above is **not included** in the trained TF-IDF model with the 'transform' function, **unless** the `fit_transform` function is called,\n",
    "\n",
    "Both `CountVectorizer` and `TfidfVectorizer` come with their own options to automatically do pre-processing, tokenization, and stop word removal -- for each of these, instead of using their default value (i.e., None),\n",
    "we could customise the two vectorizer classes by either using a built-in method or specifying our own function.\n",
    "See the function documentation for more details.\n",
    "However, we wanted to write our own function for clean the text data in this chapter to show you how \n",
    "it's done step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving Pre-processed Text to a File\n",
    "The pre-processed text needs to be saved in a proper format so that it can be easily used by the downstream analysis algorithm. There are a couple of ways of dumping the pre-processed text data into txt files. \n",
    "\n",
    "For example, use one txt file to store the tokenized documents. The tokens in a document are stored in one row in the txt file, and are separated with a given delimiter, e.g., whitespace.  \n",
    "\n",
    "In this case, the downstream text analyser needs to re-construct the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"./reuters_1.txt\", 'w')\n",
    "for d in tokenized_reuters.values():\n",
    "    out_file.write(' '.join(d) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save vocabulary in a separate file, and assign a fixed integer **id** to each word in the vocabulary. What text analysers usually do is to use **the index of each word** in the vocabulary as its integer id.\n",
    "Given the vocabulary, each document can be represented as a sequence of integers that correspond to the tokens, or in the following sparse form:\n",
    "```\n",
    "    word_index : word count\n",
    "```\n",
    "for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"./reuters_2.txt\", 'w')\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocab_dict = {}\n",
    "i = 0 #word index\n",
    "for w in vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "for d in tokenized_reuters.values():\n",
    "    d_idx = [vocab_dict[w] for w in d] #list of word index\n",
    "    for k, v in FreqDist(d_idx).items():\n",
    "        out_file.write(\"{}:{} \".format(k,v))\n",
    "    out_file.write('\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Other Features\n",
    "\n",
    "It is common for most text analysis tasks to treat documents as bags-of-words, which can **significantly simplify** the inference procedure of text analysis algorithms.  \n",
    "However, things always have pros and cons.   \n",
    "The bag-of-words representation **loses lots of information** encoded in either syntax or word order (i.e., dependencies between adjacent words in sentences.).  \n",
    "For example, representing a document as a collection of unigrams effectively disregards any word order dependence, \n",
    "which fails to capture phrases and multi-word expressions. A similar issue has been mentioned in section 2.1. of Chapter 2. \n",
    "In this section, we are going to show you how to\n",
    "* use **Part-of-Speeching (POS) tagging** to extract specific word groups, such as all nouns, verbs, etc.,\n",
    "* extract **n-grams**,\n",
    "* and extract **collocations**\n",
    "\n",
    "These features can be further used to enrich the representation of a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extracting Nouns and Verbs\n",
    "It is easy for human to tell the difference between nouns, verbs, \n",
    "adjectives and adverbs, as we have learnt them back in elementary school.  \n",
    "However, how can we automatically **classify words into their parts of speech** (i.e., lexical categories or word classes) \n",
    "and **label** them accordingly with computer program?  \n",
    "\n",
    "This section is not going to discuss how to determine the category of a word from a linguistic perspective.\n",
    "\n",
    "Instead it demonstrates the use of some existing **POS taggers** to extract words in a specific lexical category.\n",
    "It has been proven that words together with their part-of-speech (POS) are quite useful for many language processing tasks. \n",
    "\n",
    "In NLP, the process of labelling words with their corresponding part-of-speech (POS) tags is known as [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).\n",
    "> **POS tagging**: mark up a word in a text (corpus) as corresponding to a **particular part of speech**, based on both its **definition** and its **context**.\n",
    "\n",
    "A POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context. There are many POS taggers available online, such as [Sandford POS tagger](http://nlp.stanford.edu/software/tagger.shtml). \n",
    "We are going to use the one implemented by NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nltk.word_tokenize(sentence)`: tokenize into a list of words\n",
    "2. `nltk.tag.pos_tag(words_list)`: tagging into a list of tuples, each words with a tag grouped in a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('POS', 'NNP'), ('tagger', 'NN'), ('processes', 'VBZ'), ('a', 'DT'), ('sequence', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('attaches', 'VBZ'), ('a', 'DT'), ('POS', 'NNP'), ('tag', 'NN'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('both', 'DT'), ('its', 'PRP$'), ('definition', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('context', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "example_sent = 'A POS tagger processes a sequence of words and attaches a POS tag to each \\\n",
    "word based on both its definition and its context'\n",
    "text = nltk.word_tokenize(example_sent)\n",
    "tagged_sent = nltk.tag.pos_tag(text)\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are seeing these tags for the first time, you will wonder what these tags mean. \n",
    "You can find the specification of all the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g., "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.help.upenn_tagset('TAG_NAME')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "None\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (nltk.help.upenn_tagset('NNP'))\n",
    "print (nltk.help.upenn_tagset('IN'))\n",
    "print (nltk.help.upenn_tagset('PRP$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example sentence has been processed by `pos_tag` into a list of tuples, each of which is a pair of a word and its POS tag.  \n",
    "We see that:\n",
    "1. `'a'` is `'DT'`, a determiner; \n",
    "2. `'its'` is `'PRP$'`, a possessive pronoun; \n",
    "3. `'and'` is `'CC'`, a coordinating conjunction, \n",
    "4. `'words'` is `'NNS'`, a noun in the plural form, \n",
    "and so on.  \n",
    "\n",
    "Note that several of the corpora included in NLTK have been tagged for their POS. Please click [here](http://www.nltk.org/howto/corpus.html#tagged-corpora) to see how to access those tagged corpora.\n",
    "\n",
    "Here is an example of using the `tagged_words()` function to retrieve all words in Brown corpus with their tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the collection of tags is known as a **tag set**. \n",
    "There are many different conventions for tagging words.\n",
    "Therefore, tag sets can vary among different tasks.\n",
    "What we used above is **the Penn Treebank tag set**.\n",
    "\n",
    "Let's change the tag set to **the Universal POS tag set**, and print the Brown corpus again. You will find different tags are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to learn more about **POS tagging**, please refer to [1].\n",
    "\n",
    "Given the tagged text, you can easily identify all the nouns, verbs, etc.\n",
    "Nouns generally refer to people, places, things, or concepts, e.g., Monash, Melbourne, university, data, and science. \n",
    "Nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
    "Now how can we extract all the nouns from a text?\n",
    "Assume we use the Penn Treebank tag set.\n",
    "Here are all the tags for nouns:\n",
    "```\n",
    "    NN    Noun, singular or mass\n",
    "    NNS   Noun, plural\n",
    "    NNP   Proper noun, singular\n",
    "    NNPS  Proper noun, plural\n",
    "```\n",
    "It is not hard to see all the tags above start with 'NN'.\n",
    "Thus, we can iterate over all the words and check if their tag string starts with 'NN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POS',\n",
       " 'tagger',\n",
       " 'sequence',\n",
       " 'words',\n",
       " 'POS',\n",
       " 'tag',\n",
       " 'word',\n",
       " 'definition',\n",
       " 'context']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nouns = [w for w,t in tagged_sent if t.startswith('NN')]\n",
    "all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you will find that all the verb tags start with `'VB'`, see\n",
    "```\n",
    "    VB\tVerb, base form\n",
    "    VBD   Verb, past tense\n",
    "    VBG   Verb, gerund or present participle\n",
    "    VBN   Verb, past participle\n",
    "    VBP   Verb, non-3rd person singular present\n",
    "    VBZ   Verb, 3rd person singular present\n",
    "```\n",
    "Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processes', 'attaches', 'based']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verbs = [w for w,t in tagged_sent if t.startswith('VB')]\n",
    "all_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Reuters corpus that we have been using, has no built-in POS tags. But you can get sentences from Reuters corpus, and then you can get **the POS tags**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extracting N-grams and Collocations\n",
    "\n",
    "Besides unigrams that we have been working on so far,\n",
    "**N-grams** of texts are also extensively used in various text analysis tasks.\n",
    "They are basically **contiguous sequences of `n` words** from a given sequence of text.\n",
    "When computing the n-grams you typically **move a fixed size window of size n words forward**.\n",
    "\n",
    "For example, for the sentence \"Laughter is like a windshield wiper.\"\n",
    "if **N = 2** (known as **bigrams**), the n-grams would be:\n",
    "```\n",
    "    Laughter is \n",
    "    is like \n",
    "    like a \n",
    "    a windshield \n",
    "    windshield wiper\n",
    "```\n",
    "So you have **5 bigrams** in this case. Notice that the generative process above essentially **moves one word forward to generate the next bigram.**\n",
    "\n",
    "If **N = 3** (known as **trigrams**), the n-grams would be:\n",
    "```\n",
    "    Laughter is like \n",
    "    is like a \n",
    "    like a  windshield\n",
    "    a  windshield wiper\n",
    "```\n",
    "What are N-grams used for? They can be used to build n-gram language model that can be further used for **speech recognition, spelling correction, entity detection**, etc.\n",
    "\n",
    "In terms of text mining tasks, n-grams is used for **developing features** for classification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\n",
    "\n",
    "The idea is to expand the unigram feature space with n-grams.\n",
    "But please notice that the use of bigrams and trigrams in your feature space **may not necessarily yield significant performance improvement**. \n",
    "\n",
    "The only way to know this is to try it! Extracting from a text a list of n-gram can be easily accomplished with function `ngram()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams = ngrams(reuters.words(), n = 2)\n",
    "fdbigram = FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', '000'), 10266),\n",
       " ((\"'\", 's'), 9220),\n",
       " (('lt', ';'), 8693),\n",
       " (('&', 'lt'), 8688),\n",
       " (('.', 'The'), 8530),\n",
       " (('said', '.'), 7888),\n",
       " (('of', 'the'), 6803),\n",
       " (('in', 'the'), 6487),\n",
       " (('U', '.'), 6350),\n",
       " (('.', 'S'), 5833),\n",
       " (('S', '.'), 5459),\n",
       " (('1', '.'), 4414),\n",
       " ((',', 'the'), 4296),\n",
       " (('mln', 'dlrs'), 4192),\n",
       " (('said', 'it'), 4003),\n",
       " (('mln', 'vs'), 3916),\n",
       " (('said', 'the'), 3604),\n",
       " (('.', '\"'), 3512),\n",
       " (('cts', 'vs'), 3209),\n",
       " (('.', '5'), 2779),\n",
       " (('for', 'the'), 2665),\n",
       " (('000', 'vs'), 2578),\n",
       " (('to', 'the'), 2465),\n",
       " (('1', ','), 2425),\n",
       " (('2', '.'), 2375),\n",
       " (('cts', 'Net'), 2179),\n",
       " (('.', '2'), 1983),\n",
       " (('the', 'U'), 1959),\n",
       " (('the', 'company'), 1941),\n",
       " (('.', '1'), 1924),\n",
       " (('on', 'the'), 1907),\n",
       " ((',', 'which'), 1896),\n",
       " (('.', '3'), 1841),\n",
       " ((',', 'and'), 1828),\n",
       " (('.', '4'), 1815),\n",
       " (('.', '6'), 1798),\n",
       " (('.', '8'), 1789),\n",
       " (('vs', 'loss'), 1746),\n",
       " (('.', '9'), 1726),\n",
       " (('.', '7'), 1719),\n",
       " (('.', 'It'), 1707),\n",
       " (('3', '.'), 1667),\n",
       " (('he', 'said'), 1633),\n",
       " (('will', 'be'), 1623),\n",
       " (('billion', 'dlrs'), 1596),\n",
       " (('to', 'be'), 1586),\n",
       " (('.', '0'), 1586),\n",
       " (('dlrs', 'in'), 1583),\n",
       " (('and', 'the'), 1554),\n",
       " (('by', 'the'), 1543),\n",
       " ((',', 'a'), 1533),\n",
       " (('000', 'dlrs'), 1526),\n",
       " (('2', ','), 1525),\n",
       " (('pct', 'of'), 1498),\n",
       " (('with', 'the'), 1435),\n",
       " (('dlrs', ','), 1404),\n",
       " (('.', 'He'), 1385),\n",
       " ((',', 'but'), 1359),\n",
       " (('that', 'the'), 1346),\n",
       " (('5', 'mln'), 1338),\n",
       " (('from', 'the'), 1318),\n",
       " (('.', 'In'), 1315),\n",
       " (('dlrs', '.'), 1269),\n",
       " (('last', 'year'), 1243),\n",
       " (('at', 'the'), 1228),\n",
       " (('company', 'said'), 1217),\n",
       " (('4', '.'), 1215),\n",
       " (('0', '.'), 1213),\n",
       " (('in', 'a'), 1203),\n",
       " (('000', 'Revs'), 1198),\n",
       " (('The', 'company'), 1185),\n",
       " (('a', 'share'), 1167),\n",
       " (('dlrs', 'vs'), 1167),\n",
       " (('1', 'mln'), 1145),\n",
       " (('vs', '1'), 1145),\n",
       " (('2', 'mln'), 1139),\n",
       " (('of', 'its'), 1133),\n",
       " ((',', 'or'), 1119),\n",
       " (('3', 'mln'), 1117),\n",
       " (('pct', 'in'), 1115),\n",
       " (('of', 'a'), 1114),\n",
       " (('would', 'be'), 1112),\n",
       " ((',', 'said'), 1102),\n",
       " (('3', ','), 1092),\n",
       " (('6', 'mln'), 1086),\n",
       " ((',', 'it'), 1084),\n",
       " (('8', 'mln'), 1065),\n",
       " (('4', 'mln'), 1060),\n",
       " (('NET', 'Shr'), 1060),\n",
       " (('INC', '&'), 1059),\n",
       " (('9', 'mln'), 1054),\n",
       " (('NOTE', ':'), 1054),\n",
       " (('year', '.'), 1037),\n",
       " ((',', 'he'), 1031),\n",
       " (('Avg', 'shrs'), 1027),\n",
       " (('to', 'a'), 1026),\n",
       " (('this', 'year'), 1020),\n",
       " (('5', '.'), 1012),\n",
       " (('7', 'mln'), 1007),\n",
       " (('.', 'But'), 989),\n",
       " (('vs', 'profit'), 979),\n",
       " ((',\"', 'he'), 968),\n",
       " (('0', 'mln'), 966),\n",
       " (('per', 'share'), 952),\n",
       " (('000', 'tonnes'), 948),\n",
       " (('dlrs', 'a'), 930),\n",
       " (('in', '1986'), 926),\n",
       " (('.', 'A'), 908),\n",
       " (('It', 'said'), 901),\n",
       " (('year', ','), 900),\n",
       " (('the', 'dollar'), 878),\n",
       " (('in', 'January'), 873),\n",
       " (('He', 'said'), 871),\n",
       " (('4', ','), 865),\n",
       " (('/', '87'), 837),\n",
       " (('said', 'that'), 830),\n",
       " (('a', 'year'), 829),\n",
       " (('the', 'first'), 810),\n",
       " (('it', 'has'), 806),\n",
       " (('7', '.'), 800),\n",
       " (('expected', 'to'), 798),\n",
       " (('QTR', 'NET'), 797),\n",
       " (('6', '.'), 794),\n",
       " (('mln', 'stg'), 794),\n",
       " (('mln', 'tonnes'), 788),\n",
       " (('1986', '.'), 785),\n",
       " (('pct', '.'), 782),\n",
       " (('-', '1'), 776),\n",
       " (('Inc', 'said'), 776),\n",
       " (('in', 'February'), 771),\n",
       " (('said', 'its'), 743),\n",
       " (('4TH', 'QTR'), 738),\n",
       " (('\"', 'The'), 728),\n",
       " (('1986', ','), 726),\n",
       " (('for', 'a'), 724),\n",
       " (('CORP', '&'), 724),\n",
       " (('has', 'been'), 723),\n",
       " (('said', 'in'), 715),\n",
       " (('Bank', 'of'), 715),\n",
       " (('Net', 'loss'), 706),\n",
       " (('the', 'year'), 704),\n",
       " (('in', '1985'), 702),\n",
       " (('Shr', 'loss'), 696),\n",
       " (('5', ','), 694),\n",
       " (('10', '.'), 693),\n",
       " (('.', 'K'), 692),\n",
       " (('it', 'said'), 687),\n",
       " (('5', 'pct'), 676),\n",
       " (('1', '/'), 672),\n",
       " (('as', 'a'), 661),\n",
       " (('tonnes', 'of'), 658),\n",
       " (('compared', 'with'), 657),\n",
       " (('K', '.'), 656),\n",
       " ((',', 'with'), 655),\n",
       " (('pct', ','), 654),\n",
       " (('due', 'to'), 650),\n",
       " (('it', 'is'), 645),\n",
       " (('mln', 'Avg'), 638),\n",
       " (('>', '4TH'), 634),\n",
       " (('vs', '2'), 632),\n",
       " (('8', '.'), 627),\n",
       " (('said', 'he'), 625),\n",
       " (('United', 'States'), 621),\n",
       " (('the', 'government'), 620),\n",
       " (('said', ','), 614),\n",
       " (('it', 'will'), 603),\n",
       " (('\"', 'We'), 601),\n",
       " (('added', '.'), 601),\n",
       " (('.', '50'), 601),\n",
       " (('cts', 'a'), 597),\n",
       " (('the', 'United'), 595),\n",
       " (('sources', 'said'), 594),\n",
       " (('company', \"'\"), 591),\n",
       " (('.', 'O'), 591),\n",
       " (('mln', 'NOTE'), 588),\n",
       " (('also', 'said'), 586),\n",
       " (('Corp', 'said'), 585),\n",
       " ((',', 'in'), 584),\n",
       " ((',\"', 'said'), 584),\n",
       " (('dlrs', 'per'), 583),\n",
       " (('did', 'not'), 578),\n",
       " (('and', 'a'), 577),\n",
       " (('9', '.'), 576),\n",
       " (('.', 'U'), 573),\n",
       " (('dlrs', 'Net'), 571),\n",
       " (('at', 'a'), 564),\n",
       " (('6', ','), 564),\n",
       " (('billion', 'vs'), 556),\n",
       " (('more', 'than'), 552),\n",
       " (('end', 'of'), 550),\n",
       " (('sale', 'of'), 549),\n",
       " (('11', '.'), 549),\n",
       " (('share', ','), 548),\n",
       " (('12', '.'), 545),\n",
       " (('with', 'a'), 540),\n",
       " (('the', 'market'), 537),\n",
       " ((',', 'they'), 536),\n",
       " (('15', '.'), 533),\n",
       " (('the', 'same'), 533),\n",
       " (('pct', 'from'), 529),\n",
       " (('mths', 'Shr'), 523),\n",
       " (('on', 'a'), 521),\n",
       " (('to', 'buy'), 521),\n",
       " (('dlrs', 'from'), 521),\n",
       " (('3RD', 'QTR'), 519),\n",
       " (('the', 'end'), 518),\n",
       " (('1985', '.'), 513),\n",
       " (('1ST', 'QTR'), 513),\n",
       " (('agreed', 'to'), 511),\n",
       " (('have', 'been'), 511),\n",
       " (('O', '>'), 510),\n",
       " (('non', '-'), 509),\n",
       " (('Oper', 'shr'), 508),\n",
       " (('Oper', 'net'), 505),\n",
       " (('spokesman', 'said'), 503),\n",
       " (('1986', '/'), 503),\n",
       " (('7', ','), 500),\n",
       " (('over', 'the'), 497),\n",
       " (('of', '1'), 497),\n",
       " (('is', 'expected'), 492),\n",
       " (('it', 'was'), 492),\n",
       " (('16', '.'), 492),\n",
       " (('13', '.'), 489),\n",
       " (('had', 'been'), 487),\n",
       " (('>', '3RD'), 487),\n",
       " (('2', 'pct'), 487),\n",
       " (('share', '.'), 485),\n",
       " (('first', 'quarter'), 483),\n",
       " (('000', 'Sales'), 482),\n",
       " (('vs', '3'), 482),\n",
       " (('mln', 'Revs'), 482),\n",
       " (('8', ','), 479),\n",
       " (('4', 'pct'), 478),\n",
       " (('it', 'would'), 473),\n",
       " (('up', 'to'), 469),\n",
       " (('17', '.'), 468),\n",
       " (('common', 'stock'), 468),\n",
       " (('the', 'sale'), 467),\n",
       " ((',', 'to'), 458),\n",
       " (('that', 'it'), 450),\n",
       " (('loss', '1'), 450),\n",
       " (('/', '2'), 449),\n",
       " (('Net', 'profit'), 445),\n",
       " (('rise', 'in'), 444),\n",
       " (('New', 'York'), 444),\n",
       " (('because', 'of'), 442),\n",
       " (('pct', 'to'), 440),\n",
       " (('last', 'month'), 440),\n",
       " (('.', 'They'), 436),\n",
       " (('.', 'This'), 436),\n",
       " ((',', '\"'), 436),\n",
       " (('billion', 'in'), 434),\n",
       " (('told', 'Reuters'), 433),\n",
       " (('in', 'its'), 433),\n",
       " (('>', 'said'), 432),\n",
       " (('Year', 'Shr'), 428),\n",
       " (('is', 'a'), 427),\n",
       " (('cts', 'prior'), 427),\n",
       " (('in', '1987'), 426),\n",
       " (('14', '.'), 424),\n",
       " (('from', 'a'), 421),\n",
       " (('Nine', 'mths'), 421),\n",
       " (('to', 'acquire'), 420),\n",
       " (('-', 'year'), 419),\n",
       " (('central', 'bank'), 417),\n",
       " (('the', 'new'), 416),\n",
       " (('>', '1ST'), 416),\n",
       " (('Qtly', 'div'), 415),\n",
       " (('to', 'sell'), 414),\n",
       " ((',', 'while'), 414),\n",
       " (('8', 'pct'), 412),\n",
       " (('cts', 'per'), 409),\n",
       " (('the', 'current'), 408),\n",
       " (('stake', 'in'), 406),\n",
       " (('However', ','), 405),\n",
       " (('/', '4'), 405),\n",
       " (('part', 'of'), 404),\n",
       " (('loss', 'of'), 401),\n",
       " (('shares', 'of'), 401),\n",
       " ((\"'\", 't'), 399),\n",
       " ((',', 'up'), 394),\n",
       " (('tonnes', ','), 393),\n",
       " (('dlrs', 'and'), 392),\n",
       " (('\"', 'I'), 392),\n",
       " (('said', 'they'), 391),\n",
       " (('last', 'week'), 390),\n",
       " (('a', 'new'), 389),\n",
       " (('Shr', 'profit'), 389),\n",
       " (('they', 'said'), 388),\n",
       " (('9', ','), 387),\n",
       " (('the', 'previous'), 387),\n",
       " (('likely', 'to'), 383),\n",
       " (('18', '.'), 383),\n",
       " (('cts', 'Oper'), 383),\n",
       " (('dlrs', 'or'), 383),\n",
       " ((\"'\", 'S'), 378),\n",
       " (('mln', 'dlr'), 377),\n",
       " (('.', 'However'), 376),\n",
       " ((',', 'including'), 375),\n",
       " (('sales', 'of'), 375),\n",
       " (('>', 'TO'), 375),\n",
       " (('-', 'term'), 373),\n",
       " (('.', '25'), 373),\n",
       " (('of', 'about'), 372),\n",
       " (('March', '31'), 371),\n",
       " (('shares', ','), 371),\n",
       " (('Inc', ','), 370),\n",
       " (('interest', 'rates'), 368),\n",
       " (('by', 'a'), 366),\n",
       " (('under', 'the'), 366),\n",
       " (('and', 'other'), 365),\n",
       " (('rose', 'to'), 364),\n",
       " ((',', '500'), 364),\n",
       " (('mln', 'in'), 363),\n",
       " (('a', 'statement'), 362),\n",
       " ((',', 'for'), 362),\n",
       " (('as', 'the'), 360),\n",
       " (('officials', 'said'), 358),\n",
       " (('West', 'Germany'), 358),\n",
       " (('would', 'not'), 357),\n",
       " (('analysts', 'said'), 356),\n",
       " (('plans', 'to'), 351),\n",
       " (('in', 'December'), 351),\n",
       " (('1', 'pct'), 350),\n",
       " (('during', 'the'), 349),\n",
       " (('Corp', '&'), 349),\n",
       " (('year', '-'), 349),\n",
       " (('he', 'added'), 348),\n",
       " (('if', 'the'), 347),\n",
       " (('after', 'a'), 347),\n",
       " (('vs', '4'), 347),\n",
       " (('19', '.'), 347),\n",
       " (('dlrs', 'for'), 346),\n",
       " (('is', 'not'), 346),\n",
       " (('.', '00'), 345),\n",
       " (('-', '3'), 344),\n",
       " (('subject', 'to'), 343),\n",
       " ((',', 'as'), 342),\n",
       " (('to', '1'), 342),\n",
       " (('3', 'pct'), 342),\n",
       " (('increase', 'in'), 341),\n",
       " (('the', 'next'), 341),\n",
       " (('the', 'two'), 340),\n",
       " (('which', 'is'), 339),\n",
       " (('1987', '.'), 337),\n",
       " (('>', 'YEAR'), 336),\n",
       " (('against', 'the'), 334),\n",
       " (('year', 'earlier'), 334),\n",
       " (('.', '75'), 333),\n",
       " (('>', 'SETS'), 332),\n",
       " ((',', 'who'), 331),\n",
       " (('Corp', ','), 330),\n",
       " ((',', 'is'), 327),\n",
       " (('9', 'pct'), 327),\n",
       " (('to', 'take'), 326),\n",
       " (('year', \"'\"), 325),\n",
       " (('prior', 'Pay'), 323),\n",
       " (('common', 'shares'), 323),\n",
       " (('6', 'pct'), 322),\n",
       " (('official', 'said'), 321),\n",
       " (('the', 'EC'), 321),\n",
       " (('7', 'pct'), 320),\n",
       " (('is', 'the'), 319),\n",
       " (('1987', ','), 319),\n",
       " ((',', 'an'), 319),\n",
       " (('20', '.'), 318),\n",
       " (('quarter', 'and'), 318),\n",
       " (('31', ','), 317),\n",
       " (('after', 'the'), 317),\n",
       " (('foreign', 'exchange'), 316),\n",
       " (('mln', '.'), 314),\n",
       " (('it', 'had'), 313),\n",
       " (('could', 'be'), 313),\n",
       " (('oil', 'and'), 313),\n",
       " (('-', 'for'), 312),\n",
       " (('the', 'world'), 311),\n",
       " (('for', '-'), 309),\n",
       " (('company', '.'), 308),\n",
       " (('3', '/'), 306),\n",
       " (('LOSS', 'Shr'), 306),\n",
       " ((',', 'compared'), 305),\n",
       " (('The', 'U'), 303),\n",
       " (('the', 'country'), 303),\n",
       " (('have', 'to'), 302),\n",
       " (('does', 'not'), 302),\n",
       " (('30', ','), 301),\n",
       " (('it', 'expects'), 301),\n",
       " (('according', 'to'), 300),\n",
       " (('rate', 'of'), 300),\n",
       " ((',', '1986'), 300),\n",
       " (('said', 'a'), 299),\n",
       " (('would', 'have'), 299),\n",
       " ((',', 'will'), 297),\n",
       " (('/', '8'), 297),\n",
       " (('Note', ':'), 297),\n",
       " (('crude', 'oil'), 296),\n",
       " (('Japan', \"'\"), 295),\n",
       " (('stock', '.'), 295),\n",
       " (('tender', 'offer'), 292),\n",
       " (('and', 'Co'), 291),\n",
       " (('31', '.'), 291),\n",
       " (('mln', 'Year'), 291),\n",
       " (('dlrs', 'of'), 290),\n",
       " (('Inc', '&'), 290),\n",
       " (('January', ','), 290),\n",
       " (('vs', '5'), 289),\n",
       " (('acquisition', 'of'), 289),\n",
       " (('oil', 'prices'), 288),\n",
       " (('1985', ','), 288),\n",
       " (('the', 'second'), 288),\n",
       " (('West', 'German'), 288),\n",
       " (('the', 'Bank'), 288),\n",
       " (('of', 'U'), 287),\n",
       " (('gain', 'of'), 287),\n",
       " (('dlrs', 'to'), 286),\n",
       " (('were', 'not'), 286),\n",
       " (('its', 'board'), 286),\n",
       " (('continue', 'to'), 285),\n",
       " (('Securities', 'and'), 284),\n",
       " (('fourth', 'quarter'), 284),\n",
       " (('value', 'of'), 283),\n",
       " (('mln', 'shares'), 283),\n",
       " (('number', 'of'), 283),\n",
       " (('/', '86'), 283),\n",
       " (('5', 'billion'), 282),\n",
       " (('and', 'Exchange'), 282),\n",
       " (('a', 'barrel'), 282),\n",
       " (('In', 'a'), 282),\n",
       " ((',', 'was'), 281),\n",
       " (('1985', '/'), 280),\n",
       " ((',', '1987'), 280),\n",
       " (('/', '09'), 280),\n",
       " (('-', 'based'), 279),\n",
       " (('\"', 'It'), 278),\n",
       " (('such', 'as'), 278),\n",
       " (('mln', 'Nine'), 278),\n",
       " (('09', '/'), 278),\n",
       " (('year', 'ago'), 277),\n",
       " (('price', 'of'), 277),\n",
       " (('Exchange', 'Commission'), 277),\n",
       " (('buffer', 'stock'), 276),\n",
       " (('shares', '.'), 275),\n",
       " (('the', 'past'), 275),\n",
       " (('interest', 'in'), 275),\n",
       " (('this', 'week'), 274),\n",
       " (('told', 'the'), 274),\n",
       " (('two', '-'), 272),\n",
       " (('Shr', '1'), 272),\n",
       " (('not', 'be'), 271),\n",
       " (('about', 'the'), 271),\n",
       " (('was', 'not'), 270),\n",
       " (('.', 'Agriculture'), 270),\n",
       " (('The', 'Bank'), 270),\n",
       " (('company', ','), 269),\n",
       " (('Net', '1'), 269),\n",
       " (('M', '-'), 269),\n",
       " (('at', 'least'), 268),\n",
       " (('the', 'bank'), 267),\n",
       " (('10', 'pct'), 267),\n",
       " (('in', 'an'), 266),\n",
       " (('on', 'its'), 266),\n",
       " (('the', 'acquisition'), 266),\n",
       " (('based', 'on'), 266),\n",
       " (('N', '.'), 266),\n",
       " (('the', 'offer'), 266),\n",
       " (('50', 'pct'), 265),\n",
       " (('fell', 'to'), 265),\n",
       " (('to', 'make'), 264),\n",
       " (('net', 'profit'), 264),\n",
       " (('50', 'dlrs'), 264),\n",
       " (('net', 'includes'), 264),\n",
       " (('the', 'Securities'), 262),\n",
       " (('billion', 'marks'), 262),\n",
       " (('dealers', 'said'), 262),\n",
       " (('up', 'from'), 262),\n",
       " (('and', 'that'), 262),\n",
       " (('one', 'of'), 260),\n",
       " (('will', 'not'), 260),\n",
       " (('the', 'total'), 260),\n",
       " (('.', 'L'), 258),\n",
       " (('of', 'Japan'), 257),\n",
       " (('declined', 'to'), 257),\n",
       " (('two', 'cts'), 257),\n",
       " (('of', '1986'), 256),\n",
       " (('for', 'an'), 255),\n",
       " (('should', 'be'), 255),\n",
       " (('QTR', 'LOSS'), 254),\n",
       " ((',', 'according'), 253),\n",
       " (('money', 'market'), 252),\n",
       " (('25', '.'), 252),\n",
       " (('the', 'agreement'), 252),\n",
       " ((',', 'has'), 251),\n",
       " (('European', 'Community'), 251),\n",
       " (('In', 'the'), 251),\n",
       " ((',', 'from'), 250),\n",
       " (('the', 'merger'), 249),\n",
       " ((':', '1986'), 249),\n",
       " (('to', '2'), 248),\n",
       " (('22', '.'), 248),\n",
       " (('on', 'March'), 247),\n",
       " (('month', '.'), 247),\n",
       " (('Agriculture', 'Department'), 247),\n",
       " (('billion', '.'), 246),\n",
       " (('today', '.'), 246),\n",
       " (('to', 'have'), 246),\n",
       " (('discontinued', 'operations'), 246),\n",
       " (('21', '.'), 244),\n",
       " (('.', '&'), 244),\n",
       " (('mln', 'barrels'), 244),\n",
       " (('20', 'pct'), 243),\n",
       " (('of', '2'), 242),\n",
       " (('loss', '2'), 242),\n",
       " (('prices', ','), 242),\n",
       " (('share', 'in'), 241),\n",
       " (('the', 'Fed'), 241),\n",
       " (('told', 'a'), 241),\n",
       " (('be', 'a'), 241),\n",
       " ((',\"', 'the'), 240),\n",
       " (('10', 'cts'), 239),\n",
       " (('24', '.'), 238),\n",
       " (('between', 'the'), 237),\n",
       " (('stock', 'split'), 237),\n",
       " (('was', 'a'), 236),\n",
       " (('one', '-'), 236),\n",
       " (('tonnes', 'in'), 235),\n",
       " (('27', '.'), 235),\n",
       " (('a', 'major'), 235),\n",
       " (('into', 'the'), 234),\n",
       " (('the', 'economy'), 233),\n",
       " (('net', 'loss'), 233),\n",
       " ((',', 'down'), 233),\n",
       " (('2ND', 'QTR'), 233),\n",
       " (('Federal', 'Reserve'), 232),\n",
       " (('five', 'cts'), 232),\n",
       " (('when', 'the'), 232),\n",
       " (('>', 'and'), 231),\n",
       " (('week', ','), 231),\n",
       " (('30', '.'), 230),\n",
       " (('Inc', '.'), 230),\n",
       " (('.', 'Under'), 230),\n",
       " (('five', 'pct'), 229),\n",
       " (('expects', 'to'), 228),\n",
       " (('before', 'the'), 228),\n",
       " (('in', 'cash'), 228),\n",
       " (('qtr', 'and'), 228),\n",
       " (('the', 'last'), 227),\n",
       " (('-', 'tax'), 227),\n",
       " (('the', 'Gulf'), 226),\n",
       " (('1987', '/'), 226),\n",
       " (('dlrs', 'on'), 226),\n",
       " (('JAN', '31'), 226),\n",
       " (('the', 'week'), 225),\n",
       " (('in', 'March'), 225),\n",
       " (('/', '88'), 225),\n",
       " (('of', 'record'), 225),\n",
       " (('of', 'this'), 224),\n",
       " (('six', 'cts'), 224),\n",
       " (('vs', '6'), 223),\n",
       " (('of', '3'), 222),\n",
       " (('Ltd', '>'), 222),\n",
       " (('three', '-'), 222),\n",
       " (('and', 'its'), 222),\n",
       " (('four', 'cts'), 222),\n",
       " (('They', 'said'), 221),\n",
       " (('traders', 'said'), 221),\n",
       " (('the', 'group'), 221),\n",
       " (('Pay', 'April'), 221),\n",
       " (('Ltd', '&'), 220),\n",
       " (('QTR', 'JAN'), 220),\n",
       " (('Soviet', 'Union'), 220),\n",
       " (('23', '.'), 219),\n",
       " (('to', 'its'), 219),\n",
       " (('year', 'and'), 219),\n",
       " (('owned', 'by'), 218),\n",
       " (('-', 'owned'), 218),\n",
       " (('trade', 'deficit'), 217),\n",
       " (('2', 'billion'), 217),\n",
       " (('but', 'the'), 216),\n",
       " (('pct', 'stake'), 216),\n",
       " (('today', ','), 216),\n",
       " (('mln', 'bpd'), 216),\n",
       " (('.', 'For'), 216),\n",
       " (('there', 'is'), 216),\n",
       " (('may', 'be'), 216),\n",
       " (('the', 'price'), 215),\n",
       " (('0', 'pct'), 215),\n",
       " (('three', 'cts'), 215),\n",
       " (('the', 'Soviet'), 215),\n",
       " (('pct', 'and'), 214),\n",
       " (('the', 'fourth'), 214),\n",
       " (('January', '.'), 214),\n",
       " (('down', 'from'), 212),\n",
       " (('stock', ','), 212),\n",
       " (('vs', '7'), 211),\n",
       " (('an', 'agreement'), 211),\n",
       " (('and', 'to'), 210),\n",
       " (('CO', '&'), 210),\n",
       " ((',', 'however'), 209),\n",
       " (('a', 'result'), 208),\n",
       " (('a', 'total'), 208),\n",
       " (('than', 'the'), 208),\n",
       " (('to', 'increase'), 208),\n",
       " (('MLN', 'DLRS'), 208),\n",
       " (('market', '.'), 208),\n",
       " (('which', 'has'), 208),\n",
       " (('told', 'reporters'), 208),\n",
       " (('from', '1'), 207),\n",
       " (('billion', 'dlr'), 207),\n",
       " (('to', 'an'), 207),\n",
       " (('offer', 'for'), 207),\n",
       " (('quarter', '.'), 207),\n",
       " (('the', 'yen'), 206),\n",
       " (('since', 'the'), 206),\n",
       " (('February', ','), 206),\n",
       " (('the', 'Federal'), 205),\n",
       " (('added', 'that'), 205),\n",
       " (('Record', 'April'), 205),\n",
       " (('31', 'NET'), 205),\n",
       " (('and', 'will'), 205),\n",
       " (('to', 'reduce'), 204),\n",
       " (('growth', 'in'), 204),\n",
       " (('the', 'European'), 204),\n",
       " (('trade', 'surplus'), 203),\n",
       " (('.', 'Last'), 203),\n",
       " (('half', 'of'), 203),\n",
       " (('less', 'than'), 203),\n",
       " (('exchange', 'rate'), 203),\n",
       " (('vs', '8'), 203),\n",
       " (('>', '2ND'), 203),\n",
       " (('long', '-'), 202),\n",
       " (('quarter', 'of'), 202),\n",
       " (('of', 'an'), 202),\n",
       " (('amount', 'of'), 201),\n",
       " (('100', ','), 201),\n",
       " (('.', 'On'), 201),\n",
       " (('28', '.'), 201),\n",
       " (('as', 'well'), 200),\n",
       " (('.', 'And'), 199),\n",
       " (('Department', 'said'), 199),\n",
       " (('however', ','), 199),\n",
       " (('.', 'As'), 199),\n",
       " (('1986', 'net'), 199),\n",
       " (('10', ','), 199),\n",
       " (('on', 'April'), 198),\n",
       " (('market', ','), 198),\n",
       " (('the', '1986'), 198),\n",
       " (('Co', 'said'), 198),\n",
       " (('vs', '10'), 198),\n",
       " (('the', 'trade'), 197),\n",
       " (('the', 'central'), 196),\n",
       " (('of', 'England'), 196),\n",
       " (('to', 'help'), 196),\n",
       " (('is', 'to'), 195),\n",
       " (('of', '1987'), 195),\n",
       " (('L', '>'), 195),\n",
       " (('department', 'said'), 194),\n",
       " (('.', 'Analysts'), 194),\n",
       " (('government', \"'\"), 194),\n",
       " (('have', 'a'), 194),\n",
       " (('7', '-'), 193),\n",
       " (('billion', 'stg'), 193),\n",
       " (('.', '10'), 193),\n",
       " ((',', '600'), 193),\n",
       " (('the', 'stock'), 193),\n",
       " (('25', 'pct'), 192),\n",
       " (('.\"', 'The'), 192),\n",
       " (('subsidiary', 'of'), 191),\n",
       " (('and', '1'), 191),\n",
       " (('profit', '1'), 191),\n",
       " (('operations', '.'), 191),\n",
       " (('1', 'billion'), 190),\n",
       " (('the', 'Japanese'), 190),\n",
       " (('country', \"'\"), 190),\n",
       " (('share', 'of'), 190),\n",
       " (('vs', '9'), 190),\n",
       " (('.', '20'), 190),\n",
       " (('a', 'record'), 190),\n",
       " (('total', 'of'), 190),\n",
       " (('and', 'gas'), 190),\n",
       " (('Record', 'March'), 190),\n",
       " (('out', 'of'), 189),\n",
       " (('week', '.'), 189),\n",
       " (('years', '.'), 189),\n",
       " (('year', 'to'), 189),\n",
       " (('will', 'have'), 188),\n",
       " (('Plc', '&'), 188),\n",
       " (('vs', '11'), 188),\n",
       " (('.', 'At'), 188),\n",
       " (('to', 'cut'), 187),\n",
       " (('.', '30'), 187),\n",
       " (('mln', ','), 186),\n",
       " (('decline', 'in'), 186),\n",
       " (('prices', '.'), 186),\n",
       " (('end', '-'), 186),\n",
       " (('pre', '-'), 186),\n",
       " (('fall', 'in'), 185),\n",
       " (('result', 'of'), 185),\n",
       " (('terms', 'of'), 185),\n",
       " (('exchange', 'rates'), 185),\n",
       " (('-', '7'), 185),\n",
       " (('this', 'month'), 184),\n",
       " (('26', '.'), 184),\n",
       " (('.\"', 'He'), 184),\n",
       " (('assets', 'of'), 184),\n",
       " ((',', 'effective'), 184),\n",
       " (('eight', 'cts'), 184),\n",
       " (('-', 'day'), 183),\n",
       " (('share', 'for'), 183),\n",
       " (('compared', 'to'), 183),\n",
       " (('not', 'to'), 182),\n",
       " (('fiscal', 'year'), 182),\n",
       " ((',', 'told'), 182),\n",
       " (('are', 'not'), 182),\n",
       " (('well', 'as'), 182),\n",
       " (('TO', 'SELL'), 182),\n",
       " (('was', 'the'), 182),\n",
       " (('month', ','), 181),\n",
       " (('ago', '.'), 181),\n",
       " ((',', 'adding'), 181),\n",
       " (('the', 'proposed'), 181),\n",
       " (('000', 'NOTE'), 181),\n",
       " ((',', 'dealers'), 180),\n",
       " ((',', 'analysts'), 180),\n",
       " (('.', '35'), 180),\n",
       " (('the', 'third'), 180),\n",
       " (('pct', 'rise'), 179),\n",
       " (('Reuters', '.'), 179),\n",
       " (('a', 'further'), 179),\n",
       " (('TO', 'BUY'), 179),\n",
       " (('1986', 'and'), 178),\n",
       " (('drop', 'in'), 178),\n",
       " (('a', 'loss'), 178),\n",
       " (('all', 'of'), 178),\n",
       " (('January', 'and'), 178),\n",
       " (('tonnes', '.'), 177),\n",
       " (('But', 'the'), 177),\n",
       " (('the', 'official'), 177),\n",
       " ((',', 'after'), 177),\n",
       " (('will', 'continue'), 177),\n",
       " (('years', ','), 176),\n",
       " (('vs', '12'), 176),\n",
       " (('current', 'account'), 176),\n",
       " (('natural', 'gas'), 175),\n",
       " (('cost', 'of'), 175),\n",
       " (('don', \"'\"), 175),\n",
       " (('500', ','), 175),\n",
       " (('meeting', 'of'), 175),\n",
       " ((',', '700'), 175),\n",
       " (('earlier', '.'), 175),\n",
       " (('agreement', ','), 174),\n",
       " (('months', 'of'), 174),\n",
       " (('said', 'there'), 174),\n",
       " (('for', 'its'), 174),\n",
       " (('operations', 'of'), 174),\n",
       " (('.', 'F'), 174),\n",
       " ((',', 'respectively'), 174),\n",
       " (('statement', '.'), 173),\n",
       " (('agreement', 'to'), 173),\n",
       " (('>', 'SEES'), 173),\n",
       " ((',', '300'), 173),\n",
       " (('Industries', 'Inc'), 173),\n",
       " (('.', '15'), 173),\n",
       " (('net', 'income'), 173),\n",
       " (('Net', '2'), 173),\n",
       " ((',', '200'), 173),\n",
       " (('seven', 'cts'), 173),\n",
       " (('mln', 'Note'), 173),\n",
       " (('Co', ','), 173),\n",
       " (('.', '80'), 172),\n",
       " (('Saudi', 'Arabia'), 172),\n",
       " (('the', 'International'), 172),\n",
       " (('.', '60'), 172),\n",
       " (('15', 'cts'), 172),\n",
       " (('do', 'not'), 171),\n",
       " (('3', 'billion'), 171),\n",
       " (('8', 'billion'), 171),\n",
       " (('20', 'cts'), 171),\n",
       " (('from', 'discontinued'), 171),\n",
       " (('level', 'of'), 170),\n",
       " (('and', 'is'), 170),\n",
       " (('in', 'quarter'), 170),\n",
       " (('short', '-'), 169),\n",
       " (('6', 'billion'), 169),\n",
       " (('the', 'state'), 169),\n",
       " (('has', 'not'), 169),\n",
       " (('quarter', ','), 169),\n",
       " (('comment', 'on'), 169),\n",
       " (('money', 'supply'), 169),\n",
       " (('for', 'all'), 169),\n",
       " (('one', 'ct'), 169),\n",
       " (('NET', 'Oper'), 169),\n",
       " (('nine', 'mths'), 169),\n",
       " (('Japan', ','), 168),\n",
       " (('In', 'addition'), 168),\n",
       " (('000', 'shares'), 168),\n",
       " (('to', '3'), 168),\n",
       " (('Under', 'the'), 168),\n",
       " ((',', 'of'), 167),\n",
       " ((',', 'traders'), 167),\n",
       " (('the', 'Bundesbank'), 167),\n",
       " (('to', 'shareholders'), 167),\n",
       " (('.', 'If'), 167),\n",
       " (('nine', 'cts'), 167),\n",
       " (('sell', 'its'), 167),\n",
       " (('the', 'department'), 167),\n",
       " (('in', 'April'), 166),\n",
       " (('April', '30'), 166),\n",
       " (('that', 'a'), 166),\n",
       " (('7', 'billion'), 165),\n",
       " (('.', 'Some'), 165),\n",
       " (('which', 'was'), 165),\n",
       " (('Inc', '>'), 165),\n",
       " (('Revs', '1'), 165),\n",
       " (('response', 'to'), 164),\n",
       " (('of', 'oil'), 164),\n",
       " (('there', 'was'), 164),\n",
       " (('QTR', 'FEB'), 164),\n",
       " ((',', '800'), 164),\n",
       " (('to', 'meet'), 163),\n",
       " (('to', 'rise'), 163),\n",
       " (('29', '.'), 163),\n",
       " (('a', 'letter'), 163),\n",
       " (('third', 'quarter'), 163),\n",
       " (('is', 'likely'), 162),\n",
       " (('\"', 'There'), 162),\n",
       " ((',', '100'), 162),\n",
       " (('control', 'of'), 162),\n",
       " (('one', 'mln'), 162),\n",
       " (('nil', 'nil'), 162),\n",
       " (('an', 'average'), 161),\n",
       " (('A', '.'), 161),\n",
       " (('April', '15'), 161),\n",
       " (('not', 'disclosed'), 161),\n",
       " (('billion', 'francs'), 160),\n",
       " (('the', 'board'), 160),\n",
       " (('.', '05'), 160),\n",
       " (('February', '1986'), 160),\n",
       " ((',', 'vs'), 160),\n",
       " (('9', 'billion'), 159),\n",
       " (('Ltd', ','), 159),\n",
       " (('.', 'There'), 159),\n",
       " (('the', 'transaction'), 159),\n",
       " (('FEB', '28'), 159),\n",
       " (('seasonally', 'adjusted'), 159),\n",
       " (('noted', 'that'), 158),\n",
       " (('for', 'about'), 158),\n",
       " (('revenues', 'of'), 158),\n",
       " ((',', '400'), 158),\n",
       " (('able', 'to'), 157),\n",
       " (('South', 'Korea'), 157),\n",
       " (('.', 'Dlrs'), 157),\n",
       " (('are', 'expected'), 157),\n",
       " (('stg', 'vs'), 157),\n",
       " (('the', 'money'), 157),\n",
       " (('plan', 'to'), 157),\n",
       " (('is', 'subject'), 157),\n",
       " (('12', 'cts'), 157),\n",
       " ((',', 'would'), 156),\n",
       " (('G', '-'), 156),\n",
       " (('completed', 'the'), 156),\n",
       " (('interest', 'rate'), 156),\n",
       " (('S', '.,'), 155),\n",
       " (('It', 'is'), 155),\n",
       " (('stg', 'in'), 155),\n",
       " (('two', 'pct'), 155),\n",
       " (('31', 'Shr'), 155),\n",
       " (('the', 'Paris'), 155),\n",
       " (('.', '1986'), 155),\n",
       " (('per', 'tonne'), 155),\n",
       " (('.', 'Trade'), 154),\n",
       " (('to', 'pay'), 154),\n",
       " (('4', 'billion'), 154),\n",
       " (('34', '.'), 154),\n",
       " (('at', 'its'), 154),\n",
       " (('a', 'two'), 153),\n",
       " (('that', 'would'), 153),\n",
       " (('from', 'its'), 153),\n",
       " (('said', '\"'), 153),\n",
       " (('vs', '13'), 153),\n",
       " (('disclosed', '.'), 153),\n",
       " (('E', '.'), 153),\n",
       " (('The', 'government'), 152),\n",
       " (('s', '&'), 152),\n",
       " (('previously', 'announced'), 152),\n",
       " (('president', 'of'), 152),\n",
       " (('holders', 'of'), 152),\n",
       " (('16', 'pct'), 151),\n",
       " ((',', 'when'), 151),\n",
       " (('it', 'agreed'), 151),\n",
       " (('shr', 'loss'), 151),\n",
       " (('stg', '.'), 150),\n",
       " (('has', 'a'), 150),\n",
       " (('/', '16'), 150),\n",
       " (('he', 'was'), 150),\n",
       " (('15', 'pct'), 150),\n",
       " (('000', 'Avg'), 150),\n",
       " (('200', ','), 150),\n",
       " (('YEAR', 'NET'), 150),\n",
       " (('while', 'the'), 149),\n",
       " (('30', 'pct'), 149),\n",
       " (('rates', '.'), 149),\n",
       " (('.', 'Earlier'), 149),\n",
       " (('year', 'net'), 149),\n",
       " (('s', 'stock'), 149),\n",
       " (('at', '1'), 149),\n",
       " (('mid', '-'), 148),\n",
       " (('a', 'meeting'), 148),\n",
       " (('I', 'think'), 148),\n",
       " (('PCT', 'IN'), 148),\n",
       " (('the', 'National'), 148),\n",
       " (('000', 'dlr'), 148),\n",
       " (('Corp', '.'), 148),\n",
       " (('Group', 'Inc'), 148),\n",
       " (('has', 'agreed'), 148),\n",
       " (('000', 'bpd'), 148),\n",
       " ((',', 'against'), 147),\n",
       " (('of', '&'), 147),\n",
       " (('an', 'increase'), 147),\n",
       " (('to', '7'), 147),\n",
       " (('-', 'one'), 147),\n",
       " (('with', 'its'), 147),\n",
       " (('to', 'continue'), 147),\n",
       " (('dollar', ','), 147),\n",
       " (('to', 'raise'), 147),\n",
       " (('exports', 'to'), 146),\n",
       " (('change', 'in'), 146),\n",
       " (('BANK', 'OF'), 146),\n",
       " (('at', 'about'), 146),\n",
       " (('000', 'Year'), 146),\n",
       " (('000', ','), 146),\n",
       " (('1', '-'), 146),\n",
       " (('which', 'would'), 146),\n",
       " (('-', 'ago'), 146),\n",
       " (('is', 'also'), 145),\n",
       " (('need', 'to'), 145),\n",
       " (('in', 'which'), 145),\n",
       " (('prices', 'and'), 145),\n",
       " (('vs', '15'), 145),\n",
       " (('37', '.'), 145),\n",
       " (('the', 'report'), 145),\n",
       " (('to', 'keep'), 144),\n",
       " (('per', 'day'), 144),\n",
       " (('300', ','), 144),\n",
       " (('F', '.'), 144),\n",
       " (('in', '1988'), 144),\n",
       " (('10', 'mln'), 144),\n",
       " (('Hong', 'Kong'), 143),\n",
       " (('in', 'New'), 143),\n",
       " (('because', 'the'), 143),\n",
       " (('they', 'are'), 143),\n",
       " (('.', 'N'), 143),\n",
       " (('three', 'pct'), 143),\n",
       " (('6', '-'), 143),\n",
       " (('17', 'cts'), 143),\n",
       " (('25', 'cts'), 143),\n",
       " (('had', 'a'), 142),\n",
       " (('It', 'also'), 142),\n",
       " ((',', 'although'), 142),\n",
       " (('MONEY', 'MARKET'), 142),\n",
       " (('C', '.'), 142),\n",
       " (('week', \"'\"), 142),\n",
       " (('group', 'of'), 142),\n",
       " (('offer', '.'), 142),\n",
       " (('its', 'stake'), 142),\n",
       " (('Commission', ','), 142),\n",
       " (('April', '1'), 142),\n",
       " (('ended', 'March'), 141),\n",
       " (('today', \"'\"), 141),\n",
       " (('80', 'pct'), 141),\n",
       " (('a', '1'), 141),\n",
       " (('chief', 'executive'), 141),\n",
       " (('the', 'purchase'), 141),\n",
       " (('-', 'month'), 141),\n",
       " (('going', 'to'), 141),\n",
       " (('to', '10'), 141),\n",
       " (('03', '/'), 141),\n",
       " (('32', '.'), 140),\n",
       " (('when', 'it'), 140),\n",
       " (('talks', 'with'), 140),\n",
       " (('.', '40'), 140),\n",
       " (('14', 'cts'), 140),\n",
       " (('net', 'excludes'), 140),\n",
       " (('Finance', 'Minister'), 140),\n",
       " (('led', 'by'), 140),\n",
       " (('offer', 'to'), 140),\n",
       " (('preferred', 'stock'), 140),\n",
       " (('04', '/'), 140),\n",
       " (('87', '03'), 140),\n",
       " (('chairman', 'of'), 139),\n",
       " (('is', 'still'), 139),\n",
       " (('so', 'far'), 139),\n",
       " (('mln', 'marks'), 139),\n",
       " (('signed', 'a'), 139),\n",
       " (('pct', 'interest'), 139),\n",
       " (('on', '-'), 139),\n",
       " (('spokesman', 'for'), 138),\n",
       " (('\"', 'This'), 138),\n",
       " (('pct', 'increase'), 138),\n",
       " (('it', 'to'), 138),\n",
       " (('rates', ','), 138),\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdbigram.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words that commonly co-occur. \n",
    "\n",
    ">Finding collocations requires **first calculating the frequencies of words and\n",
    "their appearance in the context of other words**. Often the collection of words\n",
    "will then requiring filtering to **only retain useful content terms**. Each ngram\n",
    "of words may then be **scored** according to some association measure, in order\n",
    "to **determine the relative likelihood of each ngram being a collocation**. (Quoted from [here](http://www.nltk.org/_modules/nltk/collocations.html))\n",
    "\n",
    "For example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by **ranking the bigrams by some measures**. A commonly used measure is [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). The following code will find **the best 100 bigrams** using the PMI scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nltk.collocations.BigramAssocMeasures()`: Bigram associated measurement with numbers of method\n",
    "2. `nltk.collocations.BigramCollocationFinder.from_words(words)`: Get word from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...............', 'MAX'),\n",
       " ('11895', '2289'),\n",
       " ('11TH', 'LICENCE'),\n",
       " ('12664', '11895'),\n",
       " ('2867', '2073'),\n",
       " ('3198', '2867'),\n",
       " ('35000', '32876'),\n",
       " ('6000', 'QMS'),\n",
       " ('6834', '2292'),\n",
       " ('7289', '6834'),\n",
       " ('8440', '1409'),\n",
       " ('9009', '8440'),\n",
       " (';(', 'MWW'),\n",
       " ('ACUIRES', 'SUPERMAC'),\n",
       " ('ADELAIDE', 'STEAMSHIP'),\n",
       " ('ADVENTURE', 'Americanture'),\n",
       " ('ARMISTICE', 'ELUDES'),\n",
       " ('ARTICLES', 'HALLMARKED'),\n",
       " ('ARTILLERY', 'SHELLS'),\n",
       " ('ASPEN', 'RIBBONS'),\n",
       " ('ASSUMPTIONS', 'FLAWED'),\n",
       " ('AUTOCLAVE', 'ENGINEERS'),\n",
       " ('AVIAN', 'INFLUENZA'),\n",
       " ('Acquired', 'Immune'),\n",
       " ('Addis', 'Ababa'),\n",
       " ('Addressograph', 'Farrington'),\n",
       " ('Adventist', 'Church'),\n",
       " ('Afobaka', 'dam'),\n",
       " ('Aggregate', 'judgments'),\n",
       " ('Aghia', 'Efthymia'),\n",
       " ('Ahmad', 'Sarji'),\n",
       " ('Alfieri', 'Maserati'),\n",
       " ('Allgemeine', 'Hypothekenbank'),\n",
       " ('Almy', 'Hafild'),\n",
       " ('Alois', 'Schwietert'),\n",
       " ('Alsthom', 'Inudstrial'),\n",
       " ('Anders', 'Carlberg'),\n",
       " ('Anno', '1720'),\n",
       " ('Anwar', 'Sadat'),\n",
       " ('Aproveitamentos', 'Florestais'),\n",
       " ('Arie', 'Guldemond'),\n",
       " ('Artistic', 'Greetings'),\n",
       " ('Attilio', 'Petrocelli'),\n",
       " ('Augusto', 'Pinochet'),\n",
       " ('Ausimont', 'Compo'),\n",
       " ('BALANCED', 'PHYSICALS'),\n",
       " ('BANCO', 'SANTANDER'),\n",
       " ('BEACONS', 'Huge'),\n",
       " ('BED', 'BOILER'),\n",
       " ('BERGEN', 'BRUNSWIG')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(reuters.words())\n",
    "finder.nbest(bigram_measures.pmi, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collocations` module implements a number of measures to score collocations or other associations. \n",
    "They include: **Student's t test, Chi-Square, likelihood ratios, PMI** and so on.\n",
    "\n",
    "Here we used PMI scores for finding bigrams.\n",
    "Please read [2] for a detailed tutorial on finding collocations with NLTK.\n",
    "If you would like to know more about collocations, please refer to [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "This chapter has show you how to \n",
    "* generate vocabulary be further exploring the tokenized text with some simple statistics. \n",
    "* convert unstructured text to structured form using the bag-of-words model\n",
    "* compute TF-IDF\n",
    "* extract words in specific lexical categories, n-grams and collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference Reading Materials\n",
    "1. \"[Categorizaing and Tagging Words](http://www.nltk.org/book/ch05.html)\", \n",
    "Chapter 5 of \"Natural Language Processing with Python\".\n",
    "2. \"[Collocations](http://www.nltk.org/howto/collocations.html)\": An NTLK tutorial on how to extract collocations 📖 .\n",
    "3. \"[Collocations](http://nlp.stanford.edu/fsnlp/promo/colloc.pdf)\": An introduction to collocation by Manning and Schutze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 . Exercises\n",
    "2. We have shown you how to generate frequency of frequency bar chart with term frequency. Similarly, you can generate the bar chart based on document frequency. \n",
    "2. Remove short words. There are some very short words in the vocabulary, for example, 'aa', 'ab', 'ad', 'ax', etc.\n",
    "Write Python code to explore the distribution of word lengths, and remove those words with less than two characters.\n",
    "3. Write code to tag the Reuters corpus with the Penn Treebank tag set, find the top 10 most common tags, nouns, and verbs.\n",
    "2. There might be some text analysis tasks where the binary occurrence markers might be enough. \n",
    "Please modify the CountVectorizer code to generate binary vectors for all the Reuters articles. \n",
    "2. We have shown you how to generate feature vectors from raw text. As we mentioned in section 3, you can actually customise the two vectorizer classes by specifying, for example, the tokenizer and stopword list. So try\n",
    "to customize either vecotorizer so that it can carry out all the steps in section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
