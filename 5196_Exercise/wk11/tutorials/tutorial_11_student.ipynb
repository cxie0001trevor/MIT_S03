{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing XML Patents \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This tutorial forcus on preprocessing a set of patents documents stored in XML format and generating the sparse representations for those patents. The final output file should be exactly the same as the one stored in \"patents.txt\".\n",
    "\n",
    "In order to finish this task, you should \n",
    "1. Exatract the abstract and claims for each patent from its xml file. Use Beautiful soup \n",
    "2. Tokenise the patents\n",
    "3. Generate 100 bigram collocations \n",
    "4. Re-tokenize the patents with those bigram collocations\n",
    "5. Generate the TF-IDF vectors for those re-tokenized patents\n",
    "6. save the vectors in the form shown in \"patents.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries \n",
    "\n",
    "Here we will focus on using the existing packages as possible as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exatract Patent's abstract and Claims\n",
    "\n",
    "The first task is to parse each patent stored in the \"xml_files\" folder. The information to be extracted includes\n",
    "1. patent document number (doc-number) stored in \"publication-reference\"\n",
    "2. patent's abstract\n",
    "3. patent's claims \n",
    "\n",
    "Hint: you can use a dictionary to save patents, where the key is the doc-number, the value is a long string contains both abstracts and all claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_file_path = \"./xml_files\"\n",
    "patents_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenize the patents\n",
    "After finish extract the texts, you now need to tokenize the patents with regular expression tokenizer implemented in NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}') \n",
    "patents_tokenized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Generate the 100 bigram collocations\n",
    "The next task is go generate the bigram collocations, given the tokenized patents.\n",
    "\n",
    "The first step is to concatenate all the tokenized patents using the chain.frome_iterable function. The returned list \n",
    "by the function contains a list of all the words seprated by while space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = list(chain.from_iterable(patents_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to generate the 100 bigram cllocations. The functions you need include\n",
    "* BigramAssocMeasures()\n",
    "* BigramCollocationFinder.from_words()\n",
    "* apply_freq_filter(20)\n",
    "* apply_word_filter(lambda w: len(w) < 3)\n",
    "* nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "Please do not change the parameters given in the last three function. More information about generating collocation with NLTK can be found http://www.nltk.org/howto/collocations.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Re-tokenize the patents again.\n",
    "\n",
    "Task in Section 4 takenise the patents with only unigrams. Now, we introduce 100 collcations. we need to make sure those collocations are not split into two individual words. The tokenizer that you need is <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">MWEtokenizer</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the difference between th output of MWEtokenizer and RegexpTokenizer by <font size=3>adpating</font> the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "```python\n",
    "for pid in patents_tokenized.keys():\n",
    "    diff = set(colloc_patents[pid])-set(patents_tokenized[pid])\n",
    "    if len(diff) != 0:\n",
    "        print (pid, diff)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate the TF-IDF vectors for all the patents.\n",
    "Please refer to \n",
    "* http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the TF-IDF vector into the specified format\n",
    "Hint: you can use \n",
    "* the <a href=\"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csc_matrix.tocoo.html\">tocoo()</a> function\n",
    "* itertools.zip_longest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file = open(\"patent_student.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
