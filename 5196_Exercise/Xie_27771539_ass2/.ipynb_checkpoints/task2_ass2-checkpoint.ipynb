{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 2\n",
    "#### Student Name: Chuangfu Xie\n",
    "#### Student ID: 27771539\n",
    "\n",
    "Date: 04/05/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6.4\n",
    "\n",
    "Packages list:  \n",
    "1. **Pandas** (0.22.0)  \n",
    "2. **numpy** (1.14.0)\n",
    "3. **recordlinkage** (0.11.2): tools needed for record linkage and deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import recordlinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify conflicts and resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset2_integration.csv\")\n",
    "df_sln = pd.read_csv(\"./dataset1_solution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Schema conflicts\n",
    "First, let's have a look at these two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sln.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing these two datasets, we can find that there are some naming conflicts, where different names are used for the same object: \n",
    "1. <font color='blue'>**location**</font> againsts <font color='blue'>**Location**</font>\n",
    "2. <font color='blue'>**Contract Type**</font> againsts <font color='blue'>**ContractType**</font>, \n",
    "3. <font color='blue'>**Contract Time**</font> againsts <font color='blue'>**ContractTime**</font>, and \n",
    "4. <font color='blue'>**Source Name**</font> againsts <font color='blue'>**SourceName**</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we deal with all **naming issues** by `rename()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'location':'Location', \n",
    "                   'Contract Type':'ContractType', \n",
    "                   'Contract Time':'ContractTime',\n",
    "                   'Source Name':'SourceName'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Indentify attribute-level problems\n",
    "Previously we have solve the schema problems, but we also notice that salary data represented in both dataset are at different level of time:\n",
    "1. <font color='blue'>**Salary per month**</font> againsts <font color='blue'>**Salary per annum**</font>. \n",
    "\n",
    "Then, we use a simple arithmetic formula to unify the **salary** part:  \n",
    "$$Annual = Monthly \\times 12$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary per month'] = df['Salary per month']*12\n",
    "# Then, rename this column\n",
    "df.rename(columns={'Salary per month':'Salary per annum'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also notice that the value in following columns are different from the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From df:\")\n",
    "print(df.ContractTime.value_counts())\n",
    "print(df.ContractType.value_counts())\n",
    "print('\\n')\n",
    "print(\"From df_sln:\")\n",
    "print(df_sln.ContractTime.value_counts())\n",
    "print(df_sln.ContractType.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN in ContractTime: ' + str(df.ContractTime.isnull().sum()))\n",
    "print('NaN in ContractType: ' + str(df.ContractType.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that dataset `df` contains many missing value in <font color='blue'>**ContractTime**</font> and <font color='blue'>**ContractType**</font>. Also, we find that same entity has been presented in different forms:\n",
    "1. `perm.` againsts `permanent`\n",
    "2. `contr.` againsts `contract`\n",
    "3. `ft` againsts `full-time`\n",
    "4. `pt` againsts `part-time`\n",
    "\n",
    "Also, data in <font color='blue'>**Company**</font> are also inconsistent with other:\n",
    "1. `BOYCE RECRUITMENT` againsts `Gregory Martin International`\n",
    "\n",
    "We can use `map()` function to replace these inconsistencies and also deal with the `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Company = df.Company.str.title()\n",
    "df.ContractType = df.ContractType.map({'ft':'full-time',\n",
    "                                       'pt':'part-time'})\n",
    "df.ContractType.fillna('not available', inplace=True)\n",
    "df.ContractTime = df.ContractTime.map({'perm.':'permanent',\n",
    "                                       'contr.':'contract'})\n",
    "df.ContractTime.fillna('not available', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Deduplication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to identify duplication within and between datasets. Each data has their own key **`Id` as identifier** for each record. However, the identifier **`Id`** are not sufficient enough in this case to distinguish each record to the other, since there are other 10 attributes which characterise each record as well. Hence, we need to find the **suitable combination key** of multiple attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have worked on <font color='blue'>**ContractTime**</font>, <font color='blue'>**ContractType**</font>. These are not suitable attribute for discovering duplicate since there are only 3 possible values, which can not ideally differentiate a record. Same reason can be applied on other attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique(df):\n",
    "    for col in df.columns.tolist():\n",
    "        print(col + \": \" + str(len(df[col].unique())))\n",
    "check_unique(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can find that there are 6 attributes which has most significant number in terms of uniqueness: <font color='blue'>**Title**</font>, <font color='blue'>**Location**</font>, <font color='blue'>**Company**</font>, <font color='blue'>**Salary per annum**</font>, <font color='blue'>**OpenDate**</font> and <font color='blue'>**CloseDate**</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as there is a rull of null equality that **null value is always not equal to itself**. Let's check whether any the `null` value exists in datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sln.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these null value in <font color='blue'>**Company**</font> will mess up our comparing process, we need to deal with them: replacing it with 'Not Given'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Company.fillna(\"Not Given\",inplace=True)\n",
    "df_sln.Company.fillna(\"Not Given\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the candidate attributes for our combination key, we can work on the duplication.\n",
    "\n",
    "We use **Python Record Linkage Toolkit** to link records in or between data sources for record linkage and deduplication. Especially, `BlockIndex` class is very useful when identify records on more than one attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dup(key, df1, df2=None):\n",
    "    indexer = recordlinkage.BlockIndex(on=key)\n",
    "    try:\n",
    "        if df2 == None:\n",
    "            pairs = indexer.index(df1)\n",
    "    except:\n",
    "        pairs = indexer.index(df1,df2)\n",
    "    print(len(pairs))\n",
    "\n",
    "key = ['Title', 'Location', 'Company', 'Salary per annum', 'OpenDate', 'CloseDate']\n",
    "check_dup(key, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! It seems that all record in `df` are different.  \n",
    "Let's check another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dup(key, df_sln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to merge these two into one, how about duplications exist between these datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dup(key, df, df_sln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find there are **154 potential duplications** between datasets, we can start comparing each attribute and output the level of similarity for drop decision. \n",
    "\n",
    "The `compare()` method provide various algorithms for **string matching**: `‘jaro’`, `’jarowinkler’`, `‘levenshtein’`, `‘damerau_levenshtein’`, `‘qgram’` or `‘cosine’`. We use `'jaro'` for <font color='blue'>**Title**</font>, <font color='blue'>**Location**</font>, <font color='blue'>**Company**</font>.\n",
    "Additionally, we also should consider the threshold value where will sift the scores: \n",
    "1. Data in <font color='blue'>**Title**</font> are so varied. we give it as 0.5.\n",
    "2. As to <font color='blue'>**Location**</font>, we give 0.5.\n",
    "3. For <font color='blue'>**Company**</font>, we give 0.85.\n",
    "\n",
    "For matching numeric values, `compare()` method also provide various way: `‘step’`, `‘linear’`, `‘exp’`, `‘gauss’` or `‘squared’`. Considering each job release is an independent event, we regards <font color='blue'>**Salary per annum**</font> value as random variable which distribution is unknown. Hence, we can treat it as normal distributions, using `'gauss'` method.  \n",
    "\n",
    "For <font color='blue'>**OpenDate**</font> and <font color='blue'>**CloseDate**</font>, we using exact match becasue time is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ['Title', 'Location', 'Company', 'Salary per annum', 'OpenDate', 'CloseDate']\n",
    "indexer = recordlinkage.BlockIndex(on=key)\n",
    "pairs = indexer.index(df, df_sln)\n",
    "compare = recordlinkage.Compare()\n",
    "# Configuring paremeters for Compare()\n",
    "# String matching\n",
    "compare.string('Title','Title', threshold = 0.5, method='jaro', label='Title')\n",
    "compare.string('Location','Location', threshold = 0.5, method='jaro', label='Location')\n",
    "compare.string('Company','Company', threshold = 0.85, method='jaro', label='Company')\n",
    "# Numeric matching\n",
    "compare.numeric('Salary per annum','Salary per annum', scale=1000, method='gauss', label='Salary per annum')\n",
    "# Exact matching\n",
    "compare.exact('OpenDate','OpenDate',label='OpenDate')\n",
    "compare.exact('CloseDate','CloseDate',label='CloseDate')\n",
    "# yield the output\n",
    "results = compare.compute(pairs, df, df_sln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the comparison results.\n",
    "results.sum(axis=1).value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 154 duplications, let's have a closer look at these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = results.index.get_values()\n",
    "dp_list1= []\n",
    "dp_list2= []\n",
    "# extract index from results\n",
    "for index in indices:\n",
    "    dp_list1.append(index[0])\n",
    "    dp_list2.append(index[1])\n",
    "# For double-check\n",
    "dp_df = df.iloc[dp_list1]\n",
    "dp_df.columns = df.columns\n",
    "dp_df_sln = df_sln.iloc[dp_list2]\n",
    "dp_df_sln.columns = df_sln.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have find all the duplications. Let's have a random pick to examine in details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "pick = randint(0,len(results)-1)\n",
    "check = dp_df.iloc[pick].append(dp_df_sln.iloc[pick])\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Drop duplicate data and Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop all the duplications with index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=dp_list1)\n",
    "# Double check\n",
    "check_dup(key, df, df_sln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, merge datasets into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.concat([df, df_sln], ignore_index=True)\n",
    "# set 'id' as identifier\n",
    "new.set_index('Id',inplace=True)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv('./dataset1_dataset2_solution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "From schema integration to data integration, various identification techniques are needed to apply until finding what these conflicts are. In deduplication where data with numerous attributes, matching numeric and categorical values requires different algorithms. However, It is also important to sift attributes by their uniqueness level and finally have the ideal global key to deal with the duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* NumPy User Guide - Miscellaneous. *IEEE 754 Floating Point Special Values*. Retrieved from: [https://docs.scipy.org/doc/numpy/user/misc.html](https://docs.scipy.org/doc/numpy/user/misc.html)\n",
    "* Python Record Linkage Toolkit Documentation. *Data deduplication*. Retrieved from: [http://recordlinkage.readthedocs.io/en/latest/notebooks/data_deduplication.html](http://recordlinkage.readthedocs.io/en/latest/notebooks/data_deduplication.html)\n",
    "* Anderson J. (2017, July 5). *Pre-processing with recordlinkage*. Retrieved from:  [http://networkslab.org/2017/07/05/2017-07-05-preprocessing/](http://networkslab.org/2017/07/05/2017-07-05-preprocessing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
