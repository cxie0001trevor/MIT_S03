{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7 A: Web Scraping \n",
    "\n",
    "We cover in this part scraping data from the web. Data can be presented in HTML, XML and API etc. Web scraping is the practice of using libraries to sift through a web page and gather the data that you need in a format most useful to you while at the same time preserving the structure of the data. \n",
    "\n",
    "There are several ways to extract information from the web. Use of APIs being probably the best way to extract data from a website. Almost all large websites like Twitter, Facebook, Google, Twitter, StackOverflow provide APIs to access their data in a more structured manner. If you can get what you need through an API, it is almost always preferred approach over web scrapping. However, not all websites provide an API. Thus, we need to scrape the HTML website to fetch the information.\n",
    "\n",
    "Non-standard python libraries needed in this tutorial include\n",
    "* urllib\n",
    "* beatifulsoup \n",
    "* requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Extract a list of links on a Wikipedia page.\n",
    "\n",
    "Instead of retrieving all the links existing in a Wikipedia article, we are interested in extracting links that point to other article pages. If you look at the source code of the following page \n",
    "```\n",
    "https://en.wikipedia.org/wiki/Kevin_Bacon\n",
    "```\n",
    "in your browser, you fill find that all these links have three things in common:\n",
    "* They are in the *div* with id *set* to *bodyContent*\n",
    "* The URLs do not contain semicolons\n",
    "* The URLs begin with */wiki/*\n",
    "\n",
    "We can use these rules to construct our search through the HTML page. \n",
    "\n",
    "Firstly, use the urlopen() function to open the wikipedia page for \"Kevin Bacon\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = urlopen(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, find and print all the links. In order to finish this task, you need to\n",
    "* find the *div* whose *id = \"bodyContent\"*\n",
    "* find all the link tags, whose href starts with \"/wiki/\" and does not ends with \":\". For example\n",
    "```html\n",
    " see <a href=\"/wiki/Kevin_Bacon_(disambiguation)\" class=\"mw-disambig\" title=\"Kevin Bacon (disambiguation)\">Kevin Bacon (disambiguation)</a>\n",
    " <a href=\"/wiki/Philadelphia\" title=\"Philadelphia\">Philadelphia</a>\n",
    "```\n",
    "\n",
    "Hint: regular expression is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsobj = BeautifulSoup(html, \"lxml\")\n",
    "for link in bsobj.find(\"div\", {\"id\": \"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Perform a random walk through a given webpate.\n",
    "Assume that we will find a random object in Wikipedia that is linked to \"Kevin Bacon\" with, so-called \"Six Degrees of Wikipedia\". In other words, the task is to find two subjects linked by a chain containing no more than six subjects (including the two original subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+articleUrl)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the random walk along the links are \n",
    "* Randomly choosing a link from the list of retrieved links\n",
    "* Printing the article represented by the link\n",
    "* Retrieving a list of links \n",
    "* repeat the above step until the number of retrieved articles reaches 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while len(links) > 0 and count < 5:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Crawl the Entire Wikipedia website\n",
    "\n",
    "The general approach to an exhaustive site crawl is to start with the root, i.e., the home page of a website. Here, we will start with\n",
    "```\n",
    "https://en.wikipedia.org/\n",
    "```\n",
    "by retrieving all the links that appear in the home page. And then traverse each link recursively. However, the number of links is going to be very large and a link can appear in many Wikipedia article. Thus, we need to consider how to avoid repeatedly crawling the same article or page. In order to do so, we can keep a running list for easy lookups and slightly update the getLinks() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: add a terminating condition in your code, for example,\n",
    "```python\n",
    "    len(pages) < 10\n",
    "```\n",
    "Otherwise, the script will run through the entire Wikipedia website, which will take a long time to finish. So please avoid that in the tutorial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+pageUrl)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/wiki/)\")):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages and len(pages) < 10:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(\"----------------\\n\"+newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLinks(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Collect data across the Wikipedia site\n",
    "One purpose of traversing all the the links is to extract data. The best practice is to look at a few pages from the side and determine the patterns. By looking at a handful of Wikipedia pages both articles and non-articles pages, the following pattens can be identified:\n",
    "* All titles are under h1 span tags, and these are the only h1 tags on the page. For example,\n",
    "```html\n",
    "<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Kevin Bacon</h1>\n",
    "```\n",
    "```html\n",
    "<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Main Page</h1>\t\n",
    "```\n",
    "* All body text lives under the *div#bodyContent* tag. However, if we want to get more specific and access just the first paragraph of text, we might be better off using div#mw-content-text -> p.\n",
    "* Edit links occur only on article pages. If they occur, they will be found in the *li#ca-edit tag*, under *li#ca-edit -> span -> a*\n",
    "\n",
    "Now, the task is to further modify the getLink() function to print the title, the first paragraph and the edit link. The content from each page should be separated by \n",
    "```pyhon\n",
    "print(\"----------------\\n\"+newPage)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please also add a terminating condition in your code, for example,\n",
    "```python\n",
    "    len(pages) < 5\n",
    "```\n",
    "Otherwise, the script will run through the entire Wikipedia website, which will take a long time to finish. So please avoid that in the tutorial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+pageUrl)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    try:\n",
    "        print(bsObj.h1.get_text())\n",
    "        print(bsObj.find(id =\"mw-content-text\").findAll(\"p\")[0])\n",
    "        print(bsObj.find(id=\"ca-edit\").find(\"span\").find(\"a\").attrs['href'])\n",
    "    except AttributeError:\n",
    "        print(\"This page is missing something! No worries though!\")\n",
    "    \n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/wiki/)\")):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages and len(pages) < 5:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(\"----------------\\n\"+newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getLinks(\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Task 5 API access \n",
    "In addition to HTML format, data is commonly found on the web through public APIs. We use the 'requests' package (http://docs.python-requests.org) to call APIs using Python. In the following example, we call a public API for collecting weather data. \n",
    "\n",
    "\n",
    "** You need to sign up for a free account to get your unique API key to use in the following code. register at**  http://api.openweathermap.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we  use requests to retrieve the web page with our data\n",
    "import requests\n",
    "url = 'http://api.openweathermap.org/data/2.5/forecast?id=524901&cnt=16&APPID=1499bcd50a6310a21f11b8de4fb653a5'\n",
    "#write your APPID here#\n",
    "response= requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object contains GET query response. A successfull one has a value of 200. we need to parse the response with json to extract the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the HTTP status code https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "print (response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.content is text\n",
    "print (type(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.json() converts the content to json \n",
    "data = response.json()\n",
    "print (type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys explain the structure of the fetched data. Try displaying values for each element. In this example, the weather information exists in the 'list'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['list'][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a DataFrame with the weather information, which is demonstrated as follows. You can select a subset to display or display the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "# data with the default column headers\n",
    "weather_table_all= DataFrame(data['list'])\n",
    "weather_table_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "\n",
    "Further parsing is still required to get the table (DataFrame) in a flat shape. Now it it's your turn, parse the weather data to generate a table.\n",
    "\n",
    "*Please note that materials used in this tutorial are partially from the book \"Web Scraping with Python\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
